{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c2f4304",
   "metadata": {},
   "source": [
    "# Probability in ML\n",
    "\n",
    "Probability helps us measure uncertainty and randomness, which are common in data.\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction to Probability in ML\n",
    "\n",
    "In ML and DL, we often work with incomplete or noisy data. Probability helps us:\n",
    "- Handle uncertainty (e.g., predicting if an email is spam).\n",
    "- Quantify randomness in data features, labels, and predictions.\n",
    "- Build models that make smart decisions.\n",
    "\n",
    "---\n",
    "\n",
    "## Fundamental Concepts\n",
    "\n",
    "### What is Probability?\n",
    "Probability measures how likely an event is to happen. It ranges from:\n",
    "- **0**: Impossible (e.g., rolling a 7 on a six-sided die).\n",
    "- **1**: Certain (e.g., the sun rising tomorrow).\n",
    "\n",
    "### Random Experiments and Sample Space\n",
    "- A **random experiment** is an action with an uncertain result (e.g., flipping a coin (heads or tails?)).\n",
    "- The **sample space** is all possible outcomes (e.g., {Heads, Tails} for a coin flip).\n",
    "\n",
    "### Probability of an Event\n",
    "The probability of an event \\( A \\) is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9844684c",
   "metadata": {
    "attributes": {
     "classes": [
      "math\\["
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "P(A) = \\frac{\\text{Number of favorable outcomes}}{\\text{Total number of possible outcomes}}\n",
    "\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8489a39b",
   "metadata": {},
   "source": [
    "- **Example**: Probability of rolling a 3 on a fair die is \\( \\frac{1}{6} \\).\n",
    "\n",
    "---\n",
    "\n",
    "## Random Variables\n",
    "A **random variable (RV)** is a function that assigns numerical values to outcomes of a random experiment.\n",
    "\n",
    "### Types of Random Variables\n",
    "1. **Discrete Random Variables**:\n",
    "   - Countable values (e.g., number of heads in two coin flips: 0, 1, or 2).\n",
    "2. **Continuous Random Variables**:\n",
    "   - Any value in a range (e.g., height: 1.7m, 1.71m, etc.).\n",
    "\n",
    "### Importance in ML\n",
    "- RVs model data like:\n",
    "  - Features (e.g., pixel values in an image).\n",
    "  - Labels (e.g., spam or not spam).\n",
    "  - Predictions (e.g., chance of rain).\n",
    "\n",
    "---\n",
    "\n",
    "## Expected Value (Mean)\n",
    "\n",
    "The **expected value (E[X])** is the average outcome of a random variable over many trials.\n",
    "\n",
    "### Calculation\n",
    "For a discrete RV:\n",
    "\\[\n",
    "E[X] = \\sum (x \\times P(X = x))\n",
    "\\]\n",
    "- **Example**: Expected number of heads in two coin flips:\n",
    "  - Outcomes: 0 heads ( \\( \\frac{1}{4} \\) ), 1 head ( \\( \\frac{1}{2} \\) ), 2 heads ( \\( \\frac{1}{4} \\) ).\n",
    "  - \\( E[X] = (0 \\times \\frac{1}{4}) + (1 \\times \\frac{1}{2}) + (2 \\times \\frac{1}{4}) = 1 \\).\n",
    "\n",
    "\n",
    "### ML Application\n",
    "- Used in cost functions to measure model performance (e.g., average prediction error).\n",
    "\n",
    "---\n",
    "\n",
    "## Independent and Dependent Events\n",
    "\n",
    "- **Independent Events**: One event doesn’t change the other \n",
    "  - Example: Flipping a coin twice. The first flip (heads) does not affect the second flip.\n",
    "\n",
    "- **Dependent Events**: One event affects the other.\n",
    "  - Example: Drawing two cards from a deck without replacement. Drawing a red card first changes the probability of drawing another red card.\n",
    "\n",
    "### ML Application\n",
    "- Independence helps simplify models (e.g., Naive Bayes assumes features are independent).\n",
    "\n",
    "---\n",
    "\n",
    "## Rules of Probability\n",
    "\n",
    "1. **Total Probability Rule**:\n",
    "   - All probabilities in a sample space add to 1.\n",
    "   - Example: For a die, \\( P(1) + P(2) + \\dots + P(6) = 1 \\).\n",
    "\n",
    "2. **Addition Rule (Mutually Exclusive Events)**:\n",
    "   - \\( P(A \\text{ or } B) = P(A) + P(B) \\) if \\( A \\) and \\( B \\) can’t happen together.\n",
    "   - Example: Rolling a 1 or 2 on a die: \\( \\frac{1}{6} + \\frac{1}{6} = \\frac{1}{3} \\).\n",
    "\n",
    "3. **General Addition Rule**:\n",
    "   - \\( P(A \\text{ or } B) = P(A) + P(B) - P(A \\text{ and } B) \\) for overlapping events.\n",
    "   - Example: Drawing a king or a red card includes overlap (red kings).\n",
    "\n",
    "4. **Complement Rule**:\n",
    "   - \\( P(\\text{not } A) = 1 - P(A) \\).\n",
    "   - Example: If \\( P(\\text{rain}) = 0.3 \\), then \\( P(\\text{no rain}) = 0.7 \\).\n",
    "\n",
    "---\n",
    "\n",
    "## Conditional Probability\n",
    "\n",
    "**Conditional probability** is the chance of event \\( A \\) happening given that event \\( B \\) has occurred:\n",
    "\\[\n",
    "P(A \\mid B) = \\frac{P(A \\text{ and } B)}{P(B)}\n",
    "\\]\n",
    "\n",
    "### Example\n",
    "- Deck of 52 cards: 26 red cards, 4 kings (2 red kings).\n",
    "- Probability of drawing a king given the card is red:\n",
    "  - \\( P(\\text{King} \\mid \\text{Red}) = \\frac{2}{26} = \\frac{1}{13} \\).\n",
    "\n",
    "### ML Application\n",
    "- Used in classification (e.g., predicting a label based on features in Naive Bayes).\n",
    "\n",
    "---\n",
    "\n",
    "## Bayes' Theorem\n",
    "\n",
    "**Bayes' Theorem** updates probabilities with new information:\n",
    "\\[\n",
    "P(A \\mid B) = \\frac{P(B \\mid A) \\cdot P(A)}{P(B)}\n",
    "\\]\n",
    "\n",
    "### Example\n",
    "- A test for a disease is 99% accurate.\n",
    "- Only 1% of people have the disease (\\( P(\\text{Sick}) = 0.01 \\)).\n",
    "\n",
    "If you test **positive**, what’s the actual probability that you’re sick?  P(Test) \n",
    "- **Bayes' theorem** calculates this using prior probabilities and test accuracy. P(Test | Sick)\n",
    "\n",
    "\n",
    "### ML Application\n",
    "- Used in Bayesian models to update predictions as new data comes in.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Probability Distributions\n",
    "\n",
    "**Probability distributions** describe how probabilities are allocated across all possible outcomes. Think of it as a map that tells you how chances are distributed.\n",
    "\n",
    "- **Why They Matter in ML/DL**:\n",
    "  - Help predict outcomes (e.g., spam or not spam).\n",
    "  - Model uncertainty in data (e.g., noisy sensor readings).\n",
    "  - Reveal patterns in features (e.g., pixel values in images).\n",
    "\n",
    "- **Two Main Types**:\n",
    "  - **Discrete**: For countable outcomes (e.g., number of clicks).\n",
    "  - **Continuous**: For infinite values in a range (e.g., temperature).\n",
    "\n",
    "## Discrete Probability Distributions\n",
    "\n",
    "Discrete distributions deal with outcomes you can count, like rolling a die or flipping a coin.\n",
    "\n",
    "### What They Are\n",
    "- A discrete random variable has a set number of possible values.\n",
    "- Example: Rolling a fair six-sided die gives outcomes 1, 2, 3, 4, 5, or 6, each with a probability of \\( 1/6 \\).\n",
    "\n",
    "### Probability Mass Function (PMF)\n",
    "- The PMF tells you the probability of each specific outcome.\n",
    "- **Formula**: \\( P(X = x) \\) is the chance that \\( X \\) equals \\( x \\).\n",
    "- **Rules**:\n",
    "  - Probabilities are between 0 and 1.\n",
    "  - They sum to 1: \\( \\sum P(X = x) = 1 \\).\n",
    "- **Example**: For a die, \\( P(X = 3) = 1/6 \\).\n",
    "\n",
    "### Discrete Distributions\n",
    "- **Bernoulli Distribution**:\n",
    "  - Models a single trial (e.g., spam or not spam).\n",
    "  - Example: \\( X = 1 \\) (spam), \\( X = 0 \\) (not spam), \\( P(X=1) = 0.3 \\).\n",
    "- **Binomial Distribution**:\n",
    "  - Models multiple Bernoulli trials (e.g., number of spam emails in 10 emails).\n",
    "  - Example: \\( X \\sim \\text{Binomial}(n=10, p=0.3) \\).\n",
    "\n",
    "### Visualization\n",
    "Here’s the PMF for a fair die in a table:\n",
    "\n",
    "| Outcome | 1   | 2   | 3   | 4   | 5   | 6   |\n",
    "|---------|-----|-----|-----|-----|-----|-----|\n",
    "| Probability | 1/6 | 1/6 | 1/6 | 1/6 | 1/6 | 1/6 |\n",
    "\n",
    "A bar chart would show equal bars for each outcome.\n",
    "\n",
    "## Continuous Probability Distributions\n",
    "\n",
    "Continuous distributions handle outcomes with infinite possibilities within a range, like height or time.\n",
    "\n",
    "### What They Are\n",
    "- A continuous random variable can take any value in a range.\n",
    "- Example: Height might be 1.7m, 1.71m, 1.712m, etc.\n",
    "\n",
    "### Probability Density Function (PDF)\n",
    "- The PDF shows the \"density\" of probability across a range.\n",
    "- **Key Point**: The probability of an exact value (e.g., height = 1.7m) is 0. Instead, we look at areas under the curve.\n",
    "- **Formula**: \\( f(x) \\) is the density at \\( x \\).\n",
    "- **Rules**:\n",
    "  - \\( f(x) \\geq 0 \\) (no negative density).\n",
    "  - Total area under the curve = 1: \\( \\int_{-\\infty}^{\\infty} f(x) \\, dx = 1 \\).\n",
    "\n",
    "### Continuous Distributions\n",
    "- **Normal (Gaussian) Distribution**:\n",
    "  - Bell-shaped curve, used in many ML algorithms (e.g., Linear Regression).\n",
    "  - Example: Errors in predictions are often assumed to be normally distributed.\n",
    "- **Exponential Distribution**:\n",
    "  - Models time between events (e.g., waiting time for a bus).\n",
    "  - Used in survival analysis and reliability engineering.\n",
    "\n",
    "### Visualization\n",
    "The PDF of a normal distribution is a smooth bell curve. The area under it between two points (e.g., 1.6m to 1.8m) gives the probability of that range.\n",
    "\n",
    "---\n",
    "\n",
    "## Distribution Functions: PMF, PDF, and CDF\n",
    "\n",
    "These functions help us work with probabilities in different ways.\n",
    "\n",
    "### 1. Probability Mass Function (PMF)\n",
    "- **For Discrete Variables**: Gives the probability of exact values.\n",
    "- **Example**: For a die, \\( P(X = 4) = \\frac{1}{6} \\).\n",
    "\n",
    "### 2. Probability Density Function (PDF)\n",
    "- **For Continuous Variables**: Shows probability density.\n",
    "- **Example**: In a normal distribution, the PDF peaks at the mean.\n",
    "\n",
    "### 3. Cumulative Distribution Function (CDF)\n",
    "- **For Both Types**: Shows the probability that \\( X \\) is less than or equal to a value \\( x \\).\n",
    "- For discrete variables: Sum of PMF up to \\( x \\).\n",
    "- For continuous variables: Integral of PDF up to \\( x \\).\n",
    "- **Formula**: \\( F(x) = P(X \\leq x) \\).\n",
    "  - Discrete: \\( F(x) = \\sum_{k \\leq x} P(X = k) \\).\n",
    "  - Continuous: \\( F(x) = \\int_{-\\infty}^{x} f(t) \\, dt \\).\n",
    "- **Example**: For a die, \\( F(3) = P(X \\leq 3) = \\frac{3}{6} = 0.5 \\).\n",
    "- **Shape**: Steps for discrete, smooth S-curve for continuous.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Key Probability Principles in ML**\n",
    "- **Law of Large Numbers**: Sample mean converges to true probability as \\(n\\) increases.\n",
    "- **Central Limit Theorem**: Sums of random variables approach normality—basis for many assumptions.\n",
    "- **Maximum Likelihood Estimation (MLE)**: Optimize model parameters to maximize data likelihood.\n",
    "- **Posterior Probability**: Update beliefs with data (Bayesian ML).\n",
    "\n",
    "---\n",
    "\n",
    "## Applications in ML and DL\n",
    "\n",
    "Probability distributions power many ML and DL techniques.\n",
    "\n",
    "### PMF in Action\n",
    "- **Classification**: Predicts discrete labels (e.g., cat or dog).\n",
    "- **Naive Bayes**: Uses PMF to estimate class probabilities from features.\n",
    "\n",
    "### PDF in Action\n",
    "- **Density Estimation**: Models continuous data (e.g., Kernel Density Estimation).\n",
    "- **Generative Models**: Creates new data (e.g., Gaussian Mixture Models in image generation).\n",
    "\n",
    "### CDF in Action\n",
    "- **Anomaly Detection**: Spots outliers by checking how likely a value is.\n",
    "- **Statistical Testing**: Assesses model performance (e.g., p-values).\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Example in Python\n",
    "\n",
    "Below is a Python example to visualize PDF and CDF for a normal distribution.\n",
    "\n",
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ad0444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd41779",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664c4e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 1000 data points from a normal distribution (mean=0, std=1)\n",
    "data = np.random.normal(loc=0, scale=1, size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651d51db",
   "metadata": {},
   "source": [
    "### Plot PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfecb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PDF of the normal distribution\n",
    "x = np.linspace(-4, 4, 1000)\n",
    "pdf = norm.pdf(x, loc=0, scale=1)\n",
    "plt.plot(x, pdf, label='PDF')\n",
    "plt.title('Probability Density Function (PDF)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aa66da",
   "metadata": {},
   "source": [
    "*This plots a bell curve showing the density.*\n",
    "\n",
    "### Plot CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ed9fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CDF of the normal distribution\n",
    "cdf = norm.cdf(x, loc=0, scale=1)\n",
    "plt.plot(x, cdf, label='CDF')\n",
    "plt.title('Cumulative Distribution Function (CDF)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Probability')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ade0c92",
   "metadata": {},
   "source": [
    "*This plots an S-curve showing cumulative probability.*\n",
    "\n",
    "---\n",
    "\n",
    "### What It Means\n",
    "- The PDF shows where data is most dense (peaks at 0).\n",
    "- The CDF shows the probability of values up to any point (reaches 1 as \\( x \\) grows).\n",
    "\n",
    "*Note*: No images are embedded here, but the code generates plots in Jupyter Book.\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Probability** quantifies uncertainty using numbers from 0 to 1.\n",
    "2. **Random Variables** model outcomes in ML, either discrete or continuous.\n",
    "3. **Conditional Probability** and **Bayes' Theorem** update predictions based on new information.\n",
    "4. **Probability Distributions**: Map out how likely outcomes are.\n",
    "5. **Discrete**: Use PMF for countable values (e.g., dice rolls).\n",
    "6. **Continuous**: Use PDF for ranges (e.g., heights).\n",
    "7. **CDF**: Gives cumulative probabilities for both types.\n",
    "8. **Applications** in ML include classification, density estimation, and anomaly detection.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
