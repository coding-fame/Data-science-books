{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b9941f7",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "Decision Trees are a fundamental supervised learning algorithm in machine learning, widely used for both **classification** and **regression** tasks. They offer an intuitive, tree-like model of decisions that mimic human decision-making processes, making them highly interpretable. \n",
    "\n",
    "---\n",
    "\n",
    "### **What are Decision Trees?**\n",
    "- A decision tree is a flowchart-like model that makes decisions by recursively splitting the feature space into regions based on feature values. \n",
    "- Each internal node represents a decision (split) on a feature, each branch represents an outcome of that decision, and each leaf node represents a final prediction (class or value).\n",
    "- The process continues until it reaches **leaf nodes**, which represent the final output either a class label (classification) or a numerical value (regression).\n",
    "\n",
    "\n",
    "### **Why Use Decision Trees?**\n",
    "- **Interpretability**: Easy to understand and visualize.\n",
    "- **Non-Linearity**: Captures complex relationships without assuming linearity.\n",
    "- **Versatility**: Handles both classification (e.g., spam vs. not spam) and regression (e.g., predicting house prices).\n",
    "- **Feature Importance**: Identifies key predictors.\n",
    "\n",
    "### **How They Work**\n",
    "1. **Root Node**: Start with the entire dataset.\n",
    "2. **Splitting**: Divide data based on a feature and threshold that best separates the target variable (using criteria like Gini impurity or entropy).\n",
    "3. **Leaf Nodes**: Stop splitting when a condition is met (e.g., max depth, minimum samples).\n",
    "4. **Pruning**: Removing unnecessary branches to prevent overfitting.\n",
    "\n",
    "## **Key Concepts and Methods**\n",
    "\n",
    "### **a. Splitting Criteria**\n",
    "\n",
    "#### For Classification\n",
    "1. **Gini Impurity**:\n",
    "   - Measures the probability of incorrectly classifying a randomly chosen element if it were labeled according to the distribution of labels in the subset.\n",
    "   - Formula:  \n",
    "    ```math \n",
    "        Gini = 1 - \\sum_{i=1}^{n} p_i^2\n",
    "    ```\n",
    "     where \\( p_i \\) is the proportion of instances of class \\( i \\).\n",
    "   - Lower Gini impurity indicates a better split.\n",
    "2. **Entropy (Information Gain)**:\n",
    "   - Measures the uncertainty or disorder in the data, rooted in information theory.\n",
    "   - **Entropy formula**:  \n",
    "    ```math\n",
    "        Entropy = -\\sum_{i=1}^{n} p_i \\log_2(p_i)\n",
    "    ```\n",
    "   - Information Gain is the reduction in entropy after a split:  \n",
    "    ```math\n",
    "        IG = Entropy_{parent} - \\sum_{child} \\frac{N_{child}}{N_{parent}} Entropy_{child}\n",
    "    ```\n",
    "   - Higher Information Gain means a better split.\n",
    "\n",
    "#### For Regression\n",
    "- **Mean Squared Error (MSE)**:\n",
    "   - Measures the average squared difference between actual and predicted values in a leaf.\n",
    "   - Formula:  \n",
    "     ```math\n",
    "        MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n",
    "     ```\n",
    "     where \\( y_i \\) is the actual value and \\( \\bar{y} \\) is the mean value in the leaf.\n",
    "   - The split that minimizes MSE (or variance) is chosen.\n",
    "\n",
    "The algorithm selects the feature and threshold that optimizes the chosen criterion at each step.\n",
    "\n",
    "### Algorithms\n",
    "- **CART (Classification and Regression Trees)**:\n",
    "  - Uses a **binary splitting technique**.\n",
    "  - Uses Gini impurity (classification) or MSE (regression).\n",
    "  - Basis for scikit-learn’s implementation.\n",
    "- **ID3**:\n",
    "  - Uses entropy and information gain; handles categorical features.\n",
    "- **C4.5**:\n",
    "  - An improvement over ID3; supports continuous features and pruning.\n",
    "\n",
    "---\n",
    "\n",
    "### **b. Hyperparameters**\n",
    "- `max_depth`: Limits tree depth to prevent overfitting.\n",
    "- `min_samples_split`: Minimum samples required to split a node.\n",
    "- `min_samples_leaf`: Minimum samples in a leaf node.\n",
    "- `criterion`: Splitting metric (e.g., \"gini\", \"entropy\", \"mse\").\n",
    "\n",
    "---\n",
    "\n",
    "## Overfitting and Pruning\n",
    "\n",
    "Decision Trees can grow complex and overfit the training data, capturing noise rather than general patterns. To mitigate this:\n",
    "\n",
    "### Pre-Pruning\n",
    "- Stop tree growth early using constraints:\n",
    "  - **max_depth**: Limits the tree’s depth.\n",
    "  - **min_samples_split**: Minimum samples required to split a node.\n",
    "  - **min_samples_leaf**: Minimum samples required in a leaf node.\n",
    "\n",
    "### Post-Pruning\n",
    "- Build the full tree, then remove branches with low predictive power:\n",
    "  - **Cost Complexity Pruning**: Balances tree complexity and accuracy using a parameter (e.g., `ccp_alpha` in scikit-learn).\n",
    "  - Removes nodes that don’t significantly improve performance on a validation set.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Practical Examples**\n",
    "\n",
    "### **Example 1: Binary Classification (Iris Dataset)**\n",
    "\n",
    "Below is a complete example using the Iris dataset, including data preparation, training, tuning, evaluation, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3801a21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features: sepal length, sepal width, petal length, petal width\n",
    "y = iris.target  # Target: species (setosa, versicolor, virginica)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define hyperparameter grid for tuning\n",
    "param_grid = {'max_depth': [1, 2, 3, 4, 5, None], 'min_samples_split': [2, 5, 10]}\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best parameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_clf = grid_search.best_estimator_\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# Predict and evaluate on test set\n",
    "y_pred = best_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Visualize the tree\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(best_clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True, rounded=True)\n",
    "plt.show()\n",
    "\n",
    "# Example prediction\n",
    "new_sample = [[5.0, 3.5, 1.5, 0.2]]  # Sample measurements\n",
    "prediction = best_clf.predict(new_sample)\n",
    "print(f\"Predicted class: {iris.target_names[prediction[0]]}\")\n",
    "\n",
    "# Feature importances\n",
    "importances = best_clf.feature_importances_\n",
    "for feature, importance in zip(iris.feature_names, importances):\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718d8049",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "1. **Data Preparation**:\n",
    "   - Loads the Iris dataset, splits it into 80% training and 20% testing sets.\n",
    "2. **Model Training**:\n",
    "   - Uses `DecisionTreeClassifier` with default Gini criterion.\n",
    "3. **Hyperparameter Tuning**:\n",
    "   - `GridSearchCV` tests combinations of `max_depth` and `min_samples_split` using 5-fold cross-validation.\n",
    "4. **Evaluation**:\n",
    "   - Computes accuracy on the test set.\n",
    "5. **Visualization**:\n",
    "   - Uses `plot_tree` to display the tree structure.\n",
    "6. **Prediction**:\n",
    "   - Predicts the species for a new sample.\n",
    "7. **Feature Importance**:\n",
    "   - Shows which features (e.g., petal length) most influence decisions.\n",
    "\n",
    "### Output (Sample)\n",
    "- **Best parameters**: e.g., `{'max_depth': 3, 'min_samples_split': 2}`\n",
    "- **Accuracy**: e.g., `0.97` (97%)\n",
    "- **Visualization**: A tree diagram showing splits and decisions.\n",
    "- **Prediction**: e.g., `setosa`\n",
    "- **Feature Importances**: e.g., `petal length (cm): 0.5612` (most influential).\n",
    "\n",
    "---\n",
    "\n",
    "### **Example 2: Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc3dd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Synthetic regression data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = np.sin(X) + np.random.normal(0, 0.1, X.shape)\n",
    "\n",
    "# Fit regression tree\n",
    "reg = DecisionTreeRegressor(max_depth=3)\n",
    "reg.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_pred = reg.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(\"MSE:\", mse)\n",
    "\n",
    "# Visualize\n",
    "plt.scatter(X, y, label=\"Data\", alpha=0.5)\n",
    "plt.plot(X, y_pred, color=\"red\", label=\"Tree Fit\")\n",
    "plt.title(\"Decision Tree Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4beacbd",
   "metadata": {},
   "source": [
    "**Example 3: Visualizing the Tree**\n",
    "- **Visualization Alternatives**:\n",
    "  - Use `export_graphviz` with Graphviz for more detailed renders (requires installation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85c83fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "# Fit Iris binary classifier\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Export tree\n",
    "dot_data = export_graphviz(clf, feature_names=[\"Sepal_Length\", \"Petal_Length\"],\n",
    "                           class_names=[\"Setosa\", \"Non-Setosa\"], filled=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"iris_tree\", view=True)  # Creates iris_tree.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4348bf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **4. Tools and Methods Summary**\n",
    "- **Modeling**: `sklearn.tree.DecisionTreeClassifier`, `DecisionTreeRegressor`.\n",
    "- **Evaluation**: `sklearn.metrics.accuracy_score`, `mean_squared_error`.\n",
    "- **Visualization**: `matplotlib.pyplot.contourf()`, `graphviz`.\n",
    "- **Feature Importance**: `.feature_importances_`.\n",
    "\n",
    "---\n",
    "\n",
    "## **Limitations**:\n",
    "  - Sensitive to small data changes; may require ensemble methods for robustness.\n",
    "  - Struggles with imbalanced datasets unless adjusted (e.g., class weights).\n",
    "\n",
    "**Ensemble Methods**:\n",
    "  - Decision Trees are building blocks for **Random Forests** (bagging) and **Gradient Boosting** (boosting), which improve performance by combining multiple trees.\n",
    "\n",
    "---\n",
    "\n",
    "## Example: Play Tennis Dataset\n",
    "\n",
    "To illustrate, consider a simple dataset deciding whether to play tennis based on weather conditions:\n",
    "\n",
    "| Day | Outlook  | Temperature | Humidity | Wind  | Play Tennis |\n",
    "|-----|----------|-------------|----------|-------|-------------|\n",
    "| 1   | Sunny    | Hot         | High     | Weak  | No          |\n",
    "| 2   | Sunny    | Hot         | High     | Strong| No          |\n",
    "| 3   | Overcast | Hot         | High     | Weak  | Yes         |\n",
    "| 4   | Rain     | Mild        | High     | Weak  | Yes         |\n",
    "| 5   | Rain     | Cool        | Normal   | Weak  | Yes         |\n",
    "| 6   | Rain     | Cool        | Normal   | Strong| No          |\n",
    "| 7   | Overcast | Cool        | Normal   | Strong| Yes         |\n",
    "| 8   | Sunny    | Mild        | High     | Weak  | No          |\n",
    "| 9   | Sunny    | Cool        | Normal   | Weak  | Yes         |\n",
    "| 10  | Rain     | Mild        | Normal   | Weak  | Yes         |\n",
    "\n",
    "A Decision Tree might split as follows:\n",
    "- **Root Node**: Split on \"Outlook\".\n",
    "  - **Outlook = Overcast**: All instances are \"Yes\" → Leaf node (Yes).\n",
    "  - **Outlook = Sunny**: Split on \"Humidity\".\n",
    "    - **Humidity = High**: All \"No\" → Leaf node (No).\n",
    "    - **Humidity = Normal**: All \"Yes\" → Leaf node (Yes).\n",
    "  - **Outlook = Rain**: Split on \"Wind\".\n",
    "    - **Wind = Strong**: All \"No\" → Leaf node (No).\n",
    "    - **Wind = Weak**: All \"Yes\" → Leaf node (Yes).\n",
    "\n",
    "This tree classifies whether to play tennis based on sequential feature checks.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
