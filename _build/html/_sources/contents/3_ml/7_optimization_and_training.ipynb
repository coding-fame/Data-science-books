{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41ab1ec3",
   "metadata": {},
   "source": [
    "# 5Ô∏è‚É£ Optimization & Training\n",
    "\n",
    "---\n",
    "\n",
    "# **Loss Functions**\n",
    "- Cross-Entropy (classification), MSE (regression), Hinge Loss (SVM).\n",
    "\n",
    "# Cost Functions in Machine Learning\n",
    "\n",
    "Cost functions, also referred to as loss functions or objective functions, are critical components in machine learning. They measure the difference between a model's predicted outputs and the actual target values, providing a quantifiable metric of how well the model performs. The primary goal during training is to minimize this cost by adjusting the model's parameters iteratively, thereby improving its predictions.\n",
    "\n",
    "## What Are Cost Functions?\n",
    "A cost function is a mathematical formula that evaluates the error in a model's predictions. It serves as a guide for the optimization process, typically through techniques like gradient descent, where the model parameters are updated to reduce the cost. The choice of cost function depends on the type of machine learning task‚Äîregression or classification‚Äîand the specific problem being solved.\n",
    "\n",
    "## Purpose of Cost Functions\n",
    "The main purpose of a cost function is to:\n",
    "- Quantify the model's performance on a given dataset.\n",
    "- Provide a single scalar value to minimize during training.\n",
    "- Enable the optimization algorithm to adjust the model parameters effectively.\n",
    "\n",
    "## How Does the Model Use Cost Functions?\n",
    "1. The model makes an **initial prediction** using randomly assigned weights.\n",
    "2. The **cost function** calculates the **error** (difference between predicted and actual values).\n",
    "3. The model **adjusts its weights** using optimization algorithms like **Gradient Descent** to reduce the error.\n",
    "4. This process is repeated iteratively until the model reaches an **optimal state**.\n",
    "\n",
    "## Goal of Model Training\n",
    "The **primary goal** of training a machine learning model is to **minimize the cost function** by adjusting weights iteratively. This ensures that the model learns patterns effectively and makes accurate predictions.\n",
    "\n",
    "---\n",
    "\n",
    "# **Optimization Algorithms**\n",
    "- Gradient Descent, Stochastic Gradient Descent (SGD), Adam.\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "Gradient Descent is a fundamental optimization algorithm widely used in machine learning and deep learning to minimize a cost function. The cost function quantifies how well a model performs on a dataset, and Gradient Descent helps find the optimal model parameters (weights) that yield the lowest cost.\n",
    "\n",
    "---\n",
    "\n",
    "## What is Gradient Descent?\n",
    "\n",
    "Gradient Descent is an iterative method that adjusts a model's parameters by moving them in the direction that reduces the cost function most effectively. It leverages the gradient‚Äîa vector indicating the direction of steepest increase in the cost function. By moving in the opposite direction (negative gradient), the algorithm decreases the cost.\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "For a parameter \\( \\theta \\), the update rule is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc2c0b4",
   "metadata": {
    "attributes": {
     "classes": [
      "math"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "\\theta = \\theta - \\alpha \\cdot \\nabla J(\\theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adb471b",
   "metadata": {},
   "source": [
    "Where:\n",
    "- \\( \\theta \\): The parameter being optimized (e.g., weights in a model).\n",
    "- \\( \\alpha \\): The learning rate, a hyperparameter controlling the step size.\n",
    "- \\( \\nabla J(\\theta) \\): The gradient of the cost function \\( J \\) with respect to \\( \\theta \\).\n",
    "\n",
    "The process repeats until the cost function converges‚Äîmeaning it no longer decreases significantly.\n",
    "\n",
    "---\n",
    "## How Gradient Descent Works\n",
    "\n",
    "Here‚Äôs the step-by-step process:\n",
    "\n",
    "1. **Initialize Parameters**: Start with random values for the parameters (e.g., m=0, b=0).\n",
    "2. **Predict & Calculate Cost**  \n",
    "   ```python\n",
    "   predictions = X * m + b\n",
    "   cost = mean_squared_error(y_true, predictions)\n",
    "   ```\n",
    "3. **Compute Gradient**: Calculate the gradient of the cost function with respect to each parameter.\n",
    "   ```math\n",
    "   \\frac{\\partial}{\\partial m}J(m,b) = \\frac{1}{n}\\sum_{i=1}^n (y_i - (mx_i + b))(-x_i)\n",
    "   ```\n",
    "3. **Update Parameters**: Adjust parameters by subtracting the product of the learning rate and the gradient.\n",
    "   Adjust weights using learning rate (Œ±):\n",
    "   ```math\n",
    "   m := m - \\alpha \\cdot \\frac{\\partial J}{\\partial m}\n",
    "   ```\n",
    "   ```math\n",
    "   b := b - \\alpha \\cdot \\frac{\\partial J}{\\partial b}\n",
    "   ```\n",
    "4. **Repeat**: Iterate until convergence, typically when the cost change becomes negligible.\n",
    "\n",
    "**Convergence in Gradient Descent**\n",
    "üöÄ **Convergence** is the stage where gradient descent makes only **tiny changes** in the objective function.\n",
    "\n",
    "‚úÖ **Convergence Achieved When:**  \n",
    "- Cost changes < tolerance threshold (e.g., 0.001)  \n",
    "- Maximum iterations reached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f403761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode Implementation\n",
    "def gradient_descent(X, y, learning_rate=0.01, epochs=1000):\n",
    "    m, b = 0, 0  # Initial parameters\n",
    "    for _ in range(epochs):\n",
    "        grad_m = calculate_gradient_m(X, y, m, b)\n",
    "        grad_b = calculate_gradient_b(X, y, m, b)\n",
    "        m -= learning_rate * grad_m\n",
    "        b -= learning_rate * grad_b\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51953109",
   "metadata": {},
   "source": [
    "---\n",
    "## **Understanding the Gradient Descent Process**\n",
    "- Gradient Descent finds the **best-fit line** for a given dataset by minimizing error.\n",
    "- The error is measured using **Mean Squared Error (MSE)**.\n",
    "- If we plot **MSE** against model parameters (`m` and `b`), we get a bowl-shaped curve.\n",
    "\n",
    "### **Example Values**\n",
    "\n",
    "**Hypothetical Landscape:**  \n",
    "| m Value | b Value | MSE  |\n",
    "|---------|---------|------|\n",
    "| 0       | 0       | 1000 |\n",
    "| 50      | 15      | 800  |\n",
    "| 70      | 5       | 400  |\n",
    "| 90      | -5      | 100  |\n",
    "| 100     | -10     | 50   |\n",
    "\n",
    "### **Steps:**\n",
    "1Ô∏è‚É£ **Start with initial values** (e.g., `m=0, b=0`).\n",
    "2Ô∏è‚É£ Assume initial **MSE = 1000** at (`m=0`, `b=0`).\n",
    "3Ô∏è‚É£ Slightly **adjust `m` and `b`** and check if the error decreases.\n",
    "4Ô∏è‚É£ Repeat until we reach the **minimum error (minima).**\n",
    "5Ô∏è‚É£ The final `m` and `b` values are used in the **prediction function**.\n",
    "\n",
    "---\n",
    "## Implementation Approaches\n",
    "## **Types of Gradient Descent Approaches**\n",
    "### **1Ô∏è‚É£ Fixed Step Approach (Not Recommended ‚ùå)**\n",
    "- Uses **fixed step size** to update parameters.\n",
    "- May **overshoot** or **miss the global minima**.\n",
    "- **Not efficient** for complex functions.\n",
    "\n",
    "### **2Ô∏è‚É£ Learning Rate Approach (Recommended ‚úÖ)**\n",
    "- A **tunable parameter** that controls the step size in optimization.\n",
    "- Determines how quickly the algorithm moves towards the **minimum loss**.\n",
    "- **Each step is proportional to the slope** at the current point.\n",
    "\n",
    "### Learning Rate Comparison\n",
    "| Rate Type       | Speed | Stability | Risk          | Visual Cue  |\n",
    "|-----------------|-------|-----------|---------------|-------------|\n",
    "| **Small (0.001)** | üê¢ Slow | üõ°Ô∏è High    | Local minima  | Careful steps|\n",
    "| **Medium (0.1)**  | üö∂‚ôÇÔ∏è Moderate | ‚öñÔ∏è Balanced | Minimal       | Optimal path |\n",
    "| **Large (0.5)**   | üöÄ Fast  | üé¢ Low     | Overshooting  | Risky jumps  |\n",
    "\n",
    "---\n",
    "## Types of Gradient Descent\n",
    "\n",
    "Gradient Descent has several variants, each suited to different scenarios:\n",
    "\n",
    "### 1. Batch Gradient Descent (BGD)\n",
    "- **Description**: Computes the gradient using the entire dataset at each step.\n",
    "- **Pros**: Stable convergence due to averaging over all data points.\n",
    "- **Cons**: Computationally expensive for large datasets, as it processes everything at once.\n",
    "\n",
    "### 2. Stochastic Gradient Descent (SGD)\n",
    "- **Description**: Updates parameters using the gradient from a single, randomly selected data point per iteration.\n",
    "- **Pros**: Faster updates, can escape local minima due to noisy steps.\n",
    "- **Cons**: Noisy updates may lead to erratic convergence.\n",
    "\n",
    "### 3. Mini-Batch Gradient Descent\n",
    "- **Description**: Uses a small subset (batch) of data points to compute the gradient.\n",
    "- **Pros**: Balances the stability of BGD and the speed of SGD.\n",
    "- **Cons**: Requires tuning the batch size for optimal performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Example: Linear Regression\n",
    "\n",
    "Let‚Äôs apply Gradient Descent to a simple linear regression problem, where the goal is to fit a line \\( h_\\theta(x) = \\theta_0 + \\theta_1 x \\) to predict a continuous output based on one feature.\n",
    "\n",
    "### Cost Function\n",
    "The cost function is the Mean Squared Error (MSE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faf8778",
   "metadata": {
    "attributes": {
     "classes": [
      "math"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h_\\theta(x^{(i)}) - y^{(i)})^2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258154e1",
   "metadata": {},
   "source": [
    "Where:\n",
    "- \\( m \\): Number of training examples.\n",
    "- \\( h_\\theta(x^{(i)}) \\): Predicted value for the \\( i \\)-th example.\n",
    "- \\( y^{(i)} \\): Actual value for the \\( i \\)-th example.\n",
    "\n",
    "Gradient Descent will minimize \\( J(\\theta) \\) by adjusting \\( \\theta_0 \\) (intercept) and \\( \\theta_1 \\) (slope).\n",
    "\n",
    "---\n",
    "\n",
    "## Tools and Libraries\n",
    "\n",
    "Python offers powerful tools to implement Gradient Descent:\n",
    "- **NumPy**: For numerical computations and manual implementations.\n",
    "- **Scikit-learn**: Provides optimized machine learning algorithms like linear regression.\n",
    "- **TensorFlow/Keras**: Ideal for deep learning with automatic differentiation.\n",
    "- **PyTorch**: Offers dynamic computation graphs for flexible optimization.\n",
    "\n",
    "---\n",
    "\n",
    "## One-Stop Solution: Python Code Example\n",
    "\n",
    "Below is a complete Python script demonstrating Gradient Descent for linear regression. It includes a manual implementation using NumPy and a comparison with scikit-learn‚Äôs optimized version.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8633c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# --- Manual Gradient Descent ---\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)  # Feature values\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)  # Target with noise\n",
    "\n",
    "# Add bias term (x0 = 1) for intercept\n",
    "X_b = np.c_[np.ones((100, 1)), X]\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "n_iterations = 1000\n",
    "m = len(X_b)\n",
    "\n",
    "# Initialize parameters randomly\n",
    "theta = np.random.randn(2, 1)\n",
    "\n",
    "# Gradient Descent loop\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = (2/m) * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - learning_rate * gradients\n",
    "\n",
    "# Results\n",
    "print(\"Manual Gradient Descent Results:\")\n",
    "print(f\"Theta: {theta.ravel()}\")\n",
    "\n",
    "# Plot\n",
    "plt.scatter(X, y, label=\"Data\")\n",
    "plt.plot(X, X_b.dot(theta), color='red', label=\"Manual GD Fit\")\n",
    "plt.title(\"Manual Gradient Descent\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --- Using Scikit-learn ---\n",
    "\n",
    "# Train model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "theta_sklearn = [model.intercept_[0], model.coef_[0][0]]\n",
    "\n",
    "# Results\n",
    "print(\"\\nScikit-learn Results:\")\n",
    "print(f\"Theta: {theta_sklearn}\")\n",
    "\n",
    "# Plot\n",
    "plt.scatter(X, y, label=\"Data\")\n",
    "plt.plot(X, model.predict(X), color='green', label=\"Scikit-learn Fit\")\n",
    "plt.title(\"Scikit-learn Linear Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b842a1d",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "\n",
    "#### Manual Gradient Descent\n",
    "- **Data Generation**: Creates synthetic data with a linear relationship (\\( y = 4 + 3x + \\text{noise} \\)).\n",
    "- **Bias Term**: Adds a column of ones to \\( X \\) for the intercept (\\( \\theta_0 \\)).\n",
    "- **Hyperparameters**: Sets learning rate (\\( \\alpha = 0.1 \\)) and iterations (1000).\n",
    "- **Initialization**: Starts with random \\( \\theta \\) values.\n",
    "- **Gradient Descent**: Computes gradients and updates \\( \\theta \\) iteratively.\n",
    "- **Output**: Prints \\( \\theta_0 \\) and \\( \\theta_1 \\), plots the fitted line.\n",
    "\n",
    "#### Scikit-learn Implementation\n",
    "- **Model Training**: Uses `LinearRegression` to fit the data.\n",
    "- **Output**: Extracts intercept and slope, plots the result for comparison.\n",
    "\n",
    "### Expected Output\n",
    "- **Manual GD**: \\( \\theta \\) values close to [4, 3] (due to the data‚Äôs true relationship).\n",
    "- **Scikit-learn**: Similar \\( \\theta \\) values, optimized analytically.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Gradient Descent is a cornerstone of machine learning optimization. Its variants Batch, Stochastic, and Mini-Batch offer flexibility for different dataset sizes and computational constraints. \n",
    "\n",
    "---\n",
    "\n",
    "# **Regularization**\n",
    "- L1 (Lasso), L2 (Ridge), Dropout (in neural networks).\n",
    "\n",
    "## Lasso and Ridge Regression\n",
    "\n",
    "Lasso and Ridge Regression are powerful extensions of linear regression that incorporate **regularization** to prevent overfitting, especially when dealing with high-dimensional datasets or multicollinearity. \n",
    "\n",
    "---\n",
    "\n",
    "## Understanding Lasso and Ridge Regression\n",
    "\n",
    "Both Lasso and Ridge Regression modify the standard linear regression objective by adding a **penalty term** to the loss function. In ordinary least squares (OLS) regression, the goal is to minimize the sum of squared errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45beb468",
   "metadata": {
    "attributes": {
     "classes": [
      "math"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "\\text{Loss}_{\\text{OLS}} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "``` \n",
    "\n",
    "However, OLS can overfit when there are many features or when features are highly correlated, leading to large coefficient values and poor generalization to new data. Regularization addresses this by constraining the coefficients.\n",
    "\n",
    "## Ridge Regression (L2 Regularization)\n",
    "- **Concept**: Ridge Regression adds a penalty based on the **sum of the squared coefficients** (L2 norm) to the OLS loss function.\n",
    "- **Loss Function**:\n",
    "  ```\n",
    "  Minimize: Residual Sum of Squares + Œ± * (Œ£coefficients¬≤)\n",
    "  ```\n",
    "  Here, Œ± (`alpha`) is the regularization parameter that controls the strength of the penalty.\n",
    "- **Effect**: Shrinks the coefficients toward zero but does not set them exactly to zero.\n",
    "- **Use Case**: Ideal when all features are potentially relevant, and multicollinearity exists (e.g., highly correlated predictors). It reduces the impact of less important features without eliminating them.\n",
    "\n",
    "## Lasso Regression (L1 Regularization)\n",
    "- **Concept**: Lasso Regression adds a penalty based on the **sum of the absolute values of the coefficients** (L1 norm).\n",
    "- **Loss Function**:\n",
    "  ```\n",
    "  Minimize: Residual Sum of Squares + Œ± * (Œ£|coefficients|)\n",
    "  ```\n",
    "- **Effect**: Can shrink some coefficients to exactly zero, effectively performing **feature selection**.\n",
    "- **Use Case**: Best when many features are irrelevant or redundant, simplifying the model by excluding unimportant predictors.\n",
    "\n",
    "## Key Differences\n",
    "| Aspect               | Ridge Regression                  | Lasso Regression                  |\n",
    "|----------------------|-----------------------------------|-----------------------------------|\n",
    "| **Penalty Type**     | L2 norm (\\(\\sum \\beta_j^2\\))      | L1 norm (\\(\\sum |\\beta_j|\\))     |\n",
    "| **Coefficient Effect**| Shrinks but keeps all non-zero    | Can set some to zero             |\n",
    "| **Feature Selection**| No                                | Yes                              |\n",
    "| **Multicollinearity**| Handles well by shrinking coefficients | May arbitrarily select one from correlated features |\n",
    "\n",
    "## Additional Method: Elastic Net\n",
    "- **Concept**: Combines L1 and L2 penalties, offering a balance between Lasso and Ridge.\n",
    "- **Loss Function**:\n",
    "    ```  \n",
    "    Minimize: Residual Sum of Squares + Œª‚ÇÅ * (Œ£ |coefficients|) + Œª‚ÇÇ * (Œ£ coefficients¬≤)  \n",
    "    ```  \n",
    "- **Use Case**: Useful when there are groups of correlated features, as it can select entire groups rather than just one.\n",
    "\n",
    "---\n",
    "\n",
    "## Tools and Methods\n",
    "To implement Lasso and Ridge Regression effectively, we rely on the following tools and methods:\n",
    "\n",
    "- **Python Library**: **Scikit-learn** (`sklearn`) provides robust implementations:\n",
    "  - `Ridge` and `Lasso` for basic models.\n",
    "  - `RidgeCV` and `LassoCV` for automatic \\(\\lambda\\) selection via cross-validation.\n",
    "  - `ElasticNetCV` for combining L1 and L2 penalties.\n",
    "- **Data Preprocessing**:\n",
    "  - **Feature Scaling**: Use `StandardScaler` to standardize features (mean=0, variance=1), as regularization is sensitive to feature scales.\n",
    "  - Handle missing values and encode categorical variables if necessary.\n",
    "- **Hyperparameter Tuning**: \\(\\lambda\\) (or `alpha` in scikit-learn) controls regularization strength. Cross-validation selects the optimal value.\n",
    "- **Evaluation Metrics**: Mean Squared Error (MSE) or R-squared to assess model performance.\n",
    "- **Visualization**: Plot coefficients to compare the effects of regularization.\n",
    "\n",
    "---\n",
    "\n",
    "## Example: Synthetic Dataset\n",
    "To illustrate Lasso and Ridge Regression, we‚Äôll create a synthetic dataset with:\n",
    "- **Relevant Features**: `X1`, `X2`, `X3` (with coefficients 3, 2, 0.5 in the true model).\n",
    "- **Irrelevant Feature**: `X4` (random noise).\n",
    "- **Correlated Feature**: `X5` (highly correlated with `X1`).\n",
    "- **Target**: \\( y = 3X1 + 2X2 + 0.5X3 + \\text{noise} \\).\n",
    "\n",
    "We‚Äôll compare Linear Regression, Ridge, Lasso, and Elastic Net.\n",
    "\n",
    "### Python One-Stop Solution\n",
    "Below is a complete Python script that generates the data, trains the models, evaluates performance, and visualizes the results.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "X1 = np.random.randn(n_samples)\n",
    "X2 = np.random.randn(n_samples)\n",
    "X3 = np.random.randn(n_samples)\n",
    "X4 = np.random.randn(n_samples)  # irrelevant\n",
    "X5 = X1 + 0.1 * np.random.randn(n_samples)  # correlated with X1\n",
    "y = 3*X1 + 2*X2 + 0.5*X3 + np.random.randn(n_samples)\n",
    "X = pd.DataFrame({'X1': X1, 'X2': X2, 'X3': X3, 'X4': X4, 'X5': X5})\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define range of alphas for regularization\n",
    "alphas = np.logspace(-3, 3, 7)  # [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "# Train models\n",
    "# 1. Linear Regression (no regularization)\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 2. Ridge Regression\n",
    "ridge = RidgeCV(alphas=alphas)\n",
    "ridge.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 3. Lasso Regression\n",
    "lasso = LassoCV(alphas=alphas)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 4. Elastic Net\n",
    "elastic = ElasticNetCV(alphas=alphas, l1_ratio=[0.1, 0.5, 0.9])\n",
    "elastic.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_lr = lr.predict(X_test_scaled)\n",
    "y_pred_ridge = ridge.predict(X_test_scaled)\n",
    "y_pred_lasso = lasso.predict(X_test_scaled)\n",
    "y_pred_elastic = elastic.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate models using MSE\n",
    "print(\"=== Model Performance (MSE) ===\")\n",
    "print(f\"Linear Regression MSE: {mean_squared_error(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"Ridge MSE: {mean_squared_error(y_test, y_pred_ridge):.4f}\")\n",
    "print(f\"Best alpha for Ridge: {ridge.alpha_}\")\n",
    "print(f\"Lasso MSE: {mean_squared_error(y_test, y_pred_lasso):.4f}\")\n",
    "print(f\"Best alpha for Lasso: {lasso.alpha_}\")\n",
    "print(f\"ElasticNet MSE: {mean_squared_error(y_test, y_pred_elastic):.4f}\")\n",
    "print(f\"Best alpha for ElasticNet: {elastic.alpha_}, Best l1_ratio: {elastic.l1_ratio_}\")\n",
    "\n",
    "# Extract coefficients\n",
    "coef_lr = lr.coef_\n",
    "coef_ridge = ridge.coef_\n",
    "coef_lasso = lasso.coef_\n",
    "coef_elastic = elastic.coef_\n",
    "\n",
    "# Plot coefficients\n",
    "features = X.columns\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(coef_lr, 'o', label='Linear Regression')\n",
    "plt.plot(coef_ridge, 'o', label='Ridge')\n",
    "plt.plot(coef_lasso, 'o', label='Lasso')\n",
    "plt.plot(coef_elastic, 'o', label='ElasticNet')\n",
    "plt.xticks(range(len(features)), features)\n",
    "plt.ylabel('Coefficient Value')\n",
    "plt.title('Comparison of Coefficients Across Models')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Feature selection by Lasso\n",
    "selected_features = features[coef_lasso != 0].tolist()\n",
    "print(\"=== Lasso Feature Selection ===\")\n",
    "print(f\"Features selected by Lasso: {selected_features}\")\n",
    "print(f\"Number of features selected: {len(selected_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b12400",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Explanation of the Code and Results\n",
    "\n",
    "### Workflow\n",
    "1. **Data Generation**:\n",
    "   - Five features (`X1` to `X5`) are created with `X4` being irrelevant and `X5` correlated with `X1`.\n",
    "   - The target `y` is a linear combination of `X1`, `X2`, and `X3` plus noise.\n",
    "\n",
    "2. **Data Preprocessing**:\n",
    "   - Split into 80% training and 20% test sets.\n",
    "   - Features are scaled using `StandardScaler` to ensure regularization works correctly.\n",
    "\n",
    "3. **Model Training**:\n",
    "   - **Linear Regression**: No regularization.\n",
    "   - **Ridge**: Uses `RidgeCV` to select the best \\(\\lambda\\) from a logarithmic range.\n",
    "   - **Lasso**: Uses `LassoCV` for \\(\\lambda\\) selection.\n",
    "   - **Elastic Net**: Tunes both \\(\\lambda\\) and `l1_ratio` (mix of L1 and L2 penalties).\n",
    "\n",
    "4. **Evaluation**:\n",
    "   - MSE is calculated for each model on the test set.\n",
    "   - Optimal \\(\\lambda\\) (and `l1_ratio` for Elastic Net) is reported.\n",
    "\n",
    "5. **Visualization**:\n",
    "   - Coefficients are plotted to show how each model treats the features.\n",
    "   - Lasso typically sets coefficients of irrelevant (`X4`) or redundant (`X5`) features to zero, while Ridge shrinks all coefficients.\n",
    "\n",
    "6. **Feature Selection**:\n",
    "   - Lasso identifies the most relevant features by setting some coefficients to zero.\n",
    "\n",
    "### Expected Observations\n",
    "- **Linear Regression**: Coefficients may be large, especially for correlated features (`X1` and `X5`).\n",
    "- **Ridge**: All coefficients are non-zero but reduced in magnitude.\n",
    "- **Lasso**: Likely sets `X4` (irrelevant) and possibly `X5` (correlated with `X1`) to zero, selecting `X1`, `X2`, and `X3`.\n",
    "- **Elastic Net**: Behavior depends on `l1_ratio`; closer to Lasso with high `l1_ratio`, closer to Ridge with low `l1_ratio`.\n",
    "- **MSE**: Regularized models may have slightly higher MSE on this small dataset but generalize better in practice.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Tips\n",
    "- **Choosing \\(\\lambda\\)**: Use a wide range (e.g., `np.logspace(-3, 3, 7)`) and let cross-validation decide.\n",
    "- **Real Datasets**: Handle missing values, encode categorical variables, and explore multicollinearity (e.g., via correlation matrices).\n",
    "- **High-Dimensional Data**: Lasso and Elastic Net shine in feature selection for datasets with many predictors (e.g., genomics, text analysis).\n",
    "- **Extensions**: Use `GridSearchCV` for more flexible hyperparameter tuning if needed.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "Lasso and Ridge Regression enhance linear regression by adding regularization to control model complexity. Ridge is excellent for handling multicollinearity and retaining all features, while Lasso excels at feature selection by eliminating irrelevant predictors. \n",
    "\n",
    "---\n",
    "\n",
    "# **Hyperparameter Tuning**\n",
    "- Grid Search, Random Search, Bayesian Optimization.\n",
    "\n",
    "GridSearchCV and Hyperparameter Tuning\n",
    "\n",
    "Hyperparameter tuning is a critical step in machine learning to optimize model performance by finding the best settings for a model's hyperparameters. Among the tools available for this purpose, **GridSearchCV** from scikit-learn stands out as a robust and widely used method. \n",
    "\n",
    "## Introduction\n",
    "\n",
    "When building a Machine Learning model, two key components play a crucial role:\n",
    "\n",
    "### Model Parameters\n",
    "- Internal values that the model learns automatically from the data.\n",
    "- Example: The weights in a neural network, or the support vectors in an SVM.\n",
    "\n",
    "### Model Hyperparameters\n",
    "- External configurations set by the programmer to optimize the model's performance.\n",
    "- Example: Learning rate, the number of trees in a random forest, or the kernel type in SVM.\n",
    "\n",
    "---\n",
    "\n",
    "## What is Hyperparameter Tuning?\n",
    "\n",
    "In machine learning, **hyperparameters** are settings defined before training a model, unlike model parameters, which are learned during training. Examples include the learning rate in gradient boosting, the number of trees in a random forest, or the regularization strength in logistic regression. **Hyperparameter tuning** involves searching for the combination of these settings that maximizes a model's performance, typically evaluated using metrics like accuracy, F1-score, or mean squared error.\n",
    "\n",
    "### Why is Hyperparameter Tuning Important?\n",
    "\n",
    "- **Performance Optimization**: Small changes in hyperparameters can lead to significant improvements in model accuracy or other metrics.\n",
    "- **Avoid Overfitting/Underfitting**: Tuning helps strike a balance between bias and variance, ensuring the model generalizes well to unseen data.\n",
    "- **Efficiency**: Automated tuning methods save time and effort compared to manual trial-and-error.\n",
    "\n",
    "---\n",
    "\n",
    "## What is GridSearchCV?\n",
    "\n",
    "**GridSearchCV** (Grid Search with Cross-Validation) is a scikit-learn tool designed to systematically explore a predefined set of hyperparameter combinations. It evaluates each combination using cross-validation and selects the one with the best performance.\n",
    "\n",
    "### How GridSearchCV Works\n",
    "\n",
    "1. **Define a Parameter Grid**: Create a dictionary where keys are hyperparameter names and values are lists of possible settings to test.\n",
    "2. **Cross-Validation**: For each combination in the grid, train and evaluate the model using cross-validation (e.g., k-fold cross-validation).\n",
    "3. **Select the Best Model**: Identify the combination yielding the highest cross-validation score, such as accuracy or F1-score.\n",
    "\n",
    "### Key Parameters of GridSearchCV\n",
    "\n",
    "- **estimator**: The machine learning model to tune (e.g., `RandomForestClassifier()`).\n",
    "- **param_grid**: A dictionary specifying the hyperparameters and their possible values.\n",
    "- **cv**: The number of cross-validation folds (e.g., 5 for 5-fold CV).\n",
    "- **scoring**: The metric to optimize (e.g., `'accuracy'`, `'f1'`, `'neg_mean_squared_error'`).\n",
    "- **n_jobs**: Number of CPU cores to use for parallel processing (e.g., `-1` to use all available cores).\n",
    "\n",
    "---\n",
    "\n",
    "## Tools and Methods for Hyperparameter Tuning\n",
    "\n",
    "While GridSearchCV is a cornerstone of hyperparameter tuning, other tools and methods can complement or replace it depending on your needs.\n",
    "\n",
    "### 1. Scikit-learn Tools\n",
    "- **GridSearchCV**: Exhaustively tests all combinations in the parameter grid.\n",
    "- **RandomizedSearchCV**: Randomly samples a fixed number of combinations, making it faster for large grids.\n",
    "- **HalvingGridSearchCV**: Uses successive halving to allocate resources to promising combinations, improving efficiency.\n",
    "\n",
    "### 2. Advanced Libraries\n",
    "- **Optuna**: A flexible framework using Bayesian optimization to efficiently search for optimal hyperparameters.\n",
    "- **Hyperopt**: Another Bayesian optimization tool compatible with various models.\n",
    "- **Scikit-optimize**: Provides Bayesian optimization specifically for scikit-learn models.\n",
    "\n",
    "### 3. Cross-Validation\n",
    "- Cross-validation ensures reliable performance estimates. For classification tasks, use `StratifiedKFold` to preserve class distributions across folds.\n",
    "\n",
    "### 4. Scoring Metrics\n",
    "- Choose metrics based on your problem:\n",
    "  - Classification: `'accuracy'`, `'f1'`, `'roc_auc'`.\n",
    "  - Regression: `'neg_mean_squared_error'`, `'r2'`.\n",
    "\n",
    "### 5. Parallelization\n",
    "- Set `n_jobs=-1` in GridSearchCV to leverage multiple CPU cores.\n",
    "- For very large datasets, consider distributed frameworks like Dask or Spark.\n",
    "\n",
    "---\n",
    "\n",
    "## Example 1: Tuning a Random Forest with GridSearchCV\n",
    "\n",
    "Let‚Äôs use the **Iris dataset** to tune a **Random Forest Classifier** with GridSearchCV.\n",
    "\n",
    "### Step 1: Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbd6af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035c9c66",
   "metadata": {},
   "source": [
    "### Step 2: Define the Parameter Grid\n",
    "\n",
    "We‚Äôll tune three hyperparameters for the Random Forest:\n",
    "- `n_estimators`: Number of trees.\n",
    "- `max_depth`: Maximum depth of each tree.\n",
    "- `min_samples_split`: Minimum samples required to split a node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed03dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c99e01",
   "metadata": {},
   "source": [
    "### Step 3: Initialize and Run GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f074f25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest Classifier\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9cd60a",
   "metadata": {},
   "source": [
    "- `cv=5`: Use 5-fold cross-validation.\n",
    "- `n_jobs=-1`: Utilize all CPU cores for faster computation.\n",
    "\n",
    "### Step 4: Analyze the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ba146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best parameters and best score\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# Test the best model on the test set\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred = best_rf.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1631e4b",
   "metadata": {},
   "source": [
    "**Sample Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fb60db",
   "metadata": {},
   "outputs": [],
   "source": [
    "Best Parameters: {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n",
    "Best Cross-Validation Accuracy: 0.9583\n",
    "Test Accuracy: 1.0000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033ffbc5",
   "metadata": {},
   "source": [
    "This output indicates that the best Random Forest configuration achieves a cross-validation accuracy of 95.83% and a perfect test accuracy of 100% on the Iris dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## Example 2: Tuning XGBoost with GridSearchCV\n",
    "\n",
    "Now, let‚Äôs tune an **XGBoost Classifier** using the **Breast Cancer dataset**.\n",
    "\n",
    "### Step 1: Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a323ede2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca742e0",
   "metadata": {},
   "source": [
    "### Step 2: Define the Parameter Grid\n",
    "\n",
    "For XGBoost, we‚Äôll tune:\n",
    "- `learning_rate`: Step size for updates.\n",
    "- `max_depth`: Maximum tree depth.\n",
    "- `n_estimators`: Number of trees.\n",
    "- `subsample`: Fraction of samples used per tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771e3be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621b086d",
   "metadata": {},
   "source": [
    "### Step 3: Initialize and Run GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454549ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBoost Classifier\n",
    "xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "# Initialize and fit GridSearchCV\n",
    "grid_search = GridSearchCV(xgb_clf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5620e164",
   "metadata": {},
   "source": [
    "- `use_label_encoder=False` and `eval_metric='logloss'`: Required for newer XGBoost versions to avoid warnings.\n",
    "\n",
    "### Step 4: Analyze the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766457de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV Accuracy: {grid_search.best_score_:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, grid_search.best_estimator_.predict(X_test)):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e183309a",
   "metadata": {},
   "source": [
    "**Sample Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ada84f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
    "Best CV Accuracy: 0.9714\n",
    "Test Accuracy: 0.9737"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5b553a",
   "metadata": {},
   "source": [
    "This shows that the tuned XGBoost model achieves a cross-validation accuracy of 97.14% and a test accuracy of 97.37%.\n",
    "\n",
    "---\n",
    "\n",
    "## Alternatives to GridSearchCV\n",
    "\n",
    "While GridSearchCV is effective, it can be slow for large grids or datasets due to its exhaustive nature. Consider these alternatives:\n",
    "\n",
    "- **RandomizedSearchCV**: Samples a subset of combinations, reducing computation time while often finding near-optimal settings.\n",
    "  - Example: Replace `GridSearchCV` with `RandomizedSearchCV` and add `n_iter=10` to test 10 random combinations.\n",
    "- **Bayesian Optimization**: Uses probabilistic models to intelligently explore the parameter space (e.g., via Optuna or Hyperopt).\n",
    "- **HalvingGridSearchCV**: Starts with a small subset of data and progressively focuses on promising combinations.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Tips for Hyperparameter Tuning\n",
    "\n",
    "- **Start with a Coarse Grid**: Test a broad range of values first, then refine around the best ones.\n",
    "- **Use RandomizedSearchCV for Large Grids**: It‚Äôs more efficient when the parameter space is vast.\n",
    "- **Log-Scale for Continuous Parameters**: For parameters like learning rate, use values like `[0.001, 0.01, 0.1, 1]`.\n",
    "- **Early Stopping**: For models like XGBoost, stop training if performance doesn‚Äôt improve (not directly supported in GridSearchCV but available in native XGBoost).\n",
    "- **Feature Engineering**: Good features can reduce the need for extensive tuning.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "GridSearchCV is a powerful and straightforward tool for hyperparameter tuning, automating the process of finding the best model configuration through exhaustive search and cross-validation. While it excels in reliability, alternatives like RandomizedSearchCV or Bayesian optimization tools (e.g., Optuna) offer efficiency for larger problems. \n",
    "\n",
    "---\n",
    "\n",
    "## Ways to Tune Hyperparameters\n",
    "\n",
    "### üìå Approach 1: Manual Tuning using `train_test_split`\n",
    "A basic method where we split the dataset into training and testing sets and manually adjust parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de94de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create and train model with manually selected parameters\n",
    "model = SVC(kernel='rbf', C=30, gamma='auto')\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee52cb4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### üìå Approach 2: K-Fold Cross Validation\n",
    "Instead of a single train-test split, K-Fold Cross Validation divides data into multiple subsets (folds) and trains on different combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215d96fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Try different kernel and C values\n",
    "sc1 = cross_val_score(svm.SVC(kernel='linear', C=10, gamma='auto'), X, y, cv=5)\n",
    "sc2 = cross_val_score(svm.SVC(kernel='rbf', C=10, gamma='auto'), X, y, cv=5)\n",
    "sc3 = cross_val_score(svm.SVC(kernel='rbf', C=20, gamma='auto'), X, y, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43478c69",
   "metadata": {},
   "source": [
    "---\n",
    "### üìå Approach 3: GridSearchCV (Exhaustive Search)\n",
    "**GridSearchCV** automates hyperparameter tuning by exhaustively searching through a predefined set of hyperparameters to find the best combination.\n",
    "It evaluates every combination of hyperparameters and selects the best performing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc56017",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [1, 10, 20],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV with cross-validation\n",
    "clf = GridSearchCV(SVC(gamma='auto'), param_grid, cv=5, return_train_score=False)\n",
    "\n",
    "# Fit the model and find the best parameters\n",
    "clf.fit(iris.data, iris.target)\n",
    "\n",
    "# Print the best parameters and score\n",
    "print(\"Best Parameters:\", clf.best_params_)\n",
    "print(\"Best Score:\", clf.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc4463d",
   "metadata": {},
   "source": [
    "---\n",
    "### üìå Approach 4: RandomizedSearchCV (Efficient Search)\n",
    "Instead of an exhaustive search, RandomizedSearchCV selects a limited number of parameter combinations randomly, reducing computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc889aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "# Define the hyperparameter distribution\n",
    "param_dist = {\n",
    "    'C': [1, 10, 20],\n",
    "    'kernel': ['rbf', 'linear']\n",
    "}\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "rs = RandomizedSearchCV(SVC(gamma='auto'), param_dist, cv=5, return_train_score=False, n_iter=2)\n",
    "\n",
    "# Fit the model and find the best parameters\n",
    "rs.fit(iris.data, iris.target)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters:\", rs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291f8570",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# XGBoost\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) is a powerful and efficient machine learning algorithm widely used for structured/tabular data problems such as classification, regression, and ranking. \n",
    "\n",
    "---\n",
    "\n",
    "## **What is XGBoost?**\n",
    "**XGBoost (eXtreme Gradient Boosting)** is an advanced ensemble machine learning algorithm that combines multiple decision trees using gradient boosting.\n",
    "\n",
    "It builds an ensemble of weak learners (typically decision trees) sequentially, where each tree corrects the errors of its predecessors, guided by gradient descent on a loss function.\n",
    "\n",
    "## **Why Use XGBoost?**\n",
    "- **High Performance**: Often outperforms other algorithms in accuracy and speed.\n",
    "- **Flexibility**: Supports classification, regression, ranking, and more.\n",
    "- **Robustness**: Handles missing data, overfitting, and noisy datasets well.\n",
    "- **Feature Importance**: Provides insights into key predictors.\n",
    "\n",
    "## **How It Works**\n",
    "1. **Base Learners**: Starts with weak decision trees (shallow trees).\n",
    "2. **Gradient Boosting**: Iteratively adds trees that minimize a loss function by following the negative gradient.\n",
    "3. **Regularization**: Incorporates L1 (Lasso) and L2 (Ridge) penalties to prevent overfitting.\n",
    "4. **Optimization**: Uses advanced techniques like second-order gradients (Hessian) and parallel processing.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Concepts and Methods**\n",
    "\n",
    "### **a. Core Mechanics**\n",
    "- **Loss Function**: \n",
    "  - Classification: Log-loss (binary/multiclass).\n",
    "  - Regression: Mean Squared Error (MSE) or others (e.g., MAE).\n",
    "- **Gradient and Hessian**: Uses first (gradient) and second (Hessian) derivatives to optimize the loss.\n",
    "- **Tree Building**: Adds trees by splitting based on gain, with regularization terms:\n",
    "  \\[\n",
    "  \\text{Objective} = \\sum \\text{Loss}(y_i, \\hat{y}_i) + \\sum \\Omega(f_k)\n",
    "  \\]\n",
    "  where \\(\\Omega(f_k) = \\gamma T + \\frac{1}{2} \\lambda \\|w\\|^2\\) (T = # leaves, w = leaf weights).\n",
    "\n",
    "### **b. Hyperparameters**\n",
    "XGBoost‚Äôs performance can be optimized by tuning key parameters:\n",
    "\n",
    "- **Learning Rate (`eta`)**: Shrinks contribution of each tree (0.01 - 0.3).\n",
    "- **Max Depth (`max_depth`)**: Controls tree complexity (3 - 10).\n",
    "- **Number of Estimators (`n_estimators`)**: Number of trees (50 - 1000).\n",
    "- **Regularization**: `lambda` (L2), `alpha` (L1).\n",
    "- **Subsample**: Fraction of data sampled per tree (0.5‚Äì1).\n",
    "- **Colsample_bytree**: Fraction of features sampled per tree (0.5 - 1.0).\n",
    "\n",
    "---\n",
    "\n",
    "# Examples of Use Cases\n",
    "\n",
    "## 1. Classification: Predicting Breast Cancer\n",
    "Using the **Breast Cancer Wisconsin dataset**, classify tumors as malignant or benign.\n",
    "\n",
    "Let‚Äôs implement XGBoost for a classification task using the Breast Cancer dataset.\n",
    "\n",
    "### Step 1: Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573868fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = data.target  # 0 = malignant, 1 = benign\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff87029",
   "metadata": {},
   "source": [
    "### Step 2: Train the XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbf4f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the classifier\n",
    "xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "xgb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc338ca",
   "metadata": {},
   "source": [
    "- `use_label_encoder=False`: Avoids a deprecation warning.\n",
    "- `eval_metric='logloss'`: Optimizes for binary classification.\n",
    "\n",
    "### Step 3: Make Predictions and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7af213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "y_pred = xgb_clf.predict(X_test)\n",
    "\n",
    "# Evaluate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Detailed report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=data.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eba054f",
   "metadata": {},
   "source": [
    "**Sample Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8d63db",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy: 0.97\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "   malignant       0.98      0.95      0.96        43\n",
    "      benign       0.97      0.99      0.98        71"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4ad83f",
   "metadata": {},
   "source": [
    "### Step 4: Feature Importance\n",
    "Visualize the top features contributing to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea0f9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "xgb.plot_importance(xgb_clf, max_num_features=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c13ca6c",
   "metadata": {},
   "source": [
    "### Step 5: Hyperparameter Tuning with GridSearchCV\n",
    "Optimize the model with a grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e5512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'n_estimators': [50, 100, 200]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "                           param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Results\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV Accuracy: {grid_search.best_score_:.2f}\")\n",
    "\n",
    "# Test with best model\n",
    "best_xgb = grid_search.best_estimator_\n",
    "y_pred_best = best_xgb.predict(X_test)\n",
    "print(f\"Test Accuracy with Best Model: {accuracy_score(y_test, y_pred_best):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbd0cdf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Regression: Forecasting House Prices\n",
    "Using the **Boston Housing dataset**, predict house prices based on features like crime rate and room count.\n",
    "\n",
    "Now, let‚Äôs predict house prices using the Boston Housing dataset.\n",
    "\n",
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54447daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load data\n",
    "boston = load_boston()\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "y = boston.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the regressor\n",
    "xgb_reg = xgb.XGBRegressor(objective='reg:squarederror')\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = xgb_reg.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e01343",
   "metadata": {},
   "source": [
    "**Sample Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a835795",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mean Squared Error: 9.52"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e4eea6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **4. Tools and Methods Summary**\n",
    "- **Modeling**: `xgboost.XGBClassifier`, `XGBRegressor`.\n",
    "- **Evaluation**: `sklearn.metrics.accuracy_score`, `mean_squared_error`.\n",
    "- **Tuning**: `sklearn.model_selection.GridSearchCV`, early stopping.\n",
    "- **Visualization**: `matplotlib.pyplot`, `xgboost.plot_importance`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a973af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Detailed evaluation\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "xgb.plot_importance(xgb_clf)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c59715",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
