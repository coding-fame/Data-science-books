
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Deep Learning: Interview &#8212; Data Science Books</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-dropdown.css?v=995e94df" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-bootstrap.min.css?v=21c0b90a" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=d567e03f" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'contents/6_interview/dl';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Data Science Books</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Part I ‚Äî Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../0_maths/0_essential.html">Essential Mathematics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_maths/4_linear_algebra.html">Linear Algebra</a></li>







<li class="toctree-l1"><a class="reference internal" href="../0_maths/2_probability.html">Probability Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_maths/1_descriptive.html">Descriptive Statistics</a></li>








<li class="toctree-l1"><a class="reference internal" href="../0_maths/3_inferential.html">Inferential Statistics</a></li>



<li class="toctree-l1"><a class="reference internal" href="../0_maths/5_calculus.html">Calculus</a></li>







<li class="toctree-l1"><a class="reference internal" href="../0_maths/6_regression_analysis.html">Explanatory and Response Variables</a></li>


<li class="toctree-l1"><a class="reference internal" href="../1_python/1_basics.html">Python Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/2_advanced.html">Advanced Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/3_data_structures.html">Data Structures</a></li>





<li class="toctree-l1"><a class="reference internal" href="../1_python/4_modules_packages.html">Modules &amp; Packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/5_functions.html">Functions &amp; Modular Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/6_oop.html">Object-Oriented Programming</a></li>





<li class="toctree-l1"><a class="reference internal" href="../1_python/8_exceptions.html">Exception Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/9_regex.html">Regular Expressions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_numpy/1_numpy.html">NumPy (<strong>Numerical Python</strong>)</a></li>



<li class="toctree-l1"><a class="reference internal" href="../2_pandas/1_series.html">Pandas Series</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_pandas/2_dataframes.html">Pandas DataFrame</a></li>
















<li class="toctree-l1"><a class="reference internal" href="../2_pandas/3_visualization.html"><strong>What is Data Visualization in Data Science?</strong></a></li>


<li class="toctree-l1"><a class="reference internal" href="../2_pandas/4_eda.html">Exploratory Data Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_pandas/5_feature_engineering.html"><strong>What is Feature Engineering?</strong></a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part II ‚Äî Classical Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../3_ml/1_foundations.html">ML Foundational</a></li>





<li class="toctree-l1"><a class="reference internal" href="../3_ml/2_data_preparation.html">2Ô∏è‚É£ Data Handling</a></li>





<li class="toctree-l1"><a class="reference internal" href="../3_ml/2_train_test_split.html">Train‚ÄìTest Split</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/5_model_evaluation.html">4Ô∏è‚É£ Model Evaluation</a></li>




<li class="toctree-l1"><a class="reference internal" href="../3_ml/11_supervised_learning.html"><strong>Supervised Learning</strong></a></li>



<li class="toctree-l1"><a class="reference internal" href="../3_ml/12_regression.html">Regression Algorithms</a></li>







<li class="toctree-l1"><a class="reference internal" href="../3_ml/13_classification.html">Classification Algorithms</a></li>

<li class="toctree-l1"><a class="reference internal" href="../3_ml/10_core_algo.html">Core ML Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/14_decision_trees.html">Decision Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/15_ensemble_methods.html">Ensemble Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/16_svm.html">Support Vector Machine (SVM) in Detail</a></li>


<li class="toctree-l1"><a class="reference internal" href="../3_ml/17_knn.html">k-Nearest Neighbors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/18_naive_bayes.html">Naive Bayes</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part III ‚Äî Advanced Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../3_ml/20_unsupervised_learning.html">Unsupervised Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../3_ml/21_clustering.html">Clustering Techniques</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/7_optimization_and_training.html">5Ô∏è‚É£ Optimization &amp; Training</a></li>







<li class="toctree-l1 has-children"><a class="reference internal" href="../3_ml/6_ml_lifecycle.html">ML Lifecycle</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../3_ml/6_training.html">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3_ml/6_evaluation.html">Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3_ml/6_deployment.html">Deployment</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part IV ‚Äî Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl1_Introduction.html">Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl2_Neuron.html">Neuron</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl3_Libraries.html">Deep Learning Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl4_Terminology.html">Terminology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl5_multi_layer.html">Multi-Layer Perceptron</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl6_first_nn.html">First Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl7_evaluating_model.html">Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl8_multiclass_classification.html">Multiclass Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl9_multiclass_classification_hand.html">Handwritten Digit Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl10_saving_and_loading.html">Saving &amp; Loading Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl11_checkpointing.html">Model Checkpointing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl12_visualizing_model_training.html"><strong>Visualizing Model Training History in Deep Learning</strong></a></li>

<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl13_loss_functions_activation_functions_and_optimizers.html">Loss Functions &amp; Optimizers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part V ‚Äî NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp1.html">NLP Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp2.html">Text Cleaning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp3.html">Text Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp4.html">NLP Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp5.html">Bag of Words, TF-IDF</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp6.html">Sentiment Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp7.html">NLP with SpaCy</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part VI ‚Äî Career &amp; MLOps</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="self%20introduction.html">Self Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="py.html">Python: Interview Guide</a></li>





<li class="toctree-l1"><a class="reference internal" href="pd.html">üìö Pandas: Interview Guide</a></li>


<li class="toctree-l1"><a class="reference internal" href="ml.html">Machine Learning: Interview Guide</a></li>













<li class="toctree-l1"><a class="reference internal" href="git.html">Git</a></li>
<li class="toctree-l1"><a class="reference internal" href="dvc.html">DVC</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlflow.html">MLflow</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/coding-fame/Data-science-books" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/coding-fame/Data-science-books/edit/main/contents/6_interview/dl.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/coding-fame/Data-science-books/issues/new?title=Issue%20on%20page%20%2Fcontents/6_interview/dl.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/contents/6_interview/dl.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Deep Learning: Interview</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Deep Learning: Interview</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-deep-learning-differs-from-machine-learning">1. How Deep Learning Differs from Machine Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-differences">Key Differences:</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-advantage">Deep Learning Advantage:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-deep-neural-network">2. What is a Deep Neural Network?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-a-transfer-function-needed-in-deep-learning">3. Why is a Transfer Function Needed in Deep Learning?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid-vs-tanh-activation-function">4. Sigmoid vs. Tanh Activation Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-the-softmax-function-used-in-the-output-layer">5. Why is the Softmax Function Used in the Output Layer?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-decide-batch-size-in-deep-learning">6. How to Decide Batch Size in Deep Learning?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#difference-between-epoch-and-iteration">7. Difference Between Epoch and Iteration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-set-the-number-of-neurons-in-input-output-layers">8. How to Set the Number of Neurons in Input &amp; Output Layers?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-layer"><strong>Input Layer:</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#output-layer"><strong>Output Layer:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-set-the-number-of-neurons-in-hidden-layers">9. How to Set the Number of Neurons in Hidden Layers?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-dropout-and-why-is-it-useful">10. What is Dropout and Why is it Useful?</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-a-structured-guide-for-developers">Deep Learning: A Structured Guide for Developers</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-early-stopping">11. What is Early Stopping?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#purpose"><strong>Purpose:</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-it-works"><strong>How It Works:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-data-augmentation">12. What is Data Augmentation?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1"><strong>Purpose:</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-use-case"><strong>Example Use Case:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-data-normalization">13. What is Data Normalization?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2"><strong>Purpose:</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3"><strong>How It Works:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#can-we-initialize-all-weights-with-zero-if-not-why">14. Can We Initialize All Weights with Zero? If Not, Why?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#answer-no-initializing-all-weights-with-zero-is-a-bad-practice"><strong>Answer:</strong> No, initializing all weights with zero is a bad practice.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why"><strong>Why?</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#good-weight-initialization-methods">15. Good Weight Initialization Methods</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#commonly-used-methods"><strong>Commonly Used Methods:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-can-loss-become-nan-during-training">16. Why Can Loss Become NaN During Training?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-reasons"><strong>Common Reasons:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-the-hyperparameters-of-a-neural-network">17. What Are the Hyperparameters of a Neural Network?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-train-a-deep-neural-network">18. How Do We Train a Deep Neural Network?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#process"><strong>Process:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-prevent-overfitting-in-deep-neural-networks">19. How to Prevent Overfitting in Deep Neural Networks?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#effective-methods"><strong>Effective Methods:</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-guide-for-developers">Deep Learning Guide for Developers</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-gradient-descent-and-is-it-a-first-order-method">21. What is Gradient Descent, and is it a First-Order Method?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-the-gradient-descent-method-work">22. How Does the Gradient Descent Method Work?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-happens-when-the-learning-rate-is-too-small-or-too-large">23. What Happens When the Learning Rate is Too Small or Too Large?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-need-for-gradient-checking">24. What is the Need for Gradient Checking?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-numerical-and-analytical-gradients">25. What are Numerical and Analytical Gradients?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-difference-between-convex-and-non-convex-functions">26. What is the Difference Between Convex and Non-Convex Functions?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-stochastic-gradient-descent-sgd">27. Why Do We Need Stochastic Gradient Descent (SGD)?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-stochastic-gradient-descent-work">28. How Does Stochastic Gradient Descent Work?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-mini-batch-gradient-descent-work">29. How Does Mini-Batch Gradient Descent Work?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#difference-between-gradient-descent-stochastic-gradient-descent-and-mini-batch-gradient-descent">30. Difference Between Gradient Descent, Stochastic Gradient Descent, and Mini-Batch Gradient Descent</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#adaptive-methods-of-gradient-descent-convolutional-neural-networks-cnn">Adaptive Methods of Gradient Descent &amp; Convolutional Neural Networks (CNN)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-some-of-the-adaptive-methods-of-gradient-descent">31. What are some of the adaptive methods of gradient descent?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-can-we-set-the-learning-rate-adaptively">32. How can we set the learning rate adaptively?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#can-we-get-rid-of-the-learning-rate">33. Can we get rid of the learning rate?</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-neural-networks-cnn">Convolutional Neural Networks (CNN)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-cnn-preferred-for-image-data">34. Why is CNN preferred for image data?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-the-different-layers-used-in-cnn">35. What are the different layers used in CNN?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explain-the-convolution-operation">36. Explain the convolution operation.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-a-pooling-layer">37. Why do we need a pooling layer?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-the-different-types-of-pooling">38. What are the different types of pooling?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explain-the-working-of-cnn">39. Explain the working of CNN.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explain-the-architecture-of-lenet">40. Explain the architecture of LeNet.</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-concepts-cnn-rnn-lstm-and-more">Deep Learning Concepts: CNN, RNN, LSTM, and More</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-the-drawbacks-of-cnn">41. What are the Drawbacks of CNN?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#answer">Answer:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-rnn-useful">42. Why is RNN Useful?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Answer:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-prefer-a-recurrent-network-over-a-feedforward-network">43. When to Prefer a Recurrent Network Over a Feedforward Network?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Answer:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-lstm-differ-from-rnn">44. How Does LSTM Differ from RNN?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Answer:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-are-the-cell-state-and-hidden-state-used-in-lstm">45. How Are the Cell State and Hidden State Used in LSTM?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Answer:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-gated-recurrent-units-gru">46. Why Do We Need Gated Recurrent Units (GRU)?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Answer:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#difference-between-discriminative-and-generative-models">47. Difference Between Discriminative and Generative Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Answer:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#difference-between-autoencoders-and-pca">48. Difference Between Autoencoders and PCA</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Answer:</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="deep-learning-interview">
<h1>Deep Learning: Interview<a class="headerlink" href="#deep-learning-interview" title="Link to this heading">#</a></h1>
<section id="how-deep-learning-differs-from-machine-learning">
<h2>1. How Deep Learning Differs from Machine Learning<a class="headerlink" href="#how-deep-learning-differs-from-machine-learning" title="Link to this heading">#</a></h2>
<section id="key-differences">
<h3>Key Differences:<a class="headerlink" href="#key-differences" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>In <strong>Machine Learning</strong>, we manually design and engineer the right set of features for the model.</p></li>
<li><p><strong>Feature Engineering</strong> is crucial for the success of a Machine Learning model.</p></li>
<li><p>However, it is challenging to engineer features for unstructured data (e.g., text, images).</p></li>
</ul>
<section id="deep-learning-advantage">
<h4>Deep Learning Advantage:<a class="headerlink" href="#deep-learning-advantage" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Deep Learning</strong> eliminates the need for manual feature engineering.</p></li>
<li><p>Deep neural networks, with multiple hidden layers, learn and extract features automatically.</p></li>
<li><p>This makes Deep Learning highly effective for <strong>image recognition, text classification, and other complex tasks</strong>.</p></li>
</ul>
</section>
</section>
</section>
<hr class="docutils" />
<section id="what-is-a-deep-neural-network">
<h2>2. What is a Deep Neural Network?<a class="headerlink" href="#what-is-a-deep-neural-network" title="Link to this heading">#</a></h2>
<p>A <strong>Deep Neural Network (DNN)</strong> is an Artificial Neural Network (ANN) with:</p>
<ul class="simple">
<li><p><strong>One input layer</strong></p></li>
<li><p><strong>Multiple hidden layers (N hidden layers)</strong></p></li>
<li><p><strong>One output layer</strong></p></li>
</ul>
<p>The presence of multiple hidden layers distinguishes it as a ‚Äúdeep‚Äù neural network.</p>
</section>
<hr class="docutils" />
<section id="why-is-a-transfer-function-needed-in-deep-learning">
<h2>3. Why is a Transfer Function Needed in Deep Learning?<a class="headerlink" href="#why-is-a-transfer-function-needed-in-deep-learning" title="Link to this heading">#</a></h2>
<p>A <strong>Transfer Function</strong>, also known as an <strong>Activation Function</strong>, is crucial because:</p>
<ul class="simple">
<li><p>It introduces <strong>non-linearity</strong> to the neural network.</p></li>
<li><p>It enables the network to learn <strong>complex patterns</strong> from data.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="sigmoid-vs-tanh-activation-function">
<h2>4. Sigmoid vs. Tanh Activation Function<a class="headerlink" href="#sigmoid-vs-tanh-activation-function" title="Link to this heading">#</a></h2>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Activation Function</p></th>
<th class="head"><p>Output Range</p></th>
<th class="head"><p>Centered At</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Sigmoid</strong></p></td>
<td><p>0 to 1</p></td>
<td><p>0.5</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Tanh</strong></p></td>
<td><p>-1 to 1</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<hr class="docutils" />
<section id="why-is-the-softmax-function-used-in-the-output-layer">
<h2>5. Why is the Softmax Function Used in the Output Layer?<a class="headerlink" href="#why-is-the-softmax-function-used-in-the-output-layer" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>The <strong>Softmax function</strong> converts inputs into a probability distribution (values between 0 and 1).</p></li>
<li><p>It generalizes the <strong>Sigmoid function</strong> for multi-class classification.</p></li>
<li><p>In classification tasks, Softmax provides the <strong>probability of each class being the output</strong>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="how-to-decide-batch-size-in-deep-learning">
<h2>6. How to Decide Batch Size in Deep Learning?<a class="headerlink" href="#how-to-decide-batch-size-in-deep-learning" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Batch size is typically set to <strong>powers of 2</strong> (e.g., 32, 64, 128) based on <strong>CPU/GPU memory availability</strong>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="difference-between-epoch-and-iteration">
<h2>7. Difference Between Epoch and Iteration<a class="headerlink" href="#difference-between-epoch-and-iteration" title="Link to this heading">#</a></h2>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Term</p></th>
<th class="head"><p>Definition</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Iteration</strong></p></td>
<td><p>One batch of data passes through the network once.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Epoch</strong></p></td>
<td><p>The entire dataset passes through the network once.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<hr class="docutils" />
<section id="how-to-set-the-number-of-neurons-in-input-output-layers">
<h2>8. How to Set the Number of Neurons in Input &amp; Output Layers?<a class="headerlink" href="#how-to-set-the-number-of-neurons-in-input-output-layers" title="Link to this heading">#</a></h2>
<section id="input-layer">
<h3><strong>Input Layer:</strong><a class="headerlink" href="#input-layer" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The number of neurons = <strong>number of input features</strong>.</p></li>
</ul>
</section>
<section id="output-layer">
<h3><strong>Output Layer:</strong><a class="headerlink" href="#output-layer" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>For Regression</strong> ‚Üí <strong>1 neuron</strong>.</p></li>
<li><p><strong>For Classification</strong> ‚Üí <strong>Number of classes</strong> (each neuron represents class probability).</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="how-to-set-the-number-of-neurons-in-hidden-layers">
<h2>9. How to Set the Number of Neurons in Hidden Layers?<a class="headerlink" href="#how-to-set-the-number-of-neurons-in-hidden-layers" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>No strict rule, but general guidelines:</p>
<ul>
<li><p>Number of hidden neurons should be <strong>between the size of input and output layers</strong>.</p></li>
<li><p>A common heuristic: <strong>(2/3 * input neurons) + output neurons</strong>.</p></li>
<li><p>Should be <strong>less than twice the size of the input layer</strong>.</p></li>
</ul>
</li>
<li><p><strong>For simple problems</strong> ‚Üí 1-2 hidden layers.</p></li>
<li><p><strong>For complex problems</strong> ‚Üí Deeper networks with multiple hidden layers.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="what-is-dropout-and-why-is-it-useful">
<h2>10. What is Dropout and Why is it Useful?<a class="headerlink" href="#what-is-dropout-and-why-is-it-useful" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Dropout</strong> is a regularization technique where some neurons are <strong>randomly ignored</strong> during training.</p></li>
<li><p>Helps prevent <strong>overfitting</strong> by ensuring the model does not become too dependent on specific neurons.</p></li>
<li><p>Leads to a more <strong>generalized</strong> and <strong>robust</strong> neural network.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="deep-learning-a-structured-guide-for-developers">
<h1>Deep Learning: A Structured Guide for Developers<a class="headerlink" href="#deep-learning-a-structured-guide-for-developers" title="Link to this heading">#</a></h1>
<section id="what-is-early-stopping">
<h2>11. What is Early Stopping?<a class="headerlink" href="#what-is-early-stopping" title="Link to this heading">#</a></h2>
<section id="purpose">
<h3><strong>Purpose:</strong><a class="headerlink" href="#purpose" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Early stopping is a technique used to <strong>prevent overfitting</strong>.</p></li>
<li><p>It stops the training process before the model starts memorizing noise in the data.</p></li>
</ul>
</section>
<section id="how-it-works">
<h3><strong>How It Works:</strong><a class="headerlink" href="#how-it-works" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The model‚Äôs performance is monitored on a <strong>validation set</strong>.</p></li>
<li><p>If the validation performance <strong>stops improving</strong>, training is halted.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="what-is-data-augmentation">
<h2>12. What is Data Augmentation?<a class="headerlink" href="#what-is-data-augmentation" title="Link to this heading">#</a></h2>
<section id="id1">
<h3><strong>Purpose:</strong><a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Data augmentation artificially increases the training dataset.</p></li>
</ul>
</section>
<section id="example-use-case">
<h3><strong>Example Use Case:</strong><a class="headerlink" href="#example-use-case" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>In <strong>image classification</strong>, if the dataset is small, data augmentation can generate variations by:</p>
<ul>
<li><p><strong>Cropping</strong></p></li>
<li><p><strong>Flipping</strong></p></li>
<li><p><strong>Padding</strong></p></li>
<li><p><strong>Rotation &amp; Scaling</strong></p></li>
</ul>
</li>
</ul>
<p>This improves model generalization and reduces overfitting.</p>
</section>
</section>
<hr class="docutils" />
<section id="what-is-data-normalization">
<h2>13. What is Data Normalization?<a class="headerlink" href="#what-is-data-normalization" title="Link to this heading">#</a></h2>
<section id="id2">
<h3><strong>Purpose:</strong><a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Data normalization is a <strong>preprocessing step</strong> to improve training efficiency.</p></li>
<li><p>It helps the model converge faster by scaling data appropriately.</p></li>
</ul>
</section>
<section id="id3">
<h3><strong>How It Works:</strong><a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Each data point is <strong>subtracted by its mean</strong> and <strong>divided by its standard deviation</strong>.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="can-we-initialize-all-weights-with-zero-if-not-why">
<h2>14. Can We Initialize All Weights with Zero? If Not, Why?<a class="headerlink" href="#can-we-initialize-all-weights-with-zero-if-not-why" title="Link to this heading">#</a></h2>
<section id="answer-no-initializing-all-weights-with-zero-is-a-bad-practice">
<h3><strong>Answer:</strong> No, initializing all weights with zero is a bad practice.<a class="headerlink" href="#answer-no-initializing-all-weights-with-zero-is-a-bad-practice" title="Link to this heading">#</a></h3>
</section>
<section id="why">
<h3><strong>Why?</strong><a class="headerlink" href="#why" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>During <strong>backpropagation</strong>, the network learns by updating weights using gradients.</p></li>
<li><p>If all weights are initialized to zero:</p>
<ul>
<li><p><strong>All neurons will learn the same feature.</strong></p></li>
<li><p>The network will <strong>fail to learn complex patterns</strong>.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="good-weight-initialization-methods">
<h2>15. Good Weight Initialization Methods<a class="headerlink" href="#good-weight-initialization-methods" title="Link to this heading">#</a></h2>
<section id="commonly-used-methods">
<h3><strong>Commonly Used Methods:</strong><a class="headerlink" href="#commonly-used-methods" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Random Initialization</strong></p></li>
<li><p><strong>Xavier Initialization</strong> (suitable for sigmoid/tanh activations)</p></li>
<li><p><strong>He Initialization</strong> (suitable for ReLU activations)</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="why-can-loss-become-nan-during-training">
<h2>16. Why Can Loss Become NaN During Training?<a class="headerlink" href="#why-can-loss-become-nan-during-training" title="Link to this heading">#</a></h2>
<section id="common-reasons">
<h3><strong>Common Reasons:</strong><a class="headerlink" href="#common-reasons" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>High learning rate</strong> ‚Üí Causes unstable weight updates.</p></li>
<li><p><strong>Gradient explosion</strong> ‚Üí Causes values to go out of bounds.</p></li>
<li><p><strong>Improper loss function</strong> ‚Üí Results in undefined calculations.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="what-are-the-hyperparameters-of-a-neural-network">
<h2>17. What Are the Hyperparameters of a Neural Network?<a class="headerlink" href="#what-are-the-hyperparameters-of-a-neural-network" title="Link to this heading">#</a></h2>
<p>Hyperparameters control how the network is trained. Some key hyperparameters include:</p>
<ul class="simple">
<li><p><strong>Number of neurons in hidden layers</strong></p></li>
<li><p><strong>Number of hidden layers</strong></p></li>
<li><p><strong>Activation function per layer</strong></p></li>
<li><p><strong>Weight initialization method</strong></p></li>
<li><p><strong>Learning rate</strong></p></li>
<li><p><strong>Number of epochs</strong></p></li>
<li><p><strong>Batch size</strong></p></li>
</ul>
</section>
<hr class="docutils" />
<section id="how-do-we-train-a-deep-neural-network">
<h2>18. How Do We Train a Deep Neural Network?<a class="headerlink" href="#how-do-we-train-a-deep-neural-network" title="Link to this heading">#</a></h2>
<section id="process">
<h3><strong>Process:</strong><a class="headerlink" href="#process" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Forward Propagation</strong> ‚Üí Computes predictions.</p></li>
<li><p><strong>Backpropagation</strong> ‚Üí Adjusts weights using gradients.</p></li>
<li><p><strong>Optimization</strong> ‚Üí Uses techniques like <strong>Gradient Descent</strong> to minimize loss.</p></li>
</ol>
</section>
</section>
<hr class="docutils" />
<section id="how-to-prevent-overfitting-in-deep-neural-networks">
<h2>19. How to Prevent Overfitting in Deep Neural Networks?<a class="headerlink" href="#how-to-prevent-overfitting-in-deep-neural-networks" title="Link to this heading">#</a></h2>
<section id="effective-methods">
<h3><strong>Effective Methods:</strong><a class="headerlink" href="#effective-methods" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Dropout</strong> ‚Üí Randomly ignores neurons during training.</p></li>
<li><p><strong>Early Stopping</strong> ‚Üí Stops training when validation loss stops improving.</p></li>
<li><p><strong>Regularization (L1/L2)</strong> ‚Üí Prevents excessive weight updates.</p></li>
<li><p><strong>Data Augmentation</strong> ‚Üí Increases dataset diversity.</p></li>
</ul>
</section>
</section>
</section>
<hr class="docutils" />
<section id="deep-learning-guide-for-developers">
<h1>Deep Learning Guide for Developers<a class="headerlink" href="#deep-learning-guide-for-developers" title="Link to this heading">#</a></h1>
<section id="what-is-gradient-descent-and-is-it-a-first-order-method">
<h2>21. What is Gradient Descent, and is it a First-Order Method?<a class="headerlink" href="#what-is-gradient-descent-and-is-it-a-first-order-method" title="Link to this heading">#</a></h2>
<p><strong>Answer:</strong></p>
<ul class="simple">
<li><p>Gradient descent is one of the most popular and widely used optimization algorithms for training neural networks.</p></li>
<li><p>Yes, gradient descent is a first-order optimization method because it calculates only the first-order derivative.</p></li>
</ul>
</section>
<section id="how-does-the-gradient-descent-method-work">
<h2>22. How Does the Gradient Descent Method Work?<a class="headerlink" href="#how-does-the-gradient-descent-method-work" title="Link to this heading">#</a></h2>
<p><strong>Answer:</strong></p>
<ul>
<li><p>Gradient descent is an optimization method used for training neural networks.</p></li>
<li><p>First, we compute the derivatives of the loss function with respect to the weights of the network.</p></li>
<li><p>Then, we update the weights of the network using the following update rule:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>Weight = Weight - Learning Rate √ó Derivatives
</pre></div>
</div>
</li>
</ul>
</section>
<section id="what-happens-when-the-learning-rate-is-too-small-or-too-large">
<h2>23. What Happens When the Learning Rate is Too Small or Too Large?<a class="headerlink" href="#what-happens-when-the-learning-rate-is-too-small-or-too-large" title="Link to this heading">#</a></h2>
<p><strong>Answer:</strong></p>
<ul class="simple">
<li><p><strong>Small Learning Rate:</strong> The model takes very small steps, slowing down convergence.</p></li>
<li><p><strong>Large Learning Rate:</strong> The model takes very large steps, which may cause it to overshoot and miss the global minimum.</p></li>
</ul>
</section>
<section id="what-is-the-need-for-gradient-checking">
<h2>24. What is the Need for Gradient Checking?<a class="headerlink" href="#what-is-the-need-for-gradient-checking" title="Link to this heading">#</a></h2>
<p><strong>Answer:</strong></p>
<ul class="simple">
<li><p>Gradient checking is used to debug the gradient descent algorithm and ensure its correct implementation.</p></li>
<li><p>When implementing gradient descent for a complex neural network, a buggy implementation may still allow the network to learn something, but not optimally.</p></li>
<li><p>To ensure correctness, we use gradient checking.</p></li>
</ul>
</section>
<section id="what-are-numerical-and-analytical-gradients">
<h2>25. What are Numerical and Analytical Gradients?<a class="headerlink" href="#what-are-numerical-and-analytical-gradients" title="Link to this heading">#</a></h2>
<p><strong>Answer:</strong></p>
<ul class="simple">
<li><p><strong>Analytical Gradients:</strong> Calculated through backpropagation.</p></li>
<li><p><strong>Numerical Gradients:</strong> Approximations to the gradients.</p></li>
</ul>
</section>
<section id="what-is-the-difference-between-convex-and-non-convex-functions">
<h2>26. What is the Difference Between Convex and Non-Convex Functions?<a class="headerlink" href="#what-is-the-difference-between-convex-and-non-convex-functions" title="Link to this heading">#</a></h2>
<p><strong>Answer:</strong></p>
<ul class="simple">
<li><p><strong>Convex Function:</strong> Has only one minimum value.</p></li>
<li><p><strong>Non-Convex Function:</strong> Has multiple minimum values.</p></li>
</ul>
</section>
<section id="why-do-we-need-stochastic-gradient-descent-sgd">
<h2>27. Why Do We Need Stochastic Gradient Descent (SGD)?<a class="headerlink" href="#why-do-we-need-stochastic-gradient-descent-sgd" title="Link to this heading">#</a></h2>
<p><strong>Answer:</strong></p>
<ul class="simple">
<li><p>In standard gradient descent, we update the model parameters only after iterating through all the data points in the training set.</p></li>
<li><p>If the dataset contains millions of data points, even a single parameter update requires iterating through all of them, making training time-consuming.</p></li>
<li><p>Stochastic Gradient Descent (SGD) helps overcome this limitation.</p></li>
</ul>
</section>
<section id="how-does-stochastic-gradient-descent-work">
<h2>28. How Does Stochastic Gradient Descent Work?<a class="headerlink" href="#how-does-stochastic-gradient-descent-work" title="Link to this heading">#</a></h2>
<p><strong>Answer:</strong></p>
<ul class="simple">
<li><p>Unlike batch gradient descent, SGD updates model parameters after processing each individual data point.</p></li>
<li><p>This speeds up training but introduces more variance in updates.</p></li>
</ul>
</section>
<section id="how-does-mini-batch-gradient-descent-work">
<h2>29. How Does Mini-Batch Gradient Descent Work?<a class="headerlink" href="#how-does-mini-batch-gradient-descent-work" title="Link to this heading">#</a></h2>
<p><strong>Answer:</strong></p>
<ul class="simple">
<li><p>Mini-batch gradient descent updates the model parameters after processing a small batch of <strong>n</strong> data points instead of the whole dataset.</p></li>
<li><p>Example: If <strong>n = 32</strong>, the parameters are updated after processing every <strong>32 data points</strong>.</p></li>
</ul>
</section>
<section id="difference-between-gradient-descent-stochastic-gradient-descent-and-mini-batch-gradient-descent">
<h2>30. Difference Between Gradient Descent, Stochastic Gradient Descent, and Mini-Batch Gradient Descent<a class="headerlink" href="#difference-between-gradient-descent-stochastic-gradient-descent-and-mini-batch-gradient-descent" title="Link to this heading">#</a></h2>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Parameter Update Frequency</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Gradient Descent</strong></p></td>
<td><p>After iterating through all data points.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Stochastic Gradient Descent</strong></p></td>
<td><p>After processing each single data point.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Mini-Batch Gradient Descent</strong></p></td>
<td><p>After processing a batch of data points.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<hr class="docutils" />
<section id="adaptive-methods-of-gradient-descent-convolutional-neural-networks-cnn">
<h1>Adaptive Methods of Gradient Descent &amp; Convolutional Neural Networks (CNN)<a class="headerlink" href="#adaptive-methods-of-gradient-descent-convolutional-neural-networks-cnn" title="Link to this heading">#</a></h1>
<section id="what-are-some-of-the-adaptive-methods-of-gradient-descent">
<h2>31. What are some of the adaptive methods of gradient descent?<a class="headerlink" href="#what-are-some-of-the-adaptive-methods-of-gradient-descent" title="Link to this heading">#</a></h2>
<p><strong>Answer:</strong>
Adaptive methods of gradient descent include:</p>
<ul class="simple">
<li><p><strong>Adagrad</strong></p></li>
<li><p><strong>Adadelta</strong></p></li>
<li><p><strong>RMSProp</strong></p></li>
<li><p><strong>Adam</strong></p></li>
<li><p><strong>Adamax</strong></p></li>
<li><p><strong>AMSGrad</strong></p></li>
<li><p><strong>Nadam</strong></p></li>
</ul>
</section>
<hr class="docutils" />
<section id="how-can-we-set-the-learning-rate-adaptively">
<h2>32. How can we set the learning rate adaptively?<a class="headerlink" href="#how-can-we-set-the-learning-rate-adaptively" title="Link to this heading">#</a></h2>
<p><strong>Answer:</strong>
We can set the learning rate adaptively using <strong>Adagrad</strong>:</p>
<ul class="simple">
<li><p>Assign a <strong>high learning rate</strong> when the previous gradient value is <strong>low</strong>.</p></li>
<li><p>Assign a <strong>low learning rate</strong> when the previous gradient value is <strong>high</strong>.</p></li>
</ul>
<p>This allows the learning rate to adjust dynamically based on past gradient updates.</p>
</section>
<hr class="docutils" />
<section id="can-we-get-rid-of-the-learning-rate">
<h2>33. Can we get rid of the learning rate?<a class="headerlink" href="#can-we-get-rid-of-the-learning-rate" title="Link to this heading">#</a></h2>
<p><strong>Answer:</strong>
Yes, we can eliminate the learning rate by using <strong>Adadelta</strong>.</p>
</section>
</section>
<hr class="docutils" />
<section id="convolutional-neural-networks-cnn">
<h1>Convolutional Neural Networks (CNN)<a class="headerlink" href="#convolutional-neural-networks-cnn" title="Link to this heading">#</a></h1>
<section id="why-is-cnn-preferred-for-image-data">
<h2>34. Why is CNN preferred for image data?<a class="headerlink" href="#why-is-cnn-preferred-for-image-data" title="Link to this heading">#</a></h2>
<p><strong>Answer:</strong>
CNNs use a special operation called <strong>convolution</strong>, which extracts important features from images. Since convolutional layers focus on feature extraction, <strong>CNNs achieve higher accuracy compared to other algorithms for image data.</strong></p>
</section>
<hr class="docutils" />
<section id="what-are-the-different-layers-used-in-cnn">
<h2>35. What are the different layers used in CNN?<a class="headerlink" href="#what-are-the-different-layers-used-in-cnn" title="Link to this heading">#</a></h2>
<p><strong>Answer:</strong>
CNNs primarily use the following layers:</p>
<ul class="simple">
<li><p><strong>Convolutional Layer</strong> (Extracts features from the input image)</p></li>
<li><p><strong>Pooling Layer</strong> (Reduces the dimensionality of feature maps)</p></li>
<li><p><strong>Fully Connected Layer</strong> (Performs the classification task)</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="explain-the-convolution-operation">
<h2>36. Explain the convolution operation.<a class="headerlink" href="#explain-the-convolution-operation" title="Link to this heading">#</a></h2>
<p><strong>Answer:</strong></p>
<ul class="simple">
<li><p>The <strong>input matrix</strong> is processed using a smaller <strong>filter (kernel) matrix</strong>.</p></li>
<li><p>The <strong>filter slides over the input matrix</strong> by a certain number of pixels (stride).</p></li>
<li><p><strong>Element-wise multiplication</strong> is performed between the filter and corresponding input values.</p></li>
<li><p>The <strong>sum of these multiplications</strong> produces a <strong>single output value</strong>.</p></li>
<li><p>This operation is repeated across the input matrix to generate a <strong>feature map</strong>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="why-do-we-need-a-pooling-layer">
<h2>37. Why do we need a pooling layer?<a class="headerlink" href="#why-do-we-need-a-pooling-layer" title="Link to this heading">#</a></h2>
<p><strong>Answer:</strong></p>
<ul class="simple">
<li><p>The activation map obtained after the convolution operation <strong>has a large dimension</strong>.</p></li>
<li><p>To <strong>reduce its size</strong> and retain important features, we use <strong>pooling layers</strong>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="what-are-the-different-types-of-pooling">
<h2>38. What are the different types of pooling?<a class="headerlink" href="#what-are-the-different-types-of-pooling" title="Link to this heading">#</a></h2>
<p><strong>Answer:</strong>
The different types of pooling include:</p>
<ul class="simple">
<li><p><strong>Max Pooling</strong> (Takes the maximum value in each window)</p></li>
<li><p><strong>Average Pooling</strong> (Takes the average value in each window)</p></li>
<li><p><strong>Sum Pooling</strong> (Sums up all values in each window)</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="explain-the-working-of-cnn">
<h2>39. Explain the working of CNN.<a class="headerlink" href="#explain-the-working-of-cnn" title="Link to this heading">#</a></h2>
<p><strong>Answer:</strong>
Let‚Äôs consider an <strong>image classification task</strong>:</p>
<ol class="arabic simple">
<li><p>The <strong>image is fed as input</strong> to the network.</p></li>
<li><p>A <strong>convolution operation</strong> is performed to extract important features.</p></li>
<li><p>The <strong>feature map</strong> generated is passed through <strong>pooling layers</strong> to reduce dimensionality.</p></li>
<li><p>Finally, the feature map is <strong>flattened</strong> and fed into the <strong>fully connected layer</strong>, which performs the classification.</p></li>
</ol>
</section>
<hr class="docutils" />
<section id="explain-the-architecture-of-lenet">
<h2>40. Explain the architecture of LeNet.<a class="headerlink" href="#explain-the-architecture-of-lenet" title="Link to this heading">#</a></h2>
<p><strong>Answer:</strong>
The <strong>LeNet</strong> architecture consists of <strong>seven layers</strong>:</p>
<ul class="simple">
<li><p><strong>Three Convolutional Layers</strong></p></li>
<li><p><strong>Two Pooling Layers</strong></p></li>
<li><p><strong>One Fully Connected Layer</strong></p></li>
<li><p><strong>One Output Layer</strong></p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="deep-learning-concepts-cnn-rnn-lstm-and-more">
<h1>Deep Learning Concepts: CNN, RNN, LSTM, and More<a class="headerlink" href="#deep-learning-concepts-cnn-rnn-lstm-and-more" title="Link to this heading">#</a></h1>
<section id="what-are-the-drawbacks-of-cnn">
<h2>41. What are the Drawbacks of CNN?<a class="headerlink" href="#what-are-the-drawbacks-of-cnn" title="Link to this heading">#</a></h2>
<section id="answer">
<h3>Answer:<a class="headerlink" href="#answer" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>CNN is <strong>translation-invariant</strong>, which makes it prone to <strong>misclassification</strong>.</p></li>
<li><p>For example, in a <strong>face recognition</strong> task, CNN detects facial features like <strong>eyes, nose, mouth, and ears</strong>, but <strong>does not check their correct placement</strong>.</p></li>
<li><p>If all facial features are present, CNN may classify the image as a face <strong>regardless of their arrangement</strong>.</p></li>
<li><p>This limitation is a major drawback of CNN.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="why-is-rnn-useful">
<h2>42. Why is RNN Useful?<a class="headerlink" href="#why-is-rnn-useful" title="Link to this heading">#</a></h2>
<section id="id4">
<h3>Answer:<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>RNN (Recurrent Neural Network)</strong> uses a <strong>hidden state</strong> as memory to store past information.</p></li>
<li><p>It is widely used in <strong>sequential tasks</strong> such as:</p>
<ul>
<li><p><strong>Text generation</strong></p></li>
<li><p><strong>Time series prediction</strong></p></li>
<li><p><strong>Speech recognition</strong></p></li>
<li><p><strong>Machine translation</strong></p></li>
</ul>
</li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="when-to-prefer-a-recurrent-network-over-a-feedforward-network">
<h2>43. When to Prefer a Recurrent Network Over a Feedforward Network?<a class="headerlink" href="#when-to-prefer-a-recurrent-network-over-a-feedforward-network" title="Link to this heading">#</a></h2>
<section id="id5">
<h3>Answer:<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Recurrent networks are preferred when dealing with <strong>sequential tasks</strong>.</p></li>
<li><p>Unlike <strong>feedforward networks</strong>, RNNs store past information in the <strong>hidden state</strong>.</p></li>
<li><p>This makes RNNs highly effective for tasks involving sequences, such as <strong>language modeling</strong> and <strong>time-dependent predictions</strong>.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="how-does-lstm-differ-from-rnn">
<h2>44. How Does LSTM Differ from RNN?<a class="headerlink" href="#how-does-lstm-differ-from-rnn" title="Link to this heading">#</a></h2>
<section id="id6">
<h3>Answer:<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>LSTM (Long Short-Term Memory)</strong> introduces three special gates:</p>
<ul>
<li><p><strong>Input Gate</strong>: Controls new information added to memory.</p></li>
<li><p><strong>Forget Gate</strong>: Decides what information to discard.</p></li>
<li><p><strong>Output Gate</strong>: Determines what part of the memory is used as output.</p></li>
</ul>
</li>
<li><p>These gates help LSTM <strong>overcome vanishing gradient issues</strong>, making it superior to standard RNNs for long-term dependencies.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="how-are-the-cell-state-and-hidden-state-used-in-lstm">
<h2>45. How Are the Cell State and Hidden State Used in LSTM?<a class="headerlink" href="#how-are-the-cell-state-and-hidden-state-used-in-lstm" title="Link to this heading">#</a></h2>
<section id="id7">
<h3>Answer:<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Cell State</strong>: Stores long-term information (<strong>internal memory</strong>).</p></li>
<li><p><strong>Hidden State</strong>: Used for computing the <strong>current output</strong>.</p></li>
<li><p>These components enable LSTMs to <strong>retain and manipulate past information effectively</strong>.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="why-do-we-need-gated-recurrent-units-gru">
<h2>46. Why Do We Need Gated Recurrent Units (GRU)?<a class="headerlink" href="#why-do-we-need-gated-recurrent-units-gru" title="Link to this heading">#</a></h2>
<section id="id8">
<h3>Answer:<a class="headerlink" href="#id8" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>A major issue with <strong>LSTM</strong> is that it has <strong>too many parameters</strong> due to multiple gates and states.</p></li>
<li><p>This leads to <strong>increased training time</strong>.</p></li>
<li><p><strong>GRU (Gated Recurrent Unit)</strong> is a <strong>simplified</strong> version of LSTM:</p>
<ul>
<li><p>Uses <strong>fewer parameters</strong>.</p></li>
<li><p><strong>Faster training</strong> while maintaining performance.</p></li>
</ul>
</li>
<li><p>GRUs are preferred for applications where <strong>training efficiency</strong> is critical.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="difference-between-discriminative-and-generative-models">
<h2>47. Difference Between Discriminative and Generative Models<a class="headerlink" href="#difference-between-discriminative-and-generative-models" title="Link to this heading">#</a></h2>
<section id="id9">
<h3>Answer:<a class="headerlink" href="#id9" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Discriminative Model</p></th>
<th class="head"><p>Generative Model</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Approach</p></td>
<td><p>Learns <strong>decision boundary</strong> between classes</p></td>
<td><p>Learns the <strong>characteristics</strong> of each class</p></td>
</tr>
<tr class="row-odd"><td><p>Example Models</p></td>
<td><p>Logistic Regression, SVM, Random Forest</p></td>
<td><p>GANs, Na√Øve Bayes, Variational Autoencoders</p></td>
</tr>
<tr class="row-even"><td><p>Usage</p></td>
<td><p>Classification tasks</p></td>
<td><p>Data generation, classification</p></td>
</tr>
<tr class="row-odd"><td><p>Example Task</p></td>
<td><p>Spam detection</p></td>
<td><p>Image synthesis</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<hr class="docutils" />
<section id="difference-between-autoencoders-and-pca">
<h2>48. Difference Between Autoencoders and PCA<a class="headerlink" href="#difference-between-autoencoders-and-pca" title="Link to this heading">#</a></h2>
<section id="id10">
<h3>Answer:<a class="headerlink" href="#id10" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>PCA (Principal Component Analysis)</p></th>
<th class="head"><p>Autoencoders</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Transformation</p></td>
<td><p><strong>Linear</strong> transformation</p></td>
<td><p><strong>Nonlinear</strong> transformation</p></td>
</tr>
<tr class="row-odd"><td><p>Purpose</p></td>
<td><p>Dimensionality reduction</p></td>
<td><p>Dimensionality reduction and feature learning</p></td>
</tr>
<tr class="row-even"><td><p>Learning Type</p></td>
<td><p>Unsupervised</p></td>
<td><p>Unsupervised</p></td>
</tr>
<tr class="row-odd"><td><p>Neural Network</p></td>
<td><p>No</p></td>
<td><p>Yes</p></td>
</tr>
<tr class="row-even"><td><p>Applications</p></td>
<td><p>Data compression, visualization</p></td>
<td><p>Anomaly detection, denoising, feature extraction</p></td>
</tr>
</tbody>
</table>
</div>
<hr class="docutils" />
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./contents\6_interview"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Deep Learning: Interview</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-deep-learning-differs-from-machine-learning">1. How Deep Learning Differs from Machine Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-differences">Key Differences:</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-advantage">Deep Learning Advantage:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-deep-neural-network">2. What is a Deep Neural Network?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-a-transfer-function-needed-in-deep-learning">3. Why is a Transfer Function Needed in Deep Learning?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid-vs-tanh-activation-function">4. Sigmoid vs. Tanh Activation Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-the-softmax-function-used-in-the-output-layer">5. Why is the Softmax Function Used in the Output Layer?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-decide-batch-size-in-deep-learning">6. How to Decide Batch Size in Deep Learning?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#difference-between-epoch-and-iteration">7. Difference Between Epoch and Iteration</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-set-the-number-of-neurons-in-input-output-layers">8. How to Set the Number of Neurons in Input &amp; Output Layers?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-layer"><strong>Input Layer:</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#output-layer"><strong>Output Layer:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-set-the-number-of-neurons-in-hidden-layers">9. How to Set the Number of Neurons in Hidden Layers?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-dropout-and-why-is-it-useful">10. What is Dropout and Why is it Useful?</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-a-structured-guide-for-developers">Deep Learning: A Structured Guide for Developers</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-early-stopping">11. What is Early Stopping?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#purpose"><strong>Purpose:</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-it-works"><strong>How It Works:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-data-augmentation">12. What is Data Augmentation?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1"><strong>Purpose:</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-use-case"><strong>Example Use Case:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-data-normalization">13. What is Data Normalization?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2"><strong>Purpose:</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3"><strong>How It Works:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#can-we-initialize-all-weights-with-zero-if-not-why">14. Can We Initialize All Weights with Zero? If Not, Why?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#answer-no-initializing-all-weights-with-zero-is-a-bad-practice"><strong>Answer:</strong> No, initializing all weights with zero is a bad practice.</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why"><strong>Why?</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#good-weight-initialization-methods">15. Good Weight Initialization Methods</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#commonly-used-methods"><strong>Commonly Used Methods:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-can-loss-become-nan-during-training">16. Why Can Loss Become NaN During Training?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-reasons"><strong>Common Reasons:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-the-hyperparameters-of-a-neural-network">17. What Are the Hyperparameters of a Neural Network?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-we-train-a-deep-neural-network">18. How Do We Train a Deep Neural Network?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#process"><strong>Process:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-prevent-overfitting-in-deep-neural-networks">19. How to Prevent Overfitting in Deep Neural Networks?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#effective-methods"><strong>Effective Methods:</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-guide-for-developers">Deep Learning Guide for Developers</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-gradient-descent-and-is-it-a-first-order-method">21. What is Gradient Descent, and is it a First-Order Method?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-the-gradient-descent-method-work">22. How Does the Gradient Descent Method Work?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-happens-when-the-learning-rate-is-too-small-or-too-large">23. What Happens When the Learning Rate is Too Small or Too Large?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-need-for-gradient-checking">24. What is the Need for Gradient Checking?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-numerical-and-analytical-gradients">25. What are Numerical and Analytical Gradients?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-difference-between-convex-and-non-convex-functions">26. What is the Difference Between Convex and Non-Convex Functions?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-stochastic-gradient-descent-sgd">27. Why Do We Need Stochastic Gradient Descent (SGD)?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-stochastic-gradient-descent-work">28. How Does Stochastic Gradient Descent Work?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-mini-batch-gradient-descent-work">29. How Does Mini-Batch Gradient Descent Work?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#difference-between-gradient-descent-stochastic-gradient-descent-and-mini-batch-gradient-descent">30. Difference Between Gradient Descent, Stochastic Gradient Descent, and Mini-Batch Gradient Descent</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#adaptive-methods-of-gradient-descent-convolutional-neural-networks-cnn">Adaptive Methods of Gradient Descent &amp; Convolutional Neural Networks (CNN)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-some-of-the-adaptive-methods-of-gradient-descent">31. What are some of the adaptive methods of gradient descent?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-can-we-set-the-learning-rate-adaptively">32. How can we set the learning rate adaptively?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#can-we-get-rid-of-the-learning-rate">33. Can we get rid of the learning rate?</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-neural-networks-cnn">Convolutional Neural Networks (CNN)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-cnn-preferred-for-image-data">34. Why is CNN preferred for image data?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-the-different-layers-used-in-cnn">35. What are the different layers used in CNN?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explain-the-convolution-operation">36. Explain the convolution operation.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-a-pooling-layer">37. Why do we need a pooling layer?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-the-different-types-of-pooling">38. What are the different types of pooling?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explain-the-working-of-cnn">39. Explain the working of CNN.</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explain-the-architecture-of-lenet">40. Explain the architecture of LeNet.</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-concepts-cnn-rnn-lstm-and-more">Deep Learning Concepts: CNN, RNN, LSTM, and More</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-the-drawbacks-of-cnn">41. What are the Drawbacks of CNN?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#answer">Answer:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-rnn-useful">42. Why is RNN Useful?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Answer:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-prefer-a-recurrent-network-over-a-feedforward-network">43. When to Prefer a Recurrent Network Over a Feedforward Network?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Answer:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-lstm-differ-from-rnn">44. How Does LSTM Differ from RNN?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Answer:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-are-the-cell-state-and-hidden-state-used-in-lstm">45. How Are the Cell State and Hidden State Used in LSTM?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Answer:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-gated-recurrent-units-gru">46. Why Do We Need Gated Recurrent Units (GRU)?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Answer:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#difference-between-discriminative-and-generative-models">47. Difference Between Discriminative and Generative Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Answer:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#difference-between-autoencoders-and-pca">48. Difference Between Autoencoders and PCA</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Answer:</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gajanesh
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright ¬© 2025 Gajanesh. All rights reserved..
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>