
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Machine Learning: Interview Guide &#8212; Data Science Books</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-dropdown.css?v=995e94df" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-bootstrap.min.css?v=21c0b90a" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=d567e03f" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'contents/6_interview/ml';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Git Essentials" href="git.html" />
    <link rel="prev" title="üìö Pandas: Interview Guide" href="pd.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Data Science Books</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Part I ‚Äî Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../0_maths/0_essential.html">Essential Mathematics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_maths/4_linear_algebra.html">Linear Algebra</a></li>







<li class="toctree-l1"><a class="reference internal" href="../0_maths/2_probability.html">Probability Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_maths/1_descriptive.html">Descriptive Statistics</a></li>








<li class="toctree-l1"><a class="reference internal" href="../0_maths/3_inferential.html">Inferential Statistics</a></li>



<li class="toctree-l1"><a class="reference internal" href="../0_maths/5_calculus.html">Calculus</a></li>







<li class="toctree-l1"><a class="reference internal" href="../0_maths/6_regression_analysis.html">Explanatory and Response Variables</a></li>


<li class="toctree-l1"><a class="reference internal" href="../1_python/1_basics.html">Python Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/2_advanced.html">Advanced Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/3_data_structures.html">Data Structures</a></li>





<li class="toctree-l1"><a class="reference internal" href="../1_python/4_modules_packages.html">Modules &amp; Packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/5_functions.html">Functions &amp; Modular Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/6_oop.html">Object-Oriented Programming</a></li>





<li class="toctree-l1"><a class="reference internal" href="../1_python/8_exceptions.html">Exception Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/9_regex.html">Regular Expressions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_numpy/1_numpy.html">NumPy (<strong>Numerical Python</strong>)</a></li>



<li class="toctree-l1"><a class="reference internal" href="../2_pandas/1_series.html">Pandas Series</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_pandas/2_dataframes.html">Pandas DataFrame</a></li>
















<li class="toctree-l1"><a class="reference internal" href="../2_pandas/3_visualization.html"><strong>What is Data Visualization in Data Science?</strong></a></li>


<li class="toctree-l1"><a class="reference internal" href="../2_pandas/4_eda.html"><strong>What is Exploratory Data Analysis (EDA)?</strong></a></li>






<li class="toctree-l1"><a class="reference internal" href="../2_pandas/5_feature_engineering.html"><strong>What is Feature Engineering?</strong></a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part II ‚Äî Classical Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../3_ml/1_foundations.html">ML Foundational</a></li>





<li class="toctree-l1"><a class="reference internal" href="../3_ml/2_data_preparation.html">2Ô∏è‚É£ Data Handling</a></li>





<li class="toctree-l1"><a class="reference internal" href="../3_ml/2_train_test_split.html">Train‚ÄìTest Split</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/5_model_evaluation.html">4Ô∏è‚É£ Model Evaluation</a></li>




<li class="toctree-l1"><a class="reference internal" href="../3_ml/11_supervised_learning.html"><strong>Supervised Learning</strong></a></li>



<li class="toctree-l1"><a class="reference internal" href="../3_ml/12_regression.html">Regression Algorithms</a></li>







<li class="toctree-l1"><a class="reference internal" href="../3_ml/13_classification.html">Classification Algorithms</a></li>

<li class="toctree-l1"><a class="reference internal" href="../3_ml/10_core_algo.html">Core ML Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/14_decision_trees.html">Decision Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/15_ensemble_methods.html">Ensemble Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/16_svm.html">Support Vector Machine (SVM) in Detail</a></li>


<li class="toctree-l1"><a class="reference internal" href="../3_ml/17_knn.html">k-Nearest Neighbors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/18_naive_bayes.html">Naive Bayes</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part III ‚Äî Advanced Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../3_ml/20_unsupervised_learning.html">Unsupervised Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../3_ml/21_clustering.html">Clustering Techniques</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/7_optimization_and_training.html">5Ô∏è‚É£ Optimization &amp; Training</a></li>







<li class="toctree-l1 has-children"><a class="reference internal" href="../3_ml/6_ml_lifecycle.html">ML Lifecycle</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../3_ml/6_training.html">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3_ml/6_evaluation.html">Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3_ml/6_deployment.html">Deployment</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part IV ‚Äî Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl1_Introduction.html">Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl2_Neuron.html">Neuron</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl3_Libraries.html">Deep Learning Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl4_Terminology.html">Terminology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl5_multi_layer.html">Multi-Layer Perceptron</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl6_first_nn.html">First Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl7_evaluating_model.html">Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl8_multiclass_classification.html">Multiclass Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl9_multiclass_classification_hand.html">Handwritten Digit Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl10_saving_and_loading.html">Saving &amp; Loading Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl11_checkpointing.html">Model Checkpointing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl12_visualizing_model_training.html"><strong>Visualizing Model Training History in Deep Learning</strong></a></li>

<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl13_loss_functions_activation_functions_and_optimizers.html">Loss Functions &amp; Optimizers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part V ‚Äî NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp1.html">NLP Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp2.html">Text Cleaning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp3.html">Text Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp4.html">NLP Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp5.html">Bag of Words, TF-IDF</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp6.html">Sentiment Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp7.html">NLP with SpaCy</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part VI ‚Äî Career &amp; MLOps</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="self_introduction.html">Self Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="py.html">Python: Interview Guide</a></li>





<li class="toctree-l1"><a class="reference internal" href="pd.html">üìö Pandas: Interview Guide</a></li>


<li class="toctree-l1 current active"><a class="current reference internal" href="#">Machine Learning: Interview Guide</a></li>













<li class="toctree-l1"><a class="reference internal" href="git.html">Git</a></li>
<li class="toctree-l1"><a class="reference internal" href="dvc.html">DVC</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlflow.html">MLflow</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/coding-fame/Data-science-books" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/coding-fame/Data-science-books/edit/main/contents/6_interview/ml.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/coding-fame/Data-science-books/issues/new?title=Issue%20on%20page%20%2Fcontents/6_interview/ml.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/contents/6_interview/ml.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Machine Learning: Interview Guide</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Machine Learning: Interview Guide</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-an-algorithm">1. What is an Algorithm?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-machine-learning-algorithm">2. What is a Machine Learning Algorithm?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#difference-between-machine-learning-algorithm-and-normal-algorithm">3. Difference Between Machine Learning Algorithm and Normal Algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-machine-learning">4. Why Machine Learning?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-workflow">5. Machine Learning Workflow</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#steps-in-a-machine-learning-project">Steps in a Machine Learning Project:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-machine-learning-approaches">6. Types of Machine Learning Approaches</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-supervised-learning">7. What is Supervised Learning?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-supervised-learning">Types of Supervised Learning:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-regression">8. What is Regression?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examples">Examples:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-regression-algorithms">Common Regression Algorithms:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-classification">9. What is Classification?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Examples:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-classification-algorithms">Common Classification Algorithms:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-guide">Machine Learning Guide</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-algorithms-in-machine-learning">11. Classification Algorithms in Machine Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Common Classification Algorithms:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-unsupervised-learning">12. What is Unsupervised Learning?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-characteristics">Key Characteristics:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-unsupervised-learning">Types of Unsupervised Learning:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-clustering">13. What is Clustering?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Examples:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-clustering-algorithms">14. Common Clustering Algorithms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-dimensionality-reduction">15. What is Dimensionality Reduction?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-it-important">Why is it Important?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-dimensionality-reduction-algorithms">16. Common Dimensionality Reduction Algorithms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-reinforcement-learning">17. What is Reinforcement Learning?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts">Key Concepts:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#difference-between-regression-and-classification">18. Difference Between Regression and Classification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#difference-between-online-and-offline-batch-learning">19. Difference Between Online and Offline (Batch) Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#online-learning">Online Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#offline-batch-learning">Offline (Batch) Learning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-train-a-model-effectively">20. How to Train a Model Effectively?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">üéØ Summary</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-fundamentals-a-guide-for-junior-to-mid-level-developers">Machine Learning Fundamentals: A Guide for Junior to Mid-Level Developers</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem">21. Bayes Theorem</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-bayes-theorem">What is Bayes Theorem?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#formula">Formula:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#difference-between-knn-and-k-means">22. Difference Between KNN and K-Means</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbors-knn"><strong>K-Nearest Neighbors (KNN)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-means-clustering"><strong>K-Means Clustering</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-model-training">23. What is Model Training?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-linear-regression-formula"><strong>Example: Linear Regression Formula</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-convergence">24. What is Convergence?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-allocate-data-for-training-validation-and-testing">25. How to Allocate Data for Training, Validation, and Testing?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-p-value-why-is-it-important">26. What is a p-value? Why is it Important?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-rule"><strong>Decision Rule</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-f1-score">27. What is F1 Score?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4"><strong>Formula:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-type-i-and-type-ii-errors">28. What are Type I and Type II Errors?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#type-i-error-false-positive"><strong>Type I Error (False Positive)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#type-ii-error-false-negative"><strong>Type II Error (False Negative)</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#are-you-familiar-with-r-programming">29. Are You Familiar with R Programming?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-hypothesis">30. What is a Hypothesis?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-takeaways">üìå <strong>Key Takeaways:</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-guide">Linear Regression Guide</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-linear-regression">1. What is Linear Regression?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Examples:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-linear-regression">2. Types of Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-linear-regression"><strong>1. Simple Linear Regression</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-linear-regression"><strong>2. Multiple Linear Regression</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6"><strong>Example:</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-r-squared">3. What is R-Squared?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-accuracy-in-regression">4. What is Accuracy in Regression?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-metrics">5. Regression Metrics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error-mse"><strong>1. Mean Squared Error (MSE)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#root-mean-squared-error-rmse"><strong>2. Root Mean Squared Error (RMSE)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-absolute-error-mae"><strong>3. Mean Absolute Error (MAE)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#python-code-for-regression-metrics"><strong>Python Code for Regression Metrics:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-overfitting">6. What is Overfitting?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-correlation">7. What is Correlation?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-learning-rate">8. What is Learning Rate?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-intercept">9. What is the Intercept?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumptions-of-linear-regression">10. Assumptions of Linear Regression</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-essential-concepts-techniques">Machine Learning: Essential Concepts &amp; Techniques</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization-vs-standardization">41. Normalization vs Standardization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization"><strong>Normalization</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standardization"><strong>Standardization</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-use"><strong>When to Use?</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-selection-in-machine-learning">42. Model Selection in Machine Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-boundary">43. Decision Boundary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-a-logistic-regression-model-is-trained">44. How a Logistic Regression Model is Trained</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-naive-bayes-called-naive">45. Why is Na√Øve Bayes Called ‚ÄúNa√Øve‚Äù?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-a-classifier-based-on-training-set-size">46. Choosing a Classifier Based on Training Set Size</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-of-naive-bayes-algorithm">47. Advantages of Na√Øve Bayes Algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-the-optimal-k-in-k-nn">48. Choosing the Optimal ( k ) in k-NN</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#is-k-nn-suitable-for-large-datasets">49. Is k-NN Suitable for Large Datasets?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbors-k-nn-algorithm">50. k-Nearest Neighbors (k-NN) Algorithm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-it-works"><strong>How it Works?</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distance-metrics"><strong>Distance Metrics</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization"><strong>Visualization:</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#k-means-vs-k-nearest-neighbors-k-nn">k-Means vs. k-Nearest Neighbors (k-NN)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-main-difference-between-k-means-and-k-nearest-neighbors">1. What is the main difference between k-Means and k-Nearest Neighbors?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-means-clustering-algorithm">k-Means (Clustering Algorithm)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbors-k-nn">k-Nearest Neighbors (k-NN)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-you-select-the-value-of-k-for-k-nearest-neighbors">2. How do you select the value of K for k-Nearest Neighbors?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-disadvantages-of-k-nearest-neighbors">3. Advantages &amp; Disadvantages of k-Nearest Neighbors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages">‚úÖ Advantages</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#disadvantages">‚ùå Disadvantages</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-of-k-means-clustering">4. Applications of k-Means Clustering</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#steps-of-k-means-clustering-algorithm">5. Steps of k-Means Clustering Algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-objective-function-of-k-means">6. What is the Objective Function of k-Means?</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-trees">Decision Trees</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-decision-trees">7. What are Decision Trees?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Example:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-of-decision-trees">8. Advantages of Decision Trees</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-pure-node">9. What is a Pure Node?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-handle-overfitting-in-decision-trees">10. How to Handle Overfitting in Decision Trees?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solution-pruning-reducing-complexity-of-the-tree">Solution: <strong>Pruning</strong> (Reducing complexity of the tree)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bottom-up-pruning">üîΩ Bottom-Up Pruning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#top-down-pruning">üîº Top-Down Pruning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reduced-error-pruning">üîß Reduced Error Pruning</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-concepts-structured-guide">üìå Machine Learning Concepts: Structured Guide</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#greedy-splitting">1Ô∏è‚É£ Greedy Splitting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy">2Ô∏è‚É£ Entropy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tree-bagging-bootstrap-aggregating">3Ô∏è‚É£ Tree Bagging (Bootstrap Aggregating)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tree-boosting">4Ô∏è‚É£ Tree Boosting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#handling-outliers-in-logistic-regression">5Ô∏è‚É£ Handling Outliers in Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy-in-decision-trees">6Ô∏è‚É£ Entropy in Decision Trees</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forest">7Ô∏è‚É£ Random Forest</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#does-random-forest-require-pruning">8Ô∏è‚É£ Does Random Forest Require Pruning?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ensemble-methods">9Ô∏è‚É£ Ensemble Methods</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forest-a-comprehensive-guide">Random Forest: A Comprehensive Guide</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameters-in-random-forest">1. Hyperparameters in Random Forest</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#is-cross-validation-necessary-in-random-forest">2. Is Cross-Validation Necessary in Random Forest?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#is-random-forest-an-ensemble-algorithm">3. Is Random Forest an Ensemble Algorithm?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#handling-missing-values-in-random-forest">4. Handling Missing Values in Random Forest</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variable-selection-in-random-forest">5. Variable Selection in Random Forest</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-random-forest-considered-non-interpretable">6. Why is Random Forest Considered Non-Interpretable?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-of-random-forest">7. Advantages of Random Forest</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#drawbacks-of-random-forest">8. Drawbacks of Random Forest</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-bagging-bootstrap-aggregating">9. Understanding BAGGing (Bootstrap Aggregating)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#difference-between-oob-score-and-validation-score">10. Difference Between OOB Score and Validation Score</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-concepts-random-forest-support-vector-machines-svm">Machine Learning Concepts: Random Forest &amp; Support Vector Machines (SVM)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-proximities-in-random-forests">31. What are Proximities in Random Forests?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-define-the-criteria-to-split-at-each-node-of-the-trees">32. How to Define the Criteria to Split at Each Node of the Trees?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-adaboost-algorithm">33. What is the AdaBoost Algorithm?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-logistic-regression-handle-outliers">34. How Does Logistic Regression Handle Outliers?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-support-vector-machine-svm">35. What is a Support Vector Machine (SVM)?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-hyperplane-in-svm">36. What is a Hyperplane in SVM?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-support-vectors-in-svm">37. What are Support Vectors in SVM?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-svm-kernels">38. Types of SVM Kernels</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-the-kernel-trick">39. Why Use the Kernel Trick?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-of-svms">40. Applications of SVMs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#face-detection"><strong>1. Face Detection</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text-hypertext-categorization"><strong>2. Text &amp; Hypertext Categorization</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#image-classification"><strong>3. Image Classification</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bioinformatics"><strong>4. Bioinformatics</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#protein-fold-homology-detection"><strong>5. Protein Fold &amp; Homology Detection</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#handwriting-recognition"><strong>6. Handwriting Recognition</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generalized-predictive-control-gpc"><strong>7. Generalized Predictive Control (GPC)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion"><strong>Conclusion</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#support-vector-machine-svm-and-anomaly-detection-guide">Support Vector Machine (SVM) and Anomaly Detection Guide</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#role-of-c-hyperparameter-in-svm">41. Role of C Hyperparameter in SVM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-support-vectors">42. What are Support Vectors?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-anomaly-detection">43. What is Anomaly Detection?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-care-about-anomalies">44. Why Do We Care About Anomalies?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">45. Normalization vs. Standardization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-68-95-99-7-rule-for-normal-distribution">46. The 68-95-99.7 Rule for Normal Distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-is-iqr-interquartile-range-used-in-time-series-forecasting">47. How is IQR (Interquartile Range) Used in Time Series Forecasting?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-standard-deviation-for-anomaly-detection">48. Using Standard Deviation for Anomaly Detection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#can-you-find-outliers-using-k-means">49. Can You Find Outliers Using k-Means?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-handle-outliers-in-a-dataset">50. How to Handle Outliers in a Dataset?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approaches">Approaches:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-interview-questions-answers">Machine Learning Interview Questions &amp; Answers</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-bias-in-machine-learning">1. What is Bias in Machine Learning?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#characteristics-of-bias">Characteristics of Bias:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-bias-variance-tradeoff">2. What is the Bias-Variance Tradeoff?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solution">Solution:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-identify-and-fix-a-high-bias-model">3. How to Identify and Fix a High-Bias Model?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#identifying-high-bias">Identifying High Bias:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fixing-high-bias">Fixing High Bias:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-types-of-classification-algorithms-exist">4. What Types of Classification Algorithms Exist?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-the-adaboost-algorithm-work">5. How Does the AdaBoost Algorithm Work?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-confusion-matrix">6. What is a Confusion Matrix?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-roc-curve-auc-measure-model-performance">7. How Do ROC Curve &amp; AUC Measure Model Performance?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-difference-between-cost-function-and-gradient-descent">8. What is the Difference Between Cost Function and Gradient Descent?</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-interview-guide-data-preprocessing-optimization">Machine Learning Interview Guide: Data Preprocessing &amp; Optimization</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-data-preprocessing-what-steps-are-involved">1Ô∏è‚É£ What is Data Preprocessing? What Steps are Involved?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-preprocessing-steps">Key Preprocessing Steps:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-feature-engineering">2Ô∏è‚É£ What is Feature Engineering?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-feature-engineering-techniques">Common Feature Engineering Techniques:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-some-recommended-choices-for-imputation-values">3Ô∏è‚É£ What are Some Recommended Choices for Imputation Values?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#is-it-a-good-idea-to-clean-data-automatically">4Ô∏è‚É£ Is it a Good Idea to Clean Data Automatically?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-you-check-the-quality-of-your-dataset">5Ô∏è‚É£ How Do You Check the Quality of Your Dataset?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-anova">6Ô∏è‚É£ What is ANOVA?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-ensemble-learning">7Ô∏è‚É£ What is Ensemble Learning?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-the-differences-between-bagging-and-boosting">8Ô∏è‚É£ What are the Differences Between Bagging and Boosting?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-difference-between-cost-function-vs-gradient-descent">9Ô∏è‚É£ What is the Difference Between Cost Function vs Gradient Descent?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-idea-behind-gradient-descent">üîü What is the Idea Behind Gradient Descent?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-considerations">Key Considerations:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-gradient-descent-in-linear-regression">üìâ Understanding Gradient Descent in Linear Regression</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-gradient-descent-work">üîπ How Does Gradient Descent Work?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-by-step-process">üîÑ <strong>Step-by-Step Process:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-gradient-descent">üìä <strong>Visualizing Gradient Descent</strong></a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="machine-learning-interview-guide">
<h1>Machine Learning: Interview Guide<a class="headerlink" href="#machine-learning-interview-guide" title="Link to this heading">#</a></h1>
<section id="what-is-an-algorithm">
<h2>1. What is an Algorithm?<a class="headerlink" href="#what-is-an-algorithm" title="Link to this heading">#</a></h2>
<p>An <strong>algorithm</strong> is a program that contains a set of instructions to perform a specific task.</p>
<ul class="simple">
<li><p><strong>Input + Logic = Output</strong></p></li>
<li><p><strong>Data + Program = Result</strong></p></li>
</ul>
</section>
<hr class="docutils" />
<section id="what-is-a-machine-learning-algorithm">
<h2>2. What is a Machine Learning Algorithm?<a class="headerlink" href="#what-is-a-machine-learning-algorithm" title="Link to this heading">#</a></h2>
<p>A <strong>Machine Learning Algorithm</strong> is an application that learns patterns and knowledge automatically from data without explicit programming.</p>
<ul class="simple">
<li><p><strong>Data + Result = Model (Program)</strong></p></li>
</ul>
</section>
<hr class="docutils" />
<section id="difference-between-machine-learning-algorithm-and-normal-algorithm">
<h2>3. Difference Between Machine Learning Algorithm and Normal Algorithm<a class="headerlink" href="#difference-between-machine-learning-algorithm-and-normal-algorithm" title="Link to this heading">#</a></h2>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Machine Learning Algorithm</p></th>
<th class="head"><p>Normal Algorithm</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Learning Capability</p></td>
<td><p>Learns patterns from data</p></td>
<td><p>Does not learn</p></td>
</tr>
<tr class="row-odd"><td><p>Adaptability</p></td>
<td><p>Improves over time</p></td>
<td><p>Fixed logic</p></td>
</tr>
<tr class="row-even"><td><p>Explicit Programming</p></td>
<td><p>Not required</p></td>
<td><p>Required</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<hr class="docutils" />
<section id="why-machine-learning">
<h2>4. Why Machine Learning?<a class="headerlink" href="#why-machine-learning" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Companies generate a <strong>huge amount of data</strong> every day.</p></li>
<li><p><strong>Machine Learning extracts useful insights</strong> from this data.</p></li>
<li><p><strong>Major use cases of Machine Learning:</strong></p>
<ul>
<li><p>Creating models</p></li>
<li><p>Gaining deep insights</p></li>
<li><p>Automating decision-making</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="machine-learning-workflow">
<h2>5. Machine Learning Workflow<a class="headerlink" href="#machine-learning-workflow" title="Link to this heading">#</a></h2>
<section id="steps-in-a-machine-learning-project">
<h3>Steps in a Machine Learning Project:<a class="headerlink" href="#steps-in-a-machine-learning-project" title="Link to this heading">#</a></h3>
<p>Every ML project follows these steps:</p>
<ol class="arabic simple">
<li><p><strong>Data Gathering</strong></p>
<ul class="simple">
<li><p>Collect raw data from one or multiple sources.</p></li>
<li><p>Data consists of observations.</p></li>
</ul>
</li>
<li><p><strong>Data Cleaning</strong></p>
<ul class="simple">
<li><p>Raw data is often messy (missing values, corrupt data, etc.).</p></li>
<li><p>Clean data to ensure consistency and accuracy.</p></li>
</ul>
</li>
<li><p><strong>Feature Extraction (Feature Engineering)</strong></p>
<ul class="simple">
<li><p>Identify and extract useful features from cleaned data.</p></li>
<li><p>Domain knowledge enhances feature selection.</p></li>
<li><p>Improves model accuracy.</p></li>
</ul>
</li>
<li><p><strong>Model Training</strong></p>
<ul class="simple">
<li><p>Train the selected ML model using training data.</p></li>
</ul>
</li>
<li><p><strong>Prediction &amp; Evaluation</strong></p>
<ul class="simple">
<li><p>Evaluate the trained model‚Äôs accuracy.</p></li>
<li><p>Measure performance using metrics.</p></li>
<li><p><strong>Deploy</strong>: If performance is good.</p></li>
<li><p><strong>Retrain</strong>: If performance is low, adjust and repeat training.</p></li>
</ul>
</li>
</ol>
</section>
</section>
<hr class="docutils" />
<section id="types-of-machine-learning-approaches">
<h2>6. Types of Machine Learning Approaches<a class="headerlink" href="#types-of-machine-learning-approaches" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>Supervised Learning</strong></p></li>
<li><p><strong>Unsupervised Learning</strong></p></li>
<li><p><strong>Reinforcement Learning</strong></p></li>
</ol>
</section>
<hr class="docutils" />
<section id="what-is-supervised-learning">
<h2>7. What is Supervised Learning?<a class="headerlink" href="#what-is-supervised-learning" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Finds relationships between <strong>independent</strong> and <strong>dependent</strong> variables.</p></li>
<li><p>Learns a mapping function from <strong>input to output</strong>.</p></li>
<li><p>Requires <strong>labeled data</strong> (features &amp; labels).</p></li>
</ul>
<section id="types-of-supervised-learning">
<h3>Types of Supervised Learning:<a class="headerlink" href="#types-of-supervised-learning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Regression</strong> (Predicting numerical values)</p></li>
<li><p><strong>Classification</strong> (Categorizing data)</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="what-is-regression">
<h2>8. What is Regression?<a class="headerlink" href="#what-is-regression" title="Link to this heading">#</a></h2>
<p>Regression is a <strong>supervised learning</strong> technique used for predicting <strong>continuous numerical values</strong>.</p>
<section id="examples">
<h3>Examples:<a class="headerlink" href="#examples" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Predicting <strong>house prices</strong></p></li>
<li><p>Predicting <strong>employee salaries</strong></p></li>
</ul>
</section>
<section id="common-regression-algorithms">
<h3>Common Regression Algorithms:<a class="headerlink" href="#common-regression-algorithms" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Linear Regression</strong></p></li>
<li><p><strong>Decision Tree</strong></p></li>
<li><p><strong>Random Forest</strong></p></li>
<li><p><strong>K-Nearest Neighbors (KNN)</strong></p></li>
<li><p><strong>Support Vector Machines (SVM)</strong></p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="what-is-classification">
<h2>9. What is Classification?<a class="headerlink" href="#what-is-classification" title="Link to this heading">#</a></h2>
<p>Classification is a <strong>supervised learning</strong> technique used to categorize data into different classes.</p>
<section id="id1">
<h3>Examples:<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Will a customer <strong>buy a product</strong>? (Yes/No)</p></li>
<li><p>Is a person <strong>suffering from a disease</strong>? (Yes/No)</p></li>
<li><p>Stock market decision: <strong>Buy, Sell, or Hold</strong></p></li>
</ul>
</section>
<section id="common-classification-algorithms">
<h3>Common Classification Algorithms:<a class="headerlink" href="#common-classification-algorithms" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Logistic Regression</strong></p></li>
<li><p><strong>Decision Tree</strong></p></li>
<li><p><strong>Random Forest</strong></p></li>
<li><p><strong>K-Nearest Neighbors (KNN)</strong></p></li>
<li><p><strong>Support Vector Machines (SVM)</strong></p></li>
</ul>
</section>
</section>
</section>
<hr class="docutils" />
<section id="machine-learning-guide">
<h1>Machine Learning Guide<a class="headerlink" href="#machine-learning-guide" title="Link to this heading">#</a></h1>
<section id="classification-algorithms-in-machine-learning">
<h2>11. Classification Algorithms in Machine Learning<a class="headerlink" href="#classification-algorithms-in-machine-learning" title="Link to this heading">#</a></h2>
<p>Classification is a supervised learning technique used to categorize data into predefined classes.</p>
<section id="id2">
<h3>Common Classification Algorithms:<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Logistic Regression</strong></p></li>
<li><p><strong>Naive Bayes</strong></p></li>
<li><p><strong>K-Nearest Neighbors (KNN)</strong></p></li>
<li><p><strong>Decision Tree</strong></p></li>
<li><p><strong>Support Vector Machines (SVM)</strong></p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="what-is-unsupervised-learning">
<h2>12. What is Unsupervised Learning?<a class="headerlink" href="#what-is-unsupervised-learning" title="Link to this heading">#</a></h2>
<p><strong>Unsupervised learning</strong> is a machine learning technique where the algorithm identifies patterns and relationships in data without labeled outputs.</p>
<section id="key-characteristics">
<h3>Key Characteristics:<a class="headerlink" href="#key-characteristics" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Groups data based on similarities.</p></li>
<li><p>Finds hidden patterns without predefined labels.</p></li>
<li><p>Works only with input features (no dependent variable).</p></li>
</ul>
</section>
<section id="types-of-unsupervised-learning">
<h3>Types of Unsupervised Learning:<a class="headerlink" href="#types-of-unsupervised-learning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Clustering</strong></p></li>
<li><p><strong>Dimensionality Reduction</strong></p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="what-is-clustering">
<h2>13. What is Clustering?<a class="headerlink" href="#what-is-clustering" title="Link to this heading">#</a></h2>
<p>Clustering is an <strong>unsupervised learning</strong> technique used to group data points with similar characteristics.</p>
<section id="id3">
<h3>Examples:<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Customers buying products in different price ranges:</p>
<ul>
<li><p><strong>1000 to 5000</strong> ‚Üí Group A</p></li>
<li><p><strong>5000 to 10000</strong> ‚Üí Group B</p></li>
</ul>
</li>
<li><p>Employees categorized based on salary:</p>
<ul>
<li><p><strong>50K to 70K</strong> ‚Üí Group C</p></li>
<li><p><strong>70K to 90K</strong> ‚Üí Group D</p></li>
</ul>
</li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="common-clustering-algorithms">
<h2>14. Common Clustering Algorithms<a class="headerlink" href="#common-clustering-algorithms" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>K-Means</strong></p></li>
<li><p><strong>Hierarchical Clustering</strong></p></li>
</ul>
</section>
<hr class="docutils" />
<section id="what-is-dimensionality-reduction">
<h2>15. What is Dimensionality Reduction?<a class="headerlink" href="#what-is-dimensionality-reduction" title="Link to this heading">#</a></h2>
<p>Dimensionality reduction is the process of reducing the number of input variables in a dataset while retaining essential information.</p>
<section id="why-is-it-important">
<h3>Why is it Important?<a class="headerlink" href="#why-is-it-important" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Reduces computational complexity.</p></li>
<li><p>Removes redundant or irrelevant features.</p></li>
<li><p>Improves model performance.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="common-dimensionality-reduction-algorithms">
<h2>16. Common Dimensionality Reduction Algorithms<a class="headerlink" href="#common-dimensionality-reduction-algorithms" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Principal Component Analysis (PCA)</strong></p></li>
<li><p><strong>Linear Discriminant Analysis (LDA)</strong></p></li>
<li><p><strong>Generalized Discriminant Analysis (GDA)</strong></p></li>
</ul>
</section>
<hr class="docutils" />
<section id="what-is-reinforcement-learning">
<h2>17. What is Reinforcement Learning?<a class="headerlink" href="#what-is-reinforcement-learning" title="Link to this heading">#</a></h2>
<p>Reinforcement Learning (RL) is a type of machine learning where an <strong>agent learns by interacting with its environment</strong> and receiving rewards or penalties.</p>
<section id="key-concepts">
<h3>Key Concepts:<a class="headerlink" href="#key-concepts" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The agent performs actions.</p></li>
<li><p>It receives feedback in the form of <strong>+1 (reward)</strong> or <strong>-1 (penalty)</strong>.</p></li>
<li><p>The goal is to maximize positive rewards over time.</p></li>
<li><p>RL is widely used in <strong>AI-driven decision-making systems</strong>.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="difference-between-regression-and-classification">
<h2>18. Difference Between Regression and Classification<a class="headerlink" href="#difference-between-regression-and-classification" title="Link to this heading">#</a></h2>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Regression</p></th>
<th class="head"><p>Classification</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Type</p></td>
<td><p>Supervised Learning</p></td>
<td><p>Supervised Learning</p></td>
</tr>
<tr class="row-odd"><td><p>Purpose</p></td>
<td><p>Predicts <strong>continuous values</strong></p></td>
<td><p>Categorizes into <strong>classes</strong></p></td>
</tr>
<tr class="row-even"><td><p>Example</p></td>
<td><p>Predicting stock price</p></td>
<td><p>Identifying spam emails</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<hr class="docutils" />
<section id="difference-between-online-and-offline-batch-learning">
<h2>19. Difference Between Online and Offline (Batch) Learning<a class="headerlink" href="#difference-between-online-and-offline-batch-learning" title="Link to this heading">#</a></h2>
<section id="online-learning">
<h3>Online Learning<a class="headerlink" href="#online-learning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Data is processed in real-time, sequentially.</p></li>
<li><p>Example: <strong>Amazon‚Äôs real-time recommendation system</strong> (learns from each purchase and suggests products).</p></li>
</ul>
</section>
<section id="offline-batch-learning">
<h3>Offline (Batch) Learning<a class="headerlink" href="#offline-batch-learning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Data is processed in predefined batches.</p></li>
<li><p>The model is trained periodically with the available dataset.</p></li>
<li><p>Also known as <strong>batch learning</strong>.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="how-to-train-a-model-effectively">
<h2>20. How to Train a Model Effectively?<a class="headerlink" href="#how-to-train-a-model-effectively" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Divide the dataset</strong> into <strong>training</strong> and <strong>testing</strong> sets.</p></li>
<li><p>Train the model using the <strong>training set</strong> and evaluate it on the <strong>testing set</strong>.</p></li>
<li><p>Use <strong>cross-validation</strong> to improve model performance and select the best model.</p></li>
</ul>
<hr class="docutils" />
<section id="summary">
<h3>üéØ Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Classification</strong> categorizes data into predefined classes.</p></li>
<li><p><strong>Unsupervised learning</strong> finds hidden patterns without labeled data.</p></li>
<li><p><strong>Clustering</strong> groups similar data points.</p></li>
<li><p><strong>Dimensionality reduction</strong> improves model efficiency.</p></li>
<li><p><strong>Reinforcement learning</strong> optimizes decision-making based on rewards.</p></li>
<li><p><strong>Regression vs Classification</strong>: Continuous vs categorical predictions.</p></li>
<li><p><strong>Online vs Offline learning</strong>: Real-time vs batch processing.</p></li>
<li><p><strong>Effective model training</strong> requires proper dataset division and cross-validation.</p></li>
</ul>
</section>
</section>
</section>
<hr class="docutils" />
<section id="machine-learning-fundamentals-a-guide-for-junior-to-mid-level-developers">
<h1>Machine Learning Fundamentals: A Guide for Junior to Mid-Level Developers<a class="headerlink" href="#machine-learning-fundamentals-a-guide-for-junior-to-mid-level-developers" title="Link to this heading">#</a></h1>
<section id="bayes-theorem">
<h2>21. Bayes Theorem<a class="headerlink" href="#bayes-theorem" title="Link to this heading">#</a></h2>
<section id="what-is-bayes-theorem">
<h3>What is Bayes Theorem?<a class="headerlink" href="#what-is-bayes-theorem" title="Link to this heading">#</a></h3>
<p>Bayes Theorem explains the probability of an event based on prior knowledge of related events. It is used to update probabilities based on new evidence.</p>
</section>
<section id="formula">
<h3>Formula:<a class="headerlink" href="#formula" title="Link to this heading">#</a></h3>
<p>$$ P(A | B) = \frac{P(B | A) P(A)}{P(B)} $$</p>
</section>
<section id="example">
<h3>Example:<a class="headerlink" href="#example" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Let <strong>A</strong> be the event that a person has <strong>liver disease</strong> and <strong>B</strong> be the event that the person is <strong>an alcoholic</strong>.</p></li>
<li><p>It is easier to find <strong>P(B | A)</strong> (the probability of being an alcoholic given liver disease).</p></li>
<li><p>We need to determine <strong>P(A | B)</strong> (the probability of liver disease given alcoholism).</p></li>
<li><p>Bayes Theorem helps us calculate this probability using available data.</p></li>
</ul>
</section>
</section>
<section id="difference-between-knn-and-k-means">
<h2>22. Difference Between KNN and K-Means<a class="headerlink" href="#difference-between-knn-and-k-means" title="Link to this heading">#</a></h2>
<section id="k-nearest-neighbors-knn">
<h3><strong>K-Nearest Neighbors (KNN)</strong><a class="headerlink" href="#k-nearest-neighbors-knn" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Supervised learning algorithm</strong></p></li>
<li><p>Used for <strong>classification and regression</strong> problems</p></li>
<li><p>Classifies an observation based on its ‚Äúk‚Äù nearest neighbors</p></li>
<li><p><strong>Lazy learner</strong> (does not learn during training, only at prediction time)</p></li>
</ul>
</section>
<section id="k-means-clustering">
<h3><strong>K-Means Clustering</strong><a class="headerlink" href="#k-means-clustering" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Unsupervised learning algorithm</strong></p></li>
<li><p><strong>Clustering algorithm</strong> used to group similar data points</p></li>
<li><p>Partitions data into <strong>k clusters</strong></p></li>
<li><p>Data points in a cluster are <strong>closer to each other</strong> than to points in other clusters</p></li>
</ul>
</section>
</section>
<section id="what-is-model-training">
<h2>23. What is Model Training?<a class="headerlink" href="#what-is-model-training" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Training a model means feeding it <strong>a training dataset</strong>.</p></li>
<li><p>The model <strong>learns parameters</strong> during training.</p></li>
<li><p>These parameters define the mathematical relationships in the model.</p></li>
</ul>
<section id="example-linear-regression-formula">
<h3><strong>Example: Linear Regression Formula</strong><a class="headerlink" href="#example-linear-regression-formula" title="Link to this heading">#</a></h3>
<p>$$ y = mx + c $$</p>
<ul class="simple">
<li><p><strong>y</strong>: Output (prediction)</p></li>
<li><p><strong>x</strong>: Input</p></li>
<li><p><strong>m, c</strong>: Parameters learned during training</p></li>
</ul>
</section>
</section>
<section id="what-is-convergence">
<h2>24. What is Convergence?<a class="headerlink" href="#what-is-convergence" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Convergence</strong> means the model‚Äôs output approaches a stable value over iterations.</p></li>
<li><p>The <strong>global optimum</strong> is the lowest possible error value.</p></li>
<li><p>‚ÄúThe algorithm converges to the global optimum‚Äù means it minimizes errors over time.</p></li>
</ul>
</section>
<section id="how-to-allocate-data-for-training-validation-and-testing">
<h2>25. How to Allocate Data for Training, Validation, and Testing?<a class="headerlink" href="#how-to-allocate-data-for-training-validation-and-testing" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>No <strong>fixed rule</strong> for splitting data, but common practice is <strong>80:20 (train:test)</strong>.</p></li>
<li><p><strong>Training set too small</strong> ‚Üí High variance, model won‚Äôt learn properly.</p></li>
<li><p><strong>Test set too small</strong> ‚Üí Unreliable performance estimation.</p></li>
<li><p>The training dataset can be further divided into <strong>training and validation sets</strong> to avoid overfitting.</p></li>
</ul>
</section>
<section id="what-is-a-p-value-why-is-it-important">
<h2>26. What is a p-value? Why is it Important?<a class="headerlink" href="#what-is-a-p-value-why-is-it-important" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>A <strong>p-value</strong> represents the significance level in a <strong>hypothesis test</strong>.</p></li>
<li><p>It helps decide whether to reject the <strong>null hypothesis</strong>.</p></li>
</ul>
<section id="decision-rule">
<h3><strong>Decision Rule</strong><a class="headerlink" href="#decision-rule" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>p-value ‚â§ 0.05</strong> ‚Üí Strong evidence against the null hypothesis ‚Üí <strong>Reject the null hypothesis</strong>.</p></li>
<li><p><strong>p-value &gt; 0.05</strong> ‚Üí Weak evidence against the null hypothesis ‚Üí <strong>Fail to reject the null hypothesis</strong>.</p></li>
</ul>
</section>
</section>
<section id="what-is-f1-score">
<h2>27. What is F1 Score?<a class="headerlink" href="#what-is-f1-score" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>The <strong>F1 score</strong> measures model accuracy, balancing <strong>precision and recall</strong>.</p></li>
<li><p>It ranges from <strong>0 (worst) to 1 (best)</strong>.</p></li>
</ul>
<section id="id4">
<h3><strong>Formula:</strong><a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<p>$$ F_1 = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}} $$</p>
</section>
</section>
<section id="what-are-type-i-and-type-ii-errors">
<h2>28. What are Type I and Type II Errors?<a class="headerlink" href="#what-are-type-i-and-type-ii-errors" title="Link to this heading">#</a></h2>
<section id="type-i-error-false-positive">
<h3><strong>Type I Error (False Positive)</strong><a class="headerlink" href="#type-i-error-false-positive" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Rejecting</strong> the null hypothesis when it is <strong>true</strong>.</p></li>
<li><p>Example: A test wrongly indicates someone is <strong>pregnant</strong> when they are not.</p></li>
</ul>
</section>
<section id="type-ii-error-false-negative">
<h3><strong>Type II Error (False Negative)</strong><a class="headerlink" href="#type-ii-error-false-negative" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Failing to reject</strong> the null hypothesis when it is <strong>false</strong>.</p></li>
<li><p>Example: A test wrongly indicates someone is <strong>not pregnant</strong> when they actually are.</p></li>
</ul>
</section>
</section>
<section id="are-you-familiar-with-r-programming">
<h2>29. Are You Familiar with R Programming?<a class="headerlink" href="#are-you-familiar-with-r-programming" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>No experience yet</strong> but eager to explore.</p></li>
<li><p><strong>Strong expertise in Python programming</strong>.</p></li>
</ul>
</section>
<section id="what-is-a-hypothesis">
<h2>30. What is a Hypothesis?<a class="headerlink" href="#what-is-a-hypothesis" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>A <strong>hypothesis</strong> is a <strong>proposed explanation</strong> based on evidence.</p></li>
<li><p>It serves as the basis for further investigation.</p></li>
</ul>
<hr class="docutils" />
<section id="key-takeaways">
<h3>üìå <strong>Key Takeaways:</strong><a class="headerlink" href="#key-takeaways" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Bayes Theorem</strong> helps update probabilities using prior knowledge.</p></li>
<li><p><strong>KNN</strong> is a <strong>supervised learning</strong> algorithm; <strong>K-Means</strong> is <strong>unsupervised</strong>.</p></li>
<li><p><strong>Model training</strong> involves learning parameters from data.</p></li>
<li><p><strong>Convergence</strong> ensures the model reaches optimal performance.</p></li>
<li><p><strong>80:20 split</strong> is a common practice for training and testing data.</p></li>
<li><p><strong>p-value</strong> helps decide hypothesis test outcomes.</p></li>
<li><p><strong>F1 score</strong> balances precision and recall.</p></li>
<li><p><strong>Type I &amp; II errors</strong> affect hypothesis testing accuracy.</p></li>
</ul>
</section>
</section>
</section>
<hr class="docutils" />
<section id="linear-regression-guide">
<h1>Linear Regression Guide<a class="headerlink" href="#linear-regression-guide" title="Link to this heading">#</a></h1>
<section id="what-is-linear-regression">
<h2>1. What is Linear Regression?<a class="headerlink" href="#what-is-linear-regression" title="Link to this heading">#</a></h2>
<p><strong>Linear Regression</strong> is a supervised learning technique used to predict continuous values.</p>
<section id="id5">
<h3>Examples:<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>House Price Prediction</strong></p></li>
<li><p><strong>Employee Salary Prediction</strong></p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="types-of-linear-regression">
<h2>2. Types of Linear Regression<a class="headerlink" href="#types-of-linear-regression" title="Link to this heading">#</a></h2>
<section id="simple-linear-regression">
<h3><strong>1. Simple Linear Regression</strong><a class="headerlink" href="#simple-linear-regression" title="Link to this heading">#</a></h3>
<ul>
<li><p>Involves only one explanatory variable.</p></li>
<li><p>Formula:</p>
<p>[ y = mx + b ]</p>
<ul class="simple">
<li><p><strong>y</strong> = Prediction (output)</p></li>
<li><p><strong>x</strong> = Input (feature)</p></li>
<li><p><strong>m</strong> = Slope</p></li>
<li><p><strong>b</strong> = Y-intercept</p></li>
</ul>
</li>
</ul>
</section>
<section id="multiple-linear-regression">
<h3><strong>2. Multiple Linear Regression</strong><a class="headerlink" href="#multiple-linear-regression" title="Link to this heading">#</a></h3>
<ul>
<li><p>Involves more than one explanatory variable.</p></li>
<li><p>Formula:</p>
<p>[ f(x, y, z) = w_1 \cdot x + w_2 \cdot y + w_3 \cdot z ]</p>
<ul class="simple">
<li><p><strong>w</strong> = Weights (coefficients)</p></li>
<li><p><strong>x, y, z</strong> = Attributes (features)</p></li>
</ul>
</li>
</ul>
<section id="id6">
<h4><strong>Example:</strong><a class="headerlink" href="#id6" title="Link to this heading">#</a></h4>
<p>Predicting sales based on advertising spend:</p>
<p>[ Sales = w_1 \cdot Radio + w_2 \cdot TV + w_3 \cdot Newspaper ]</p>
</section>
</section>
</section>
<hr class="docutils" />
<section id="what-is-r-squared">
<h2>3. What is R-Squared?<a class="headerlink" href="#what-is-r-squared" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>R-squared</strong> is a statistical measure that shows how close data points are to the fitted regression line.</p></li>
<li><p>Value is between <strong>0 and 1</strong>.</p></li>
<li><p>Higher values (e.g., <strong>0.7 or 0.8</strong>) indicate a better fit.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="what-is-accuracy-in-regression">
<h2>4. What is Accuracy in Regression?<a class="headerlink" href="#what-is-accuracy-in-regression" title="Link to this heading">#</a></h2>
<p>üö® <strong>Accuracy is NOT used for regression models!</strong></p>
<ul class="simple">
<li><p>Accuracy is a classification metric, not for regression.</p></li>
<li><p>Instead, regression performance is measured using <strong>error metrics</strong>:</p>
<ul>
<li><p><strong>Mean Squared Error (MSE)</strong></p></li>
<li><p><strong>Root Mean Squared Error (RMSE)</strong></p></li>
<li><p><strong>Mean Absolute Error (MAE)</strong></p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="regression-metrics">
<h2>5. Regression Metrics<a class="headerlink" href="#regression-metrics" title="Link to this heading">#</a></h2>
<section id="mean-squared-error-mse">
<h3><strong>1. Mean Squared Error (MSE)</strong><a class="headerlink" href="#mean-squared-error-mse" title="Link to this heading">#</a></h3>
<ul>
<li><p>Average of squared differences between predicted and actual values.</p></li>
<li><p>Formula:</p>
<p>[ MSE = \frac{1}{n} \sum (y_{true} - y_{pred})^2 ]</p>
</li>
</ul>
</section>
<section id="root-mean-squared-error-rmse">
<h3><strong>2. Root Mean Squared Error (RMSE)</strong><a class="headerlink" href="#root-mean-squared-error-rmse" title="Link to this heading">#</a></h3>
<ul>
<li><p>Square root of MSE.</p></li>
<li><p>Formula:</p>
<p>[ RMSE = \sqrt{MSE} ]</p>
</li>
</ul>
</section>
<section id="mean-absolute-error-mae">
<h3><strong>3. Mean Absolute Error (MAE)</strong><a class="headerlink" href="#mean-absolute-error-mae" title="Link to this heading">#</a></h3>
<ul>
<li><p>Average absolute difference between predicted and actual values.</p></li>
<li><p>Formula:</p>
<p>[ MAE = \frac{1}{n} \sum |y_{true} - y_{pred}| ]</p>
</li>
</ul>
</section>
<section id="python-code-for-regression-metrics">
<h3><strong>Python Code for Regression Metrics:</strong><a class="headerlink" href="#python-code-for-regression-metrics" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">mean_absolute_error</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Sample data</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">11</span><span class="p">)</span>
<span class="n">predicted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mi">11</span><span class="p">)</span>

<span class="c1"># Mean Squared Error</span>
<span class="n">mse_value</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MSE:&quot;</span><span class="p">,</span> <span class="n">mse_value</span><span class="p">)</span>

<span class="c1"># Root Mean Squared Error</span>
<span class="n">rmse_value</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">predicted</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RMSE:&quot;</span><span class="p">,</span> <span class="n">rmse_value</span><span class="p">)</span>

<span class="c1"># Mean Absolute Error</span>
<span class="n">mae_value</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MAE:&quot;</span><span class="p">,</span> <span class="n">mae_value</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="what-is-overfitting">
<h2>6. What is Overfitting?<a class="headerlink" href="#what-is-overfitting" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>When a model performs well on training data but poorly on test data, it is <strong>overfitting</strong>.</p></li>
<li><p><strong>Sign:</strong> Low training error but high test error.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="what-is-correlation">
<h2>7. What is Correlation?<a class="headerlink" href="#what-is-correlation" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Correlation measures how strongly two variables are related.</p></li>
<li><p>Ranges between <strong>-1 and +1</strong>.</p></li>
<li><p>Determines both <strong>strength</strong> and <strong>direction</strong> of the relationship.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="what-is-learning-rate">
<h2>8. What is Learning Rate?<a class="headerlink" href="#what-is-learning-rate" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>A <strong>hyperparameter</strong> in optimization algorithms.</p></li>
<li><p>Determines the step size at each iteration during gradient descent.</p></li>
<li><p><strong>Choosing the right value:</strong></p>
<ul>
<li><p><strong>Too small:</strong> Slow convergence.</p></li>
<li><p><strong>Too large:</strong> May overshoot and never reach the optimal solution.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="what-is-the-intercept">
<h2>9. What is the Intercept?<a class="headerlink" href="#what-is-the-intercept" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>The constant term in regression analysis.</p></li>
<li><p>Represents the point where the regression line crosses the <strong>y-axis</strong>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="assumptions-of-linear-regression">
<h2>10. Assumptions of Linear Regression<a class="headerlink" href="#assumptions-of-linear-regression" title="Link to this heading">#</a></h2>
<p>Before using a linear regression model, ensure the following assumptions hold:</p>
<ol class="arabic simple">
<li><p><strong>Linearity</strong>: The relationship between independent and dependent variables is linear.</p></li>
<li><p><strong>Normality</strong>: For any value of <strong>x</strong>, <strong>y</strong> is normally distributed.</p></li>
<li><p><strong>Homoscedasticity</strong>: The variance of residuals is constant across all values of <strong>x</strong>.</p></li>
<li><p><strong>Independence of Errors</strong>: Residuals are independent and not correlated.</p></li>
</ol>
</section>
</section>
<hr class="docutils" />
<section id="machine-learning-essential-concepts-techniques">
<h1>Machine Learning: Essential Concepts &amp; Techniques<a class="headerlink" href="#machine-learning-essential-concepts-techniques" title="Link to this heading">#</a></h1>
<section id="normalization-vs-standardization">
<h2>41. Normalization vs Standardization<a class="headerlink" href="#normalization-vs-standardization" title="Link to this heading">#</a></h2>
<section id="normalization">
<h3><strong>Normalization</strong><a class="headerlink" href="#normalization" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Transforms data into a range between <strong>0 and 1</strong>.</p></li>
<li><p>Useful when data has different scales and needs uniformity.</p></li>
</ul>
</section>
<section id="standardization">
<h3><strong>Standardization</strong><a class="headerlink" href="#standardization" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Transforms data so that it has a <strong>mean of 0</strong> and a <strong>standard deviation of 1</strong>.</p></li>
<li><p>Helpful when data follows a normal distribution but has varying scales.</p></li>
</ul>
</section>
<section id="when-to-use">
<h3><strong>When to Use?</strong><a class="headerlink" href="#when-to-use" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>If all features are within a similar range, <strong>no need for normalization/standardization</strong>.</p></li>
<li><p>When values vary significantly, <strong>apply normalization/standardization</strong> to ensure a balanced dataset.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="model-selection-in-machine-learning">
<h2>42. Model Selection in Machine Learning<a class="headerlink" href="#model-selection-in-machine-learning" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Model selection</strong> is the process of identifying the <strong>best</strong> machine learning model for a given problem.</p></li>
<li><p>It depends on factors like <strong>dataset size, feature types, problem complexity, and computational resources</strong>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="decision-boundary">
<h2>43. Decision Boundary<a class="headerlink" href="#decision-boundary" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>A <strong>decision boundary</strong> is a line or hyperplane that separates different classes in classification problems.</p></li>
<li><p>It helps determine how new data points are classified based on their position relative to the boundary.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="how-a-logistic-regression-model-is-trained">
<h2>44. How a Logistic Regression Model is Trained<a class="headerlink" href="#how-a-logistic-regression-model-is-trained" title="Link to this heading">#</a></h2>
<ul>
<li><p>Logistic regression uses the <strong>logistic function</strong> (sigmoid function):</p>
<p>[ P(y) = \frac{1}{1 + e^{-wx}} ]</p>
</li>
<li><p><strong>Where:</strong></p>
<ul class="simple">
<li><p>( x ) = Input data</p></li>
<li><p>( w ) = Weight vector</p></li>
<li><p>( y ) = Output label</p></li>
<li><p>( P(y) ) = Probability of classification</p></li>
</ul>
</li>
<li><p><strong>Prediction Rule:</strong></p>
<ul class="simple">
<li><p>If <strong>( P(y) &gt; 0.5 )</strong> ‚Üí Predicted class = <strong>1</strong></p></li>
<li><p>If <strong>( P(y) \leq 0.5 )</strong> ‚Üí Predicted class = <strong>0</strong></p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="why-is-naive-bayes-called-naive">
<h2>45. Why is Na√Øve Bayes Called ‚ÄúNa√Øve‚Äù?<a class="headerlink" href="#why-is-naive-bayes-called-naive" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>It assumes <strong>all features are independent</strong>, which is often not the case in real-world scenarios.</p></li>
<li><p>Treats <strong>all predictors as equally important</strong>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="choosing-a-classifier-based-on-training-set-size">
<h2>46. Choosing a Classifier Based on Training Set Size<a class="headerlink" href="#choosing-a-classifier-based-on-training-set-size" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Small dataset with many features</strong> ‚Üí Use <strong>high bias/low variance</strong> models:</p>
<ul>
<li><p><strong>Na√Øve Bayes, Linear SVM</strong></p></li>
</ul>
</li>
<li><p><strong>Large dataset with fewer features</strong> ‚Üí Use <strong>low bias/high variance</strong> models:</p>
<ul>
<li><p><strong>K-NN, Decision Trees, Random Forests, Kernel SVM</strong></p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="advantages-of-naive-bayes-algorithm">
<h2>47. Advantages of Na√Øve Bayes Algorithm<a class="headerlink" href="#advantages-of-naive-bayes-algorithm" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Simple, fast, and robust</strong></p></li>
<li><p>Works well with both <strong>clean and noisy data</strong></p></li>
<li><p>Requires <strong>few training examples</strong></p></li>
<li><p>Easily calculates <strong>probabilities for predictions</strong></p></li>
</ul>
</section>
<hr class="docutils" />
<section id="choosing-the-optimal-k-in-k-nn">
<h2>48. Choosing the Optimal ( k ) in k-NN<a class="headerlink" href="#choosing-the-optimal-k-in-k-nn" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>No fixed rule, <strong>varies by dataset</strong>.</p></li>
<li><p><strong>General Guidelines:</strong></p>
<ul>
<li><p>Should be <strong>small enough</strong> to capture local patterns.</p></li>
<li><p>Should be <strong>large enough</strong> to minimize noise.</p></li>
</ul>
</li>
<li><p><strong>Elbow Method</strong> is commonly used to determine optimal ( k ).</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="is-k-nn-suitable-for-large-datasets">
<h2>49. Is k-NN Suitable for Large Datasets?<a class="headerlink" href="#is-k-nn-suitable-for-large-datasets" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Not recommended</strong> for large datasets due to:</p>
<ul>
<li><p><strong>High memory consumption</strong> (stores all training data)</p></li>
<li><p><strong>Expensive computations</strong> (calculates distance for every new sample)</p></li>
<li><p><strong>Sorting overhead</strong> (ranks all distances before classifying)</p></li>
</ul>
</li>
<li><p>Instead, consider <strong>Na√Øve Bayes or SVM</strong> for large datasets.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="k-nearest-neighbors-k-nn-algorithm">
<h2>50. k-Nearest Neighbors (k-NN) Algorithm<a class="headerlink" href="#k-nearest-neighbors-k-nn-algorithm" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>A <strong>supervised learning algorithm</strong> used for both <strong>classification</strong> and <strong>regression</strong>.</p></li>
<li><p>Assumes that <strong>similar data points are close together</strong>.</p></li>
</ul>
<section id="how-it-works">
<h3><strong>How it Works?</strong><a class="headerlink" href="#how-it-works" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Calculate distances</strong> between the new data point and all training samples.</p></li>
<li><p><strong>Sort distances</strong> in ascending order.</p></li>
<li><p><strong>Select the k nearest neighbors</strong>.</p></li>
<li><p><strong>Assign the most common class</strong> among k neighbors.</p></li>
</ol>
</section>
<section id="distance-metrics">
<h3><strong>Distance Metrics</strong><a class="headerlink" href="#distance-metrics" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Euclidean Distance (most common):</strong>
[ d(p, q) = \sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2} ]</p></li>
</ul>
</section>
<section id="visualization">
<h3><strong>Visualization:</strong><a class="headerlink" href="#visualization" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Similar points tend to cluster together</strong>, forming decision regions.</p></li>
<li><p>Choosing the <strong>right k-value</strong> prevents underfitting or overfitting.</p></li>
</ul>
</section>
</section>
</section>
<hr class="docutils" />
<section id="k-means-vs-k-nearest-neighbors-k-nn">
<h1>k-Means vs. k-Nearest Neighbors (k-NN)<a class="headerlink" href="#k-means-vs-k-nearest-neighbors-k-nn" title="Link to this heading">#</a></h1>
<section id="what-is-the-main-difference-between-k-means-and-k-nearest-neighbors">
<h2>1. What is the main difference between k-Means and k-Nearest Neighbors?<a class="headerlink" href="#what-is-the-main-difference-between-k-means-and-k-nearest-neighbors" title="Link to this heading">#</a></h2>
<section id="k-means-clustering-algorithm">
<h3>k-Means (Clustering Algorithm)<a class="headerlink" href="#k-means-clustering-algorithm" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Unsupervised learning technique.</p></li>
<li><p>Tries to partition a set of points into <strong>k clusters</strong>.</p></li>
<li><p>The points in each cluster are <strong>close to each other</strong>.</p></li>
<li><p>Used for <strong>grouping similar data points</strong> together.</p></li>
</ul>
</section>
<section id="k-nearest-neighbors-k-nn">
<h3>k-Nearest Neighbors (k-NN)<a class="headerlink" href="#k-nearest-neighbors-k-nn" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Supervised learning</strong> technique.</p></li>
<li><p>Used for <strong>classification</strong> and <strong>regression</strong> problems.</p></li>
<li><p>Classifies a point based on <strong>nearest known points</strong>.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="how-do-you-select-the-value-of-k-for-k-nearest-neighbors">
<h2>2. How do you select the value of K for k-Nearest Neighbors?<a class="headerlink" href="#how-do-you-select-the-value-of-k-for-k-nearest-neighbors" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>The value of K should be chosen based on experimentation.</p></li>
<li><p>Key observations:</p>
<ul>
<li><p><strong>Small K</strong> ‚Üí Less stable predictions.</p></li>
<li><p><strong>Large K</strong> ‚Üí More stable predictions but can lead to misclassification.</p></li>
<li><p><strong>Too large K</strong> ‚Üí Increasing number of errors.</p></li>
</ul>
</li>
<li><p>A simple approach to choose <strong>K</strong> is:</p>
<ul>
<li><p>( k = \sqrt{n} ) where <strong>n</strong> is the number of features.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="advantages-disadvantages-of-k-nearest-neighbors">
<h2>3. Advantages &amp; Disadvantages of k-Nearest Neighbors<a class="headerlink" href="#advantages-disadvantages-of-k-nearest-neighbors" title="Link to this heading">#</a></h2>
<section id="advantages">
<h3>‚úÖ Advantages<a class="headerlink" href="#advantages" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Simple and easy to implement</strong>.</p></li>
<li><p>Works for <strong>both classification and regression</strong>.</p></li>
<li><p>No need for model building or hyperparameter tuning.</p></li>
</ul>
</section>
<section id="disadvantages">
<h3>‚ùå Disadvantages<a class="headerlink" href="#disadvantages" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Becomes <strong>slower</strong> as the number of examples increases.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="applications-of-k-means-clustering">
<h2>4. Applications of k-Means Clustering<a class="headerlink" href="#applications-of-k-means-clustering" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Document Classification</strong>: Clustering documents based on topics and content.</p></li>
<li><p><strong>Insurance Fraud Detection</strong>: Identifying fraudulent claims based on past data.</p></li>
<li><p><strong>Cyber-Profiling Criminals</strong>: Identifying patterns in digital behavior.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="steps-of-k-means-clustering-algorithm">
<h2>5. Steps of k-Means Clustering Algorithm<a class="headerlink" href="#steps-of-k-means-clustering-algorithm" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>Select <strong>k</strong> random points as cluster centers.</p></li>
<li><p>Assign data points to their <strong>closest cluster center</strong> (using Euclidean distance).</p></li>
<li><p>Calculate the <strong>centroid</strong> of each cluster.</p></li>
<li><p>Repeat steps 2 &amp; 3 <strong>until cluster assignments remain unchanged</strong>.</p></li>
</ol>
</section>
<hr class="docutils" />
<section id="what-is-the-objective-function-of-k-means">
<h2>6. What is the Objective Function of k-Means?<a class="headerlink" href="#what-is-the-objective-function-of-k-means" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Minimize <strong>total intra-cluster variance</strong>, defined as:
[ \sum_{i=1}^{k} \sum_{x \in C_i} || x - \mu_i ||^2 ]</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="decision-trees">
<h1>Decision Trees<a class="headerlink" href="#decision-trees" title="Link to this heading">#</a></h1>
<section id="what-are-decision-trees">
<h2>7. What are Decision Trees?<a class="headerlink" href="#what-are-decision-trees" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Supervised learning technique</strong> used for <strong>classification and regression</strong>.</p></li>
<li><p>Uses a <strong>tree structure</strong> with a root node and child nodes.</p></li>
<li><p>Works like a set of <strong>if-else conditions</strong> to make decisions.</p></li>
</ul>
<section id="id7">
<h3>Example:<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<p><img alt="Decision Tree Example" src="https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png" /> <em>(Titanic Dataset Example)</em></p>
</section>
</section>
<hr class="docutils" />
<section id="advantages-of-decision-trees">
<h2>8. Advantages of Decision Trees<a class="headerlink" href="#advantages-of-decision-trees" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Easy to understand and interpret</strong>.</p></li>
<li><p><strong>Can be visualized</strong>.</p></li>
<li><p>Works with <strong>both numerical and categorical data</strong>.</p></li>
<li><p>Handles <strong>multiple output problems</strong>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="what-is-a-pure-node">
<h2>9. What is a Pure Node?<a class="headerlink" href="#what-is-a-pure-node" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>A node is <strong>pure</strong> if the <strong>Gini Index = 0</strong> (all elements belong to the same class).</p></li>
<li><p>When a pure node is reached, it becomes a <strong>leaf node</strong> that represents the final classification.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="how-to-handle-overfitting-in-decision-trees">
<h2>10. How to Handle Overfitting in Decision Trees?<a class="headerlink" href="#how-to-handle-overfitting-in-decision-trees" title="Link to this heading">#</a></h2>
<section id="solution-pruning-reducing-complexity-of-the-tree">
<h3>Solution: <strong>Pruning</strong> (Reducing complexity of the tree)<a class="headerlink" href="#solution-pruning-reducing-complexity-of-the-tree" title="Link to this heading">#</a></h3>
</section>
<section id="bottom-up-pruning">
<h3>üîΩ Bottom-Up Pruning<a class="headerlink" href="#bottom-up-pruning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Starts from <strong>leaf nodes</strong> and moves upward.</p></li>
<li><p>Removes nodes <strong>that do not contribute to classification</strong>.</p></li>
</ul>
</section>
<section id="top-down-pruning">
<h3>üîº Top-Down Pruning<a class="headerlink" href="#top-down-pruning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Starts at the <strong>root</strong> and checks for relevance.</p></li>
<li><p>Removes entire <strong>sub-trees</strong> if they do not contribute significantly.</p></li>
</ul>
</section>
<section id="reduced-error-pruning">
<h3>üîß Reduced Error Pruning<a class="headerlink" href="#reduced-error-pruning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Each node is replaced by its <strong>most common class</strong>.</p></li>
<li><p>If accuracy <strong>does not decrease</strong>, the node is pruned.</p></li>
</ul>
</section>
</section>
</section>
<hr class="docutils" />
<section id="machine-learning-concepts-structured-guide">
<h1>üìå Machine Learning Concepts: Structured Guide<a class="headerlink" href="#machine-learning-concepts-structured-guide" title="Link to this heading">#</a></h1>
<section id="greedy-splitting">
<h2>1Ô∏è‚É£ Greedy Splitting<a class="headerlink" href="#greedy-splitting" title="Link to this heading">#</a></h2>
<p><strong>Definition:</strong></p>
<ul class="simple">
<li><p>Also known as <em>Recursive Binary Splitting</em>.</p></li>
<li><p>Evaluates all features and possible split points using a cost function.</p></li>
<li><p>Chooses the split with the lowest cost in a <em>greedy manner</em> (minimizing cost at each step).</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="entropy">
<h2>2Ô∏è‚É£ Entropy<a class="headerlink" href="#entropy" title="Link to this heading">#</a></h2>
<p><strong>Definition:</strong></p>
<ul class="simple">
<li><p>Entropy is a measure of disorder in data.</p></li>
<li><p>In Machine Learning, the goal is to reduce uncertainty ‚Üí lower entropy.</p></li>
<li><p>The reduction in entropy is called <strong>Information Gain</strong>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="tree-bagging-bootstrap-aggregating">
<h2>3Ô∏è‚É£ Tree Bagging (Bootstrap Aggregating)<a class="headerlink" href="#tree-bagging-bootstrap-aggregating" title="Link to this heading">#</a></h2>
<p><strong>What is it?</strong></p>
<ul class="simple">
<li><p>An <em>ensemble learning method</em> that reduces variance in a dataset.</p></li>
<li><p>Creates multiple decision trees by randomly sampling data <strong>with replacement</strong>.</p></li>
<li><p>Each tree is trained independently and then aggregated to make predictions.</p></li>
</ul>
<p><strong>Key Points:</strong></p>
<ul class="simple">
<li><p>Data points may be chosen more than once (bootstrapping).</p></li>
<li><p>Decision trees are <strong>deep</strong> and <strong>not pruned</strong>.</p></li>
<li><p>Majority voting (classification) or averaging (regression) is used for predictions.</p></li>
<li><p><strong>Random Forest</strong> is an extension of Bagging with added feature randomness.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="tree-boosting">
<h2>4Ô∏è‚É£ Tree Boosting<a class="headerlink" href="#tree-boosting" title="Link to this heading">#</a></h2>
<p><strong>What is it?</strong></p>
<ul class="simple">
<li><p>An <em>ensemble meta-algorithm</em> that reduces both <strong>bias</strong> and <strong>variance</strong>.</p></li>
<li><p>Models are trained <strong>sequentially</strong>, correcting previous errors at each step.</p></li>
</ul>
<p><strong>How it Works:</strong></p>
<ol class="arabic simple">
<li><p>Each decision tree learns from the errors of the previous tree.</p></li>
<li><p>When an input is misclassified, its <strong>weight is increased</strong> to help the next tree classify it correctly.</p></li>
<li><p>Trees work <strong>collaboratively</strong>, rather than independently, like in Bagging.</p></li>
</ol>
</section>
<hr class="docutils" />
<section id="handling-outliers-in-logistic-regression">
<h2>5Ô∏è‚É£ Handling Outliers in Logistic Regression<a class="headerlink" href="#handling-outliers-in-logistic-regression" title="Link to this heading">#</a></h2>
<p><strong>Should you use Logistic Regression if outliers are present?</strong> ‚ùå No!</p>
<p><strong>Why?</strong></p>
<ul class="simple">
<li><p>Logistic Regression is <strong>highly sensitive to outliers</strong>.</p></li>
<li><p>Outliers shift the <strong>decision boundary</strong>, leading to incorrect predictions.</p></li>
</ul>
<p><strong>Alternative:</strong>
‚úî <strong>Tree-Based Models</strong> (Decision Trees, Random Forests) are more robust to outliers.</p>
</section>
<hr class="docutils" />
<section id="entropy-in-decision-trees">
<h2>6Ô∏è‚É£ Entropy in Decision Trees<a class="headerlink" href="#entropy-in-decision-trees" title="Link to this heading">#</a></h2>
<p><strong>Usage:</strong></p>
<ul class="simple">
<li><p>Helps determine <strong>whether to split</strong> a node.</p></li>
<li><p><strong>Lower entropy = more homogeneity in a split</strong>.</p></li>
<li><p>The goal is to maximize <strong>Information Gain</strong> (i.e., decrease entropy as much as possible).</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="random-forest">
<h2>7Ô∏è‚É£ Random Forest<a class="headerlink" href="#random-forest" title="Link to this heading">#</a></h2>
<p><strong>Definition:</strong></p>
<ul class="simple">
<li><p>An <em>ensemble learning method</em> for <strong>classification &amp; regression</strong>.</p></li>
<li><p>Constructs <strong>multiple decision trees</strong> and aggregates results.</p></li>
<li><p><strong>Classification Output:</strong> Majority vote across trees.</p></li>
<li><p><strong>Regression Output:</strong> Mean of all tree predictions.</p></li>
</ul>
<p><strong>Why Use It?</strong>
‚úî Reduces <strong>overfitting</strong> compared to a single decision tree.
‚úî Works well with large datasets.
‚úî Handles missing data effectively.</p>
</section>
<hr class="docutils" />
<section id="does-random-forest-require-pruning">
<h2>8Ô∏è‚É£ Does Random Forest Require Pruning?<a class="headerlink" href="#does-random-forest-require-pruning" title="Link to this heading">#</a></h2>
<p>‚ùå <strong>No!</strong> Unlike single decision trees, Random Forests do not need pruning.</p>
<p><strong>Why?</strong></p>
<ul class="simple">
<li><p>Trees are built on <strong>random subsets of data &amp; features</strong>, preventing overfitting.</p></li>
<li><p>Each tree is <strong>uncorrelated</strong>, ensuring generalization.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="ensemble-methods">
<h2>9Ô∏è‚É£ Ensemble Methods<a class="headerlink" href="#ensemble-methods" title="Link to this heading">#</a></h2>
<p><strong>What are they?</strong></p>
<ul class="simple">
<li><p>Machine learning techniques that <strong>combine multiple models</strong> to improve accuracy.</p></li>
<li><p><strong>Types:</strong></p>
<ul>
<li><p>Bagging (Random Forest)</p></li>
<li><p>Boosting (Gradient Boosting, AdaBoost, XGBoost)</p></li>
<li><p>Stacking (combining multiple models)</p></li>
</ul>
</li>
</ul>
<p><strong>Why Use Ensemble Methods?</strong>
‚úî Improve accuracy
‚úî Reduce variance
‚úî Handle complex relationships better than individual models</p>
</section>
</section>
<hr class="docutils" />
<section id="random-forest-a-comprehensive-guide">
<h1>Random Forest: A Comprehensive Guide<a class="headerlink" href="#random-forest-a-comprehensive-guide" title="Link to this heading">#</a></h1>
<section id="hyperparameters-in-random-forest">
<h2>1. Hyperparameters in Random Forest<a class="headerlink" href="#hyperparameters-in-random-forest" title="Link to this heading">#</a></h2>
<p>Random Forest has several key hyperparameters that influence its performance:</p>
<ul class="simple">
<li><p><strong>Number of decision trees</strong> in the forest.</p></li>
<li><p><strong>Number of features</strong> considered by each tree when splitting a node.</p></li>
<li><p><strong>Maximum depth</strong> of the individual trees.</p></li>
<li><p><strong>Minimum samples</strong> required to split an internal node.</p></li>
<li><p><strong>Maximum number of leaf nodes.</strong></p></li>
<li><p><strong>Number of random features.</strong></p></li>
<li><p><strong>Size of the bootstrapped dataset.</strong></p></li>
</ul>
</section>
<section id="is-cross-validation-necessary-in-random-forest">
<h2>2. Is Cross-Validation Necessary in Random Forest?<a class="headerlink" href="#is-cross-validation-necessary-in-random-forest" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Out-of-Bag Error (OOB)</strong> is a built-in validation method in Random Forest.</p></li>
<li><p>OOB error is calculated using data that was not included in the training of each tree.</p></li>
<li><p>Since OOB is similar to cross-validation, performing an additional cross-validation is <strong>not necessary</strong>.</p></li>
</ul>
</section>
<section id="is-random-forest-an-ensemble-algorithm">
<h2>3. Is Random Forest an Ensemble Algorithm?<a class="headerlink" href="#is-random-forest-an-ensemble-algorithm" title="Link to this heading">#</a></h2>
<p>Yes, Random Forest is a tree-based <strong>ensemble learning algorithm</strong> that:</p>
<ul class="simple">
<li><p>Uses multiple <strong>decision trees</strong> to make predictions.</p></li>
<li><p>Applies <strong>bagging</strong> (Bootstrap Aggregation) as its ensemble method.</p></li>
<li><p>Works for <strong>both classification and regression</strong> problems:</p>
<ul>
<li><p><strong>Classification</strong>: The final prediction is the class selected by the majority of trees.</p></li>
<li><p><strong>Regression</strong>: The final prediction is the <strong>mean</strong> of all tree outputs.</p></li>
</ul>
</li>
</ul>
</section>
<section id="handling-missing-values-in-random-forest">
<h2>4. Handling Missing Values in Random Forest<a class="headerlink" href="#handling-missing-values-in-random-forest" title="Link to this heading">#</a></h2>
<p>Random Forest offers two primary methods for handling missing values:</p>
<ol class="arabic simple">
<li><p><strong>Dropping Data Points with Missing Values</strong> (Not recommended, as it reduces data availability).</p></li>
<li><p><strong>Imputation</strong>:</p>
<ul class="simple">
<li><p>Numerical values: Replace missing values with the <strong>median</strong>.</p></li>
<li><p>Categorical values: Replace missing values with the <strong>mode</strong>.</p></li>
<li><p>More advanced techniques involve estimating missing values based on similarity weights.</p></li>
</ul>
</li>
</ol>
</section>
<section id="variable-selection-in-random-forest">
<h2>5. Variable Selection in Random Forest<a class="headerlink" href="#variable-selection-in-random-forest" title="Link to this heading">#</a></h2>
<p>Variable selection refers to choosing the most important features for the model. It has two key objectives:</p>
<ul class="simple">
<li><p><strong>Interpretation:</strong> Selecting features that are strongly related to the target variable.</p></li>
<li><p><strong>Prediction:</strong> Selecting a minimal subset of features that maximize predictive accuracy.</p></li>
</ul>
</section>
<section id="why-is-random-forest-considered-non-interpretable">
<h2>6. Why is Random Forest Considered Non-Interpretable?<a class="headerlink" href="#why-is-random-forest-considered-non-interpretable" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Individual decision trees are easy to interpret as they follow <strong>if-else rules</strong>.</p></li>
<li><p>Random Forest, however, consists of <strong>many trees</strong>, making it difficult to explain <strong>why</strong> a particular decision was made.</p></li>
<li><p>The <strong>more trees</strong>, the harder it becomes to interpret the model.</p></li>
</ul>
</section>
<section id="advantages-of-random-forest">
<h2>7. Advantages of Random Forest<a class="headerlink" href="#advantages-of-random-forest" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Works well for <strong>both classification and regression</strong> tasks.</p></li>
<li><p>Can handle <strong>binary, categorical, and numerical</strong> features.</p></li>
<li><p>Supports <strong>parallel processing</strong>, allowing efficient computation.</p></li>
<li><p>Handles <strong>high-dimensional</strong> data effectively by working on feature subsets.</p></li>
<li><p>Faster training than a single decision tree (as it operates on subsets of data).</p></li>
<li><p>Performs well even with <strong>hundreds of features</strong>.</p></li>
</ul>
</section>
<section id="drawbacks-of-random-forest">
<h2>8. Drawbacks of Random Forest<a class="headerlink" href="#drawbacks-of-random-forest" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Lack of Interpretability</strong>: Acts as a ‚Äúblack-box‚Äù model.</p></li>
<li><p><strong>Memory Intensive</strong>: Large datasets require significant storage for multiple trees.</p></li>
<li><p><strong>Overfitting Risk</strong>: Requires hyperparameter tuning to prevent overfitting.</p></li>
<li><p><strong>Slow Real-Time Predictions</strong>: A large number of trees can slow down prediction speed.</p></li>
</ul>
</section>
<section id="understanding-bagging-bootstrap-aggregating">
<h2>9. Understanding BAGGing (Bootstrap Aggregating)<a class="headerlink" href="#understanding-bagging-bootstrap-aggregating" title="Link to this heading">#</a></h2>
<p>BAGGing is an ensemble method that improves the stability and accuracy of models by:</p>
<ol class="arabic simple">
<li><p>Drawing <strong>multiple bootstrapped subsamples</strong> from the dataset.</p></li>
<li><p>Training a <strong>decision tree</strong> on each subsample.</p></li>
<li><p>Aggregating predictions from all trees to create a final model.</p></li>
</ol>
<p><strong>Process:</strong></p>
<ul class="simple">
<li><p>Given a dataset, multiple random subsamples are taken <strong>with replacement</strong>.</p></li>
<li><p>Each sample is used to train a <strong>decision tree</strong>.</p></li>
<li><p>The final prediction is obtained by <strong>aggregating</strong> all the trees (majority vote for classification, average for regression).</p></li>
</ul>
</section>
<section id="difference-between-oob-score-and-validation-score">
<h2>10. Difference Between OOB Score and Validation Score<a class="headerlink" href="#difference-between-oob-score-and-validation-score" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>OOB Score:</strong></p>
<ul>
<li><p>Calculated using only the data points <strong>not used in training</strong> a particular tree.</p></li>
<li><p>Uses a subset of decision trees.</p></li>
<li><p>Provides an <strong>unbiased</strong> estimation of model performance.</p></li>
</ul>
</li>
<li><p><strong>Validation Score:</strong></p>
<ul>
<li><p>Calculated using a dedicated validation dataset.</p></li>
<li><p>Uses <strong>all</strong> decision trees in the model.</p></li>
<li><p>Typically obtained via <strong>train-test split</strong> or cross-validation.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="machine-learning-concepts-random-forest-support-vector-machines-svm">
<h1>Machine Learning Concepts: Random Forest &amp; Support Vector Machines (SVM)<a class="headerlink" href="#machine-learning-concepts-random-forest-support-vector-machines-svm" title="Link to this heading">#</a></h1>
<section id="what-are-proximities-in-random-forests">
<h2>31. What are Proximities in Random Forests?<a class="headerlink" href="#what-are-proximities-in-random-forests" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Proximity</strong> refers to the closeness or nearness between pairs of cases.</p></li>
<li><p>It is calculated for each pair of cases/observations/sample points.</p></li>
<li><p><strong>Uses of Proximities:</strong></p>
<ul>
<li><p>Replacing missing data.</p></li>
<li><p>Locating outliers.</p></li>
<li><p>Producing low-dimensional visualizations of the data.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="how-to-define-the-criteria-to-split-at-each-node-of-the-trees">
<h2>32. How to Define the Criteria to Split at Each Node of the Trees?<a class="headerlink" href="#how-to-define-the-criteria-to-split-at-each-node-of-the-trees" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Decision Trees make <strong>locally optimal</strong> decisions at each node.</p></li>
<li><p>The best feature and split value are chosen using:</p>
<ul>
<li><p><strong>Classification</strong>: Gini Index or Entropy.</p></li>
<li><p><strong>Regression</strong>: Mean Absolute Error (MAE) or Mean Squared Error (MSE).</p></li>
</ul>
</li>
<li><p><strong>Fine-tuning the split criteria</strong> can impact the accuracy and performance of the model.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="what-is-the-adaboost-algorithm">
<h2>33. What is the AdaBoost Algorithm?<a class="headerlink" href="#what-is-the-adaboost-algorithm" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>AdaBoost (Adaptive Boosting)</strong> is a boosting ensemble method.</p></li>
<li><p>Differences from Random Forest:</p>
<ul>
<li><p>AdaBoost creates a <strong>forest of stumps</strong> (a stump is a tree with only one node and two leaves).</p></li>
<li><p>Each stump‚Äôs decision <strong>is weighted</strong> based on accuracy.</p></li>
<li><p>Each subsequent stump focuses more on <strong>previously misclassified</strong> samples.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="how-does-logistic-regression-handle-outliers">
<h2>34. How Does Logistic Regression Handle Outliers?<a class="headerlink" href="#how-does-logistic-regression-handle-outliers" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Logistic Regression</strong> is highly influenced by outliers.</p>
<ul>
<li><p>Outliers can shift the <strong>decision boundary</strong>, leading to incorrect predictions.</p></li>
</ul>
</li>
<li><p><strong>Alternative:</strong> Tree-based models (Decision Trees, Random Forests) are more robust to outliers since they split the data based on threshold values.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="what-is-a-support-vector-machine-svm">
<h2>35. What is a Support Vector Machine (SVM)?<a class="headerlink" href="#what-is-a-support-vector-machine-svm" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>SVM</strong> is a supervised learning algorithm used for:</p>
<ul>
<li><p><strong>Classification</strong></p></li>
<li><p><strong>Regression</strong></p></li>
<li><p><strong>Outlier Detection</strong></p></li>
</ul>
</li>
<li><p>The goal is to find a <strong>hyperplane</strong> in an N-dimensional space that distinctly separates data points.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="what-is-a-hyperplane-in-svm">
<h2>36. What is a Hyperplane in SVM?<a class="headerlink" href="#what-is-a-hyperplane-in-svm" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Hyperplanes</strong> are decision boundaries that classify data points.</p></li>
<li><p>The number of dimensions of the hyperplane depends on the number of features:</p>
<ul>
<li><p><strong>2 features</strong> ‚Üí Hyperplane is a <strong>line</strong>.</p></li>
<li><p><strong>3 features</strong> ‚Üí Hyperplane is a <strong>plane</strong>.</p></li>
</ul>
</li>
<li><p>The <strong>best hyperplane</strong> is the one that maximizes the margin between classes.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="what-are-support-vectors-in-svm">
<h2>37. What are Support Vectors in SVM?<a class="headerlink" href="#what-are-support-vectors-in-svm" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Support Vectors</strong> are the data points closest to the hyperplane.</p></li>
<li><p>These points define the <strong>margin</strong> of the classifier.</p></li>
<li><p>Only support vectors are used for computing predictions.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="types-of-svm-kernels">
<h2>38. Types of SVM Kernels<a class="headerlink" href="#types-of-svm-kernels" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>Linear Kernel</strong>: Best for high-dimensional feature spaces.</p></li>
<li><p><strong>Polynomial Kernel</strong>: Extends the linear kernel by considering higher-degree interactions.</p></li>
<li><p><strong>Radial Basis Function (RBF) Kernel</strong>: Works well for non-linearly separable data.</p></li>
<li><p><strong>Sigmoid Kernel</strong>: Used in neural networks as an activation function.</p></li>
</ol>
</section>
<hr class="docutils" />
<section id="why-use-the-kernel-trick">
<h2>39. Why Use the Kernel Trick?<a class="headerlink" href="#why-use-the-kernel-trick" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>The <strong>Kernel Trick</strong> helps map non-linearly separable data into a higher-dimensional space without explicitly computing the transformation.</p></li>
<li><p><strong>Advantages:</strong></p>
<ul>
<li><p>Makes computations more efficient.</p></li>
<li><p>Reduces computational cost while improving classification accuracy.</p></li>
</ul>
</li>
<li><p>Common Kernel Functions:</p>
<ul>
<li><p><strong>Linear</strong></p></li>
<li><p><strong>Polynomial</strong></p></li>
<li><p><strong>RBF</strong></p></li>
<li><p><strong>Sigmoid</strong></p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="applications-of-svms">
<h2>40. Applications of SVMs<a class="headerlink" href="#applications-of-svms" title="Link to this heading">#</a></h2>
<section id="face-detection">
<h3><strong>1. Face Detection</strong><a class="headerlink" href="#face-detection" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>SVMs classify parts of images as <strong>face or non-face</strong>.</p></li>
<li><p>A bounding box is created around detected faces.</p></li>
</ul>
</section>
<section id="text-hypertext-categorization">
<h3><strong>2. Text &amp; Hypertext Categorization</strong><a class="headerlink" href="#text-hypertext-categorization" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>SVMs classify documents based on <strong>topic or category</strong>.</p></li>
<li><p>Works by generating a score and comparing it to a threshold.</p></li>
</ul>
</section>
<section id="image-classification">
<h3><strong>3. Image Classification</strong><a class="headerlink" href="#image-classification" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>SVMs improve accuracy in <strong>image recognition tasks</strong>.</p></li>
</ul>
</section>
<section id="bioinformatics">
<h3><strong>4. Bioinformatics</strong><a class="headerlink" href="#bioinformatics" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Used for <strong>protein classification</strong> and <strong>cancer detection</strong>.</p></li>
<li><p>Helps in <strong>gene classification</strong> and other biological analyses.</p></li>
</ul>
</section>
<section id="protein-fold-homology-detection">
<h3><strong>5. Protein Fold &amp; Homology Detection</strong><a class="headerlink" href="#protein-fold-homology-detection" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>SVMs help in <strong>remote homology detection</strong> of proteins.</p></li>
</ul>
</section>
<section id="handwriting-recognition">
<h3><strong>6. Handwriting Recognition</strong><a class="headerlink" href="#handwriting-recognition" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Used for recognizing <strong>handwritten characters</strong>.</p></li>
</ul>
</section>
<section id="generalized-predictive-control-gpc">
<h3><strong>7. Generalized Predictive Control (GPC)</strong><a class="headerlink" href="#generalized-predictive-control-gpc" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>SVM-based GPC is used to <strong>control chaotic dynamics</strong> in predictive modeling.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="conclusion">
<h3><strong>Conclusion</strong><a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Random Forest</strong> is an ensemble method that improves accuracy through <strong>bagging</strong>.</p></li>
<li><p><strong>Support Vector Machines (SVMs)</strong> use hyperplanes and kernel functions to <strong>classify data points effectively</strong>.</p></li>
<li><p>Both are widely used in <strong>classification, regression, and anomaly detection</strong> tasks.</p></li>
</ul>
</section>
</section>
</section>
<hr class="docutils" />
<section id="support-vector-machine-svm-and-anomaly-detection-guide">
<h1>Support Vector Machine (SVM) and Anomaly Detection Guide<a class="headerlink" href="#support-vector-machine-svm-and-anomaly-detection-guide" title="Link to this heading">#</a></h1>
<section id="role-of-c-hyperparameter-in-svm">
<h2>41. Role of C Hyperparameter in SVM<a class="headerlink" href="#role-of-c-hyperparameter-in-svm" title="Link to this heading">#</a></h2>
<p>In an SVM, you are optimizing two objectives:</p>
<ul class="simple">
<li><p>Finding a hyperplane with the largest minimum margin.</p></li>
<li><p>Ensuring the hyperplane correctly separates as many instances as possible.</p></li>
</ul>
<p>The <strong>C hyperparameter</strong> controls the balance between these objectives:</p>
<ul class="simple">
<li><p>A <strong>small C</strong> allows a larger margin, tolerating some misclassified points.</p></li>
<li><p>A <strong>large C</strong> forces correct classification of training examples but may lead to overfitting.</p></li>
</ul>
</section>
<section id="what-are-support-vectors">
<h2>42. What are Support Vectors?<a class="headerlink" href="#what-are-support-vectors" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Support vectors are <strong>data points closest to the hyperplane</strong>.</p></li>
<li><p>They influence the <strong>position and orientation</strong> of the hyperplane.</p></li>
<li><p><strong>Maximizing the margin</strong> is done using these vectors.</p></li>
<li><p><strong>Removing support vectors</strong> would change the hyperplane.</p></li>
</ul>
</section>
<section id="what-is-anomaly-detection">
<h2>43. What is Anomaly Detection?<a class="headerlink" href="#what-is-anomaly-detection" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Anomaly detection</strong> (or outlier detection) is the <strong>identification of rare events, observations, or items</strong> that differ significantly from the majority of the data.</p></li>
<li><p>Used in <strong>fraud detection, network security, and system monitoring</strong>.</p></li>
</ul>
</section>
<section id="why-do-we-care-about-anomalies">
<h2>44. Why Do We Care About Anomalies?<a class="headerlink" href="#why-do-we-care-about-anomalies" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Outliers can <strong>distort model performance</strong>.</p></li>
<li><p><strong>Analyzing outliers</strong> helps understand data distribution and prevent errors.</p></li>
<li><p><strong>Eliminating anomalies</strong> can improve model accuracy and efficiency.</p></li>
</ul>
</section>
<section id="id8">
<h2>45. Normalization vs. Standardization<a class="headerlink" href="#id8" title="Link to this heading">#</a></h2>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Purpose</p></th>
<th class="head"><p>Formula</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Normalization</strong></p></td>
<td><p>Rescales values to <strong>[0,1]</strong> range</p></td>
<td><p>( X‚Äô = \frac{X - X_{min}}{X_{max} - X_{min}} )</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Standardization</strong></p></td>
<td><p>Rescales data to mean = <strong>0</strong> and std. dev = <strong>1</strong></p></td>
<td><p>( X‚Äô = \frac{X - \mu}{\sigma} )</p></td>
</tr>
</tbody>
</table>
</div>
<ul class="simple">
<li><p><strong>Normalization</strong> is useful when <strong>features have different scales</strong>.</p></li>
<li><p><strong>Standardization</strong> is preferred for <strong>normally distributed data</strong>.</p></li>
</ul>
</section>
<section id="the-68-95-99-7-rule-for-normal-distribution">
<h2>46. The 68-95-99.7 Rule for Normal Distribution<a class="headerlink" href="#the-68-95-99-7-rule-for-normal-distribution" title="Link to this heading">#</a></h2>
<p>The <strong>Empirical Rule</strong> states:</p>
<ul class="simple">
<li><p><strong>68%</strong> of data falls within <strong>1 standard deviation (œÉ)</strong>.</p></li>
<li><p><strong>95%</strong> of data falls within <strong>2 standard deviations (2œÉ)</strong>.</p></li>
<li><p><strong>99.7%</strong> of data falls within <strong>3 standard deviations (3œÉ)</strong>.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>     <span class="mi">68</span><span class="o">%</span>    <span class="mi">95</span><span class="o">%</span>   <span class="mf">99.7</span><span class="o">%</span>
 <span class="o">|----|----|----|----|----|</span>
<span class="o">-</span><span class="mi">3</span><span class="n">œÉ</span>  <span class="o">-</span><span class="mi">2</span><span class="n">œÉ</span>  <span class="o">-</span><span class="mi">1</span><span class="n">œÉ</span>   <span class="n">Œº</span>   <span class="o">+</span><span class="mi">1</span><span class="n">œÉ</span>  <span class="o">+</span><span class="mi">2</span><span class="n">œÉ</span>  <span class="o">+</span><span class="mi">3</span><span class="n">œÉ</span>
</pre></div>
</div>
</section>
<section id="how-is-iqr-interquartile-range-used-in-time-series-forecasting">
<h2>47. How is IQR (Interquartile Range) Used in Time Series Forecasting?<a class="headerlink" href="#how-is-iqr-interquartile-range-used-in-time-series-forecasting" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>IQR = Q3 - Q1</strong> (Range between the 1st and 3rd quartile).</p></li>
<li><p><strong>50% of the data</strong> falls within <strong>IQR</strong>.</p></li>
<li><p><strong>Detects outliers</strong>: Points <strong>outside Q1 - 1.5√óIQR or Q3 + 1.5√óIQR</strong> are potential anomalies.</p></li>
</ul>
</section>
<section id="using-standard-deviation-for-anomaly-detection">
<h2>48. Using Standard Deviation for Anomaly Detection<a class="headerlink" href="#using-standard-deviation-for-anomaly-detection" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>If data follows a <strong>normal distribution</strong>, standard deviation can identify anomalies:</p>
<ul>
<li><p><strong>68%</strong> within <strong>1œÉ</strong>.</p></li>
<li><p><strong>95%</strong> within <strong>2œÉ</strong>.</p></li>
<li><p><strong>99.7%</strong> within <strong>3œÉ</strong>.</p></li>
</ul>
</li>
<li><p>Points <strong>beyond 3œÉ</strong> are likely outliers.</p></li>
</ul>
</section>
<section id="can-you-find-outliers-using-k-means">
<h2>49. Can You Find Outliers Using k-Means?<a class="headerlink" href="#can-you-find-outliers-using-k-means" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>k-Means is not optimal for outlier detection</strong> because:</p>
<ul>
<li><p>It minimizes within-cluster variance.</p></li>
<li><p>Outliers may not form a separate cluster.</p></li>
</ul>
</li>
<li><p><strong>Better alternatives:</strong> DBSCAN, Isolation Forest, LOF (Local Outlier Factor).</p></li>
</ul>
</section>
<section id="how-to-handle-outliers-in-a-dataset">
<h2>50. How to Handle Outliers in a Dataset?<a class="headerlink" href="#how-to-handle-outliers-in-a-dataset" title="Link to this heading">#</a></h2>
<section id="approaches">
<h3>Approaches:<a class="headerlink" href="#approaches" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Univariate Method</strong></p>
<ul class="simple">
<li><p>Uses boxplots to detect extreme values in a <strong>single feature</strong>.</p></li>
<li><p>Outliers lie outside the whiskers of the boxplot.</p></li>
</ul>
</li>
<li><p><strong>Multivariate Method</strong></p>
<ul class="simple">
<li><p>Analyzes <strong>relationships between multiple features</strong>.</p></li>
<li><p>Uses techniques like <strong>Mahalanobis distance</strong> and <strong>PCA</strong>.</p></li>
</ul>
</li>
<li><p><strong>Machine Learning-Based</strong></p>
<ul class="simple">
<li><p><strong>Isolation Forest</strong> (randomly partitions data to detect anomalies).</p></li>
<li><p><strong>Autoencoders</strong> (deep learning-based outlier detection).</p></li>
</ul>
</li>
<li><p><strong>Domain Knowledge-Based</strong></p>
<ul class="simple">
<li><p>Using industry-specific thresholds for data cleaning.</p></li>
</ul>
</li>
</ol>
</section>
</section>
</section>
<hr class="docutils" />
<section id="machine-learning-interview-questions-answers">
<h1>Machine Learning Interview Questions &amp; Answers<a class="headerlink" href="#machine-learning-interview-questions-answers" title="Link to this heading">#</a></h1>
<section id="what-is-bias-in-machine-learning">
<h2>1. What is Bias in Machine Learning?<a class="headerlink" href="#what-is-bias-in-machine-learning" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>In supervised machine learning, an algorithm learns a model from training data.</p></li>
<li><p>The goal of any supervised machine learning algorithm is to best estimate the mapping function <strong>f(X) ‚Üí Y</strong>.</p></li>
<li><p>The mapping function is often called the <strong>target function</strong>.</p></li>
<li><p><strong>Bias</strong> refers to the simplifying assumptions a model makes to generalize well.</p></li>
</ul>
<section id="characteristics-of-bias">
<h3>Characteristics of Bias:<a class="headerlink" href="#characteristics-of-bias" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>High-bias models</strong>: Simple models that may underfit the data.</p>
<ul>
<li><p>Examples: <strong>Linear Regression, Logistic Regression, Linear Discriminant Analysis</strong>.</p></li>
</ul>
</li>
<li><p><strong>Low-bias models</strong>: More flexible and complex.</p>
<ul>
<li><p>Examples: <strong>Decision Trees, k-Nearest Neighbors, Support Vector Machines</strong>.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="what-is-the-bias-variance-tradeoff">
<h2>2. What is the Bias-Variance Tradeoff?<a class="headerlink" href="#what-is-the-bias-variance-tradeoff" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>High Bias</strong> ‚Üí Model oversimplifies, leading to <strong>underfitting</strong>.</p></li>
<li><p><strong>High Variance</strong> ‚Üí Model is too complex, leading to <strong>overfitting</strong>.</p></li>
</ul>
<section id="solution">
<h3>Solution:<a class="headerlink" href="#solution" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The tradeoff is about <strong>finding a balance</strong> where the model works accurately on unseen data.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="how-to-identify-and-fix-a-high-bias-model">
<h2>3. How to Identify and Fix a High-Bias Model?<a class="headerlink" href="#how-to-identify-and-fix-a-high-bias-model" title="Link to this heading">#</a></h2>
<section id="identifying-high-bias">
<h3>Identifying High Bias:<a class="headerlink" href="#identifying-high-bias" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>High training error</strong>.</p></li>
<li><p><strong>Validation/test error ‚âà training error</strong>.</p></li>
</ul>
</section>
<section id="fixing-high-bias">
<h3>Fixing High Bias:<a class="headerlink" href="#fixing-high-bias" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Add <strong>more input features</strong>.</p></li>
<li><p>Increase <strong>model complexity</strong> (e.g., use polynomial features).</p></li>
<li><p><strong>Reduce regularization</strong>.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="what-types-of-classification-algorithms-exist">
<h2>4. What Types of Classification Algorithms Exist?<a class="headerlink" href="#what-types-of-classification-algorithms-exist" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Logistic Regression</strong>: Used for <strong>binary classification</strong> (sigmoid function).</p></li>
<li><p><strong>k-Nearest Neighbors (kNN)</strong>: Classifies data by majority vote of nearest neighbors.</p></li>
<li><p><strong>Decision Trees</strong>: Tree structure where nodes split data into smaller groups.</p></li>
<li><p><strong>Random Forest</strong>: Uses multiple decision trees and aggregates results.</p></li>
<li><p><strong>Support Vector Machines (SVMs)</strong>: Creates hyperplanes to separate data.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="how-does-the-adaboost-algorithm-work">
<h2>5. How Does the AdaBoost Algorithm Work?<a class="headerlink" href="#how-does-the-adaboost-algorithm-work" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Adaptive Boosting (AdaBoost)</strong> is an ensemble learning method.</p></li>
<li><p>It combines multiple <strong>weak classifiers</strong> (low accuracy) to form a <strong>strong classifier</strong>.</p></li>
<li><p>Works by:</p>
<ol class="arabic simple">
<li><p>Giving more weight to misclassified points.</p></li>
<li><p>Training weak classifiers in sequence.</p></li>
<li><p>Combining them to make better predictions.</p></li>
</ol>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="what-is-a-confusion-matrix">
<h2>6. What is a Confusion Matrix?<a class="headerlink" href="#what-is-a-confusion-matrix" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>A <strong>confusion matrix</strong> evaluates classification model performance.</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Predicted Positive</p></th>
<th class="head"><p>Predicted Negative</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Actual Positive</strong></p></td>
<td><p>True Positive (TP)</p></td>
<td><p>False Negative (FN)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Actual Negative</strong></p></td>
<td><p>False Positive (FP)</p></td>
<td><p>True Negative (TN)</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<hr class="docutils" />
<section id="how-do-roc-curve-auc-measure-model-performance">
<h2>7. How Do ROC Curve &amp; AUC Measure Model Performance?<a class="headerlink" href="#how-do-roc-curve-auc-measure-model-performance" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>ROC Curve (Receiver Operating Characteristic Curve)</strong>:</p>
<ul>
<li><p>Plots <strong>True Positive Rate (TPR)</strong> vs. <strong>False Positive Rate (FPR)</strong>.</p></li>
<li><p>Helps visualize classification model performance.</p></li>
</ul>
</li>
<li><p><strong>AUC (Area Under the ROC Curve)</strong>:</p>
<ul>
<li><p>Measures overall performance.</p></li>
<li><p><strong>Higher AUC = Better model</strong>.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="what-is-the-difference-between-cost-function-and-gradient-descent">
<h2>8. What is the Difference Between Cost Function and Gradient Descent?<a class="headerlink" href="#what-is-the-difference-between-cost-function-and-gradient-descent" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Cost Function</strong>:</p>
<ul>
<li><p>Measures <strong>how well the model is performing</strong>.</p></li>
<li><p>Example: <strong>Mean Squared Error (MSE)</strong>.</p></li>
</ul>
</li>
<li><p><strong>Gradient Descent</strong>:</p>
<ul>
<li><p>Optimizes the cost function by iteratively adjusting model parameters.</p></li>
<li><p>Works by <strong>moving in the direction of the steepest descent</strong>.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="machine-learning-interview-guide-data-preprocessing-optimization">
<h1>Machine Learning Interview Guide: Data Preprocessing &amp; Optimization<a class="headerlink" href="#machine-learning-interview-guide-data-preprocessing-optimization" title="Link to this heading">#</a></h1>
<section id="what-is-data-preprocessing-what-steps-are-involved">
<h2>1Ô∏è‚É£ What is Data Preprocessing? What Steps are Involved?<a class="headerlink" href="#what-is-data-preprocessing-what-steps-are-involved" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Data Preprocessing</strong> is the process of cleaning and converting raw data into a usable format for machine learning models.</p></li>
</ul>
<section id="key-preprocessing-steps">
<h3>Key Preprocessing Steps:<a class="headerlink" href="#key-preprocessing-steps" title="Link to this heading">#</a></h3>
<p>‚úÖ Rescaling attributes with different scales<br />
‚úÖ Standardizing the dataset<br />
‚úÖ Encoding categorical attributes into integer values<br />
‚úÖ Handling missing data<br />
‚úÖ Removing duplicate data points<br />
‚úÖ Removing outliers or handling noisy data<br />
‚úÖ Discretizing the data<br />
‚úÖ Splitting the dataset into <strong>training and test sets</strong></p>
</section>
</section>
<hr class="docutils" />
<section id="what-is-feature-engineering">
<h2>2Ô∏è‚É£ What is Feature Engineering?<a class="headerlink" href="#what-is-feature-engineering" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Feature Engineering</strong> is the process of creating new features from existing ones to improve model performance.</p></li>
</ul>
<section id="common-feature-engineering-techniques">
<h3>Common Feature Engineering Techniques:<a class="headerlink" href="#common-feature-engineering-techniques" title="Link to this heading">#</a></h3>
<p>üîπ <strong>Filling missing values</strong> within a variable<br />
üîπ <strong>Encoding categorical variables</strong> into numbers<br />
üîπ <strong>Variable transformation</strong> (e.g., log transformation for skewed data)</p>
</section>
</section>
<hr class="docutils" />
<section id="what-are-some-recommended-choices-for-imputation-values">
<h2>3Ô∏è‚É£ What are Some Recommended Choices for Imputation Values?<a class="headerlink" href="#what-are-some-recommended-choices-for-imputation-values" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>For Numeric Features</strong>:</p>
<ul>
<li><p><strong>Normally distributed</strong> ‚Üí Use <strong>mean</strong></p></li>
<li><p><strong>Skewed or outliers present</strong> ‚Üí Use <strong>median</strong></p></li>
</ul>
</li>
<li><p><strong>For Categorical Features</strong>:</p>
<ul>
<li><p><strong>Sortable categories</strong> ‚Üí Use <strong>median</strong></p></li>
<li><p><strong>Non-sortable categories</strong> ‚Üí Use <strong>mode</strong></p></li>
</ul>
</li>
<li><p><strong>For Boolean Features</strong>:</p>
<ul>
<li><p>Use the <strong>most frequent value (True/False)</strong></p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="is-it-a-good-idea-to-clean-data-automatically">
<h2>4Ô∏è‚É£ Is it a Good Idea to Clean Data Automatically?<a class="headerlink" href="#is-it-a-good-idea-to-clean-data-automatically" title="Link to this heading">#</a></h2>
<p>üö® <strong>No, automatically cleaning data (e.g., removing extreme observations) is risky!</strong></p>
<ul class="simple">
<li><p>Data should <strong>not be removed unless there is a strong reason</strong> (e.g., data entry errors).</p></li>
<li><p><strong>Forcing data to ‚Äúlook‚Äù normal</strong> may introduce bias and distort real patterns.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="how-do-you-check-the-quality-of-your-dataset">
<h2>5Ô∏è‚É£ How Do You Check the Quality of Your Dataset?<a class="headerlink" href="#how-do-you-check-the-quality-of-your-dataset" title="Link to this heading">#</a></h2>
<p>üîç <strong>Steps to Ensure Data Quality:</strong></p>
<ul class="simple">
<li><p><strong>Fix obvious errors</strong> (e.g., negative values in age).</p></li>
<li><p><strong>Check sample distribution</strong> for balance.</p></li>
<li><p><strong>Search for outliers</strong> and decide whether to remove or keep them.</p></li>
<li><p><strong>Visualize data</strong> using plots (histograms, scatter plots, box plots).</p></li>
<li><p><strong>Calculate summary statistics</strong>: mean, standard deviation, min/max, etc.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="what-is-anova">
<h2>6Ô∏è‚É£ What is ANOVA?<a class="headerlink" href="#what-is-anova" title="Link to this heading">#</a></h2>
<p>üìä <strong>ANOVA (Analysis of Variance)</strong> is a statistical test used to compare the means of <strong>two or more groups</strong> to determine if there is a <strong>significant difference</strong> between them.</p>
<p>Example Use Case: Comparing average sales across multiple stores.</p>
</section>
<hr class="docutils" />
<section id="what-is-ensemble-learning">
<h2>7Ô∏è‚É£ What is Ensemble Learning?<a class="headerlink" href="#what-is-ensemble-learning" title="Link to this heading">#</a></h2>
<p>ü§ñ <strong>Ensemble Learning</strong> is a technique that <strong>combines multiple models</strong> to improve prediction accuracy.</p>
<p>üîπ <strong>Example</strong>: <strong>Random Forest</strong> is an ensemble of Decision Trees.</p>
</section>
<hr class="docutils" />
<section id="what-are-the-differences-between-bagging-and-boosting">
<h2>8Ô∏è‚É£ What are the Differences Between Bagging and Boosting?<a class="headerlink" href="#what-are-the-differences-between-bagging-and-boosting" title="Link to this heading">#</a></h2>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Feature</p></th>
<th class="head"><p>Bagging üèÜ</p></th>
<th class="head"><p>Boosting üöÄ</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Purpose</strong></p></td>
<td><p>Reduces <strong>variance</strong></p></td>
<td><p>Reduces <strong>bias</strong></p></td>
</tr>
<tr class="row-odd"><td><p><strong>Base Models</strong></p></td>
<td><p>Uses high-variance models (Decision Trees)</p></td>
<td><p>Uses low-variance, high-bias models</p></td>
</tr>
<tr class="row-even"><td><p><strong>Parallelization</strong></p></td>
<td><p><strong>Yes</strong> (models train independently)</p></td>
<td><p><strong>No</strong> (models train sequentially)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Computational Cost</strong></p></td>
<td><p>Lower</p></td>
<td><p>Higher</p></td>
</tr>
</tbody>
</table>
</div>
<p>üõ† <strong>Bagging Example</strong>: <strong>Random Forest</strong><br />
üîù <strong>Boosting Example</strong>: <strong>AdaBoost, XGBoost</strong></p>
</section>
<hr class="docutils" />
<section id="what-is-the-difference-between-cost-function-vs-gradient-descent">
<h2>9Ô∏è‚É£ What is the Difference Between Cost Function vs Gradient Descent?<a class="headerlink" href="#what-is-the-difference-between-cost-function-vs-gradient-descent" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Cost Function</strong>: Measures how well the model is performing by calculating the error (e.g., <strong>Mean Squared Error (MSE)</strong>).</p></li>
<li><p><strong>Gradient Descent</strong>: An <strong>optimization algorithm</strong> used to minimize the cost function by iteratively adjusting model parameters.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="what-is-the-idea-behind-gradient-descent">
<h2>üîü What is the Idea Behind Gradient Descent?<a class="headerlink" href="#what-is-the-idea-behind-gradient-descent" title="Link to this heading">#</a></h2>
<p>üîΩ <strong>Gradient Descent</strong> helps find the optimal model parameters by iteratively moving <strong>towards the minimum cost</strong>.</p>
<section id="key-considerations">
<h3>Key Considerations:<a class="headerlink" href="#key-considerations" title="Link to this heading">#</a></h3>
<p>‚úÖ <strong>Small learning rate</strong> ‚Üí Slower convergence, but more precise.<br />
‚ùå <strong>Large learning rate</strong> ‚Üí May overshoot the minimum and fail to converge.</p>
<p>üìâ <strong>Goal</strong>: Adjust parameters step by step to reach the global minimum of the cost function.</p>
</section>
</section>
</section>
<hr class="docutils" />
<section id="understanding-gradient-descent-in-linear-regression">
<h1>üìâ Understanding Gradient Descent in Linear Regression<a class="headerlink" href="#understanding-gradient-descent-in-linear-regression" title="Link to this heading">#</a></h1>
<section id="how-does-gradient-descent-work">
<h2>üîπ How Does Gradient Descent Work?<a class="headerlink" href="#how-does-gradient-descent-work" title="Link to this heading">#</a></h2>
<p>Gradient Descent is an <strong>optimization algorithm</strong> used to minimize the <strong>error (cost function)</strong> in <strong>Linear Regression</strong>. It adjusts the model parameters (coefficients) iteratively to find the best fit line.</p>
<section id="step-by-step-process">
<h3>üîÑ <strong>Step-by-Step Process:</strong><a class="headerlink" href="#step-by-step-process" title="Link to this heading">#</a></h3>
<p>1Ô∏è‚É£ <strong>Initialize coefficients randomly</strong> (e.g., slope <code class="docutils literal notranslate"><span class="pre">m</span></code> and intercept <code class="docutils literal notranslate"><span class="pre">b</span></code>).<br />
2Ô∏è‚É£ <strong>Compute the loss function</strong> (Sum of Squared Errors - SSE).<br />
3Ô∏è‚É£ <strong>Calculate the gradients</strong> (derivatives) to find the direction of steepest descent.<br />
4Ô∏è‚É£ <strong>Update coefficients</strong> using the learning rate to move in the direction that minimizes the error:</p>
<p>[
m = m - \alpha \cdot \frac{dJ}{dm}
]</p>
<p>[
b = b - \alpha \cdot \frac{dJ}{db}
]</p>
<p>where:</p>
<ul class="simple">
<li><p>( \alpha ) is the <strong>learning rate</strong></p></li>
<li><p>( J ) is the <strong>cost function</strong></p></li>
</ul>
<p>5Ô∏è‚É£ <strong>Repeat</strong> until convergence (i.e., the error stops decreasing).</p>
</section>
</section>
<hr class="docutils" />
<section id="visualizing-gradient-descent">
<h2>üìä <strong>Visualizing Gradient Descent</strong><a class="headerlink" href="#visualizing-gradient-descent" title="Link to this heading">#</a></h2>
<p>Imagine a bowl-shaped curve where we start at a random point on the slope.</p>
<ul class="simple">
<li><p>If <strong>the learning rate is too large</strong>, we might jump over the minimum and never converge.</p></li>
<li><p>If <strong>the learning rate is too small</strong>, the algorithm will take too long to reach the minimum.</p></li>
</ul>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./contents\6_interview"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="pd.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">üìö Pandas: Interview Guide</p>
      </div>
    </a>
    <a class="right-next"
       href="git.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Git Essentials</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Machine Learning: Interview Guide</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-an-algorithm">1. What is an Algorithm?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-machine-learning-algorithm">2. What is a Machine Learning Algorithm?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#difference-between-machine-learning-algorithm-and-normal-algorithm">3. Difference Between Machine Learning Algorithm and Normal Algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-machine-learning">4. Why Machine Learning?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-workflow">5. Machine Learning Workflow</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#steps-in-a-machine-learning-project">Steps in a Machine Learning Project:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-machine-learning-approaches">6. Types of Machine Learning Approaches</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-supervised-learning">7. What is Supervised Learning?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-supervised-learning">Types of Supervised Learning:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-regression">8. What is Regression?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examples">Examples:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-regression-algorithms">Common Regression Algorithms:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-classification">9. What is Classification?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Examples:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-classification-algorithms">Common Classification Algorithms:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-guide">Machine Learning Guide</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-algorithms-in-machine-learning">11. Classification Algorithms in Machine Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Common Classification Algorithms:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-unsupervised-learning">12. What is Unsupervised Learning?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-characteristics">Key Characteristics:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-unsupervised-learning">Types of Unsupervised Learning:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-clustering">13. What is Clustering?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Examples:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-clustering-algorithms">14. Common Clustering Algorithms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-dimensionality-reduction">15. What is Dimensionality Reduction?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-it-important">Why is it Important?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-dimensionality-reduction-algorithms">16. Common Dimensionality Reduction Algorithms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-reinforcement-learning">17. What is Reinforcement Learning?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts">Key Concepts:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#difference-between-regression-and-classification">18. Difference Between Regression and Classification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#difference-between-online-and-offline-batch-learning">19. Difference Between Online and Offline (Batch) Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#online-learning">Online Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#offline-batch-learning">Offline (Batch) Learning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-train-a-model-effectively">20. How to Train a Model Effectively?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">üéØ Summary</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-fundamentals-a-guide-for-junior-to-mid-level-developers">Machine Learning Fundamentals: A Guide for Junior to Mid-Level Developers</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem">21. Bayes Theorem</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-bayes-theorem">What is Bayes Theorem?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#formula">Formula:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#difference-between-knn-and-k-means">22. Difference Between KNN and K-Means</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbors-knn"><strong>K-Nearest Neighbors (KNN)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-means-clustering"><strong>K-Means Clustering</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-model-training">23. What is Model Training?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-linear-regression-formula"><strong>Example: Linear Regression Formula</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-convergence">24. What is Convergence?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-allocate-data-for-training-validation-and-testing">25. How to Allocate Data for Training, Validation, and Testing?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-p-value-why-is-it-important">26. What is a p-value? Why is it Important?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-rule"><strong>Decision Rule</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-f1-score">27. What is F1 Score?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4"><strong>Formula:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-type-i-and-type-ii-errors">28. What are Type I and Type II Errors?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#type-i-error-false-positive"><strong>Type I Error (False Positive)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#type-ii-error-false-negative"><strong>Type II Error (False Negative)</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#are-you-familiar-with-r-programming">29. Are You Familiar with R Programming?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-hypothesis">30. What is a Hypothesis?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-takeaways">üìå <strong>Key Takeaways:</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-guide">Linear Regression Guide</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-linear-regression">1. What is Linear Regression?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Examples:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-linear-regression">2. Types of Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-linear-regression"><strong>1. Simple Linear Regression</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-linear-regression"><strong>2. Multiple Linear Regression</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6"><strong>Example:</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-r-squared">3. What is R-Squared?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-accuracy-in-regression">4. What is Accuracy in Regression?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-metrics">5. Regression Metrics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error-mse"><strong>1. Mean Squared Error (MSE)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#root-mean-squared-error-rmse"><strong>2. Root Mean Squared Error (RMSE)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-absolute-error-mae"><strong>3. Mean Absolute Error (MAE)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#python-code-for-regression-metrics"><strong>Python Code for Regression Metrics:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-overfitting">6. What is Overfitting?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-correlation">7. What is Correlation?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-learning-rate">8. What is Learning Rate?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-intercept">9. What is the Intercept?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumptions-of-linear-regression">10. Assumptions of Linear Regression</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-essential-concepts-techniques">Machine Learning: Essential Concepts &amp; Techniques</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization-vs-standardization">41. Normalization vs Standardization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization"><strong>Normalization</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#standardization"><strong>Standardization</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-use"><strong>When to Use?</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-selection-in-machine-learning">42. Model Selection in Machine Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-boundary">43. Decision Boundary</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-a-logistic-regression-model-is-trained">44. How a Logistic Regression Model is Trained</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-naive-bayes-called-naive">45. Why is Na√Øve Bayes Called ‚ÄúNa√Øve‚Äù?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-a-classifier-based-on-training-set-size">46. Choosing a Classifier Based on Training Set Size</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-of-naive-bayes-algorithm">47. Advantages of Na√Øve Bayes Algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-the-optimal-k-in-k-nn">48. Choosing the Optimal ( k ) in k-NN</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#is-k-nn-suitable-for-large-datasets">49. Is k-NN Suitable for Large Datasets?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbors-k-nn-algorithm">50. k-Nearest Neighbors (k-NN) Algorithm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-it-works"><strong>How it Works?</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#distance-metrics"><strong>Distance Metrics</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualization"><strong>Visualization:</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#k-means-vs-k-nearest-neighbors-k-nn">k-Means vs. k-Nearest Neighbors (k-NN)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-main-difference-between-k-means-and-k-nearest-neighbors">1. What is the main difference between k-Means and k-Nearest Neighbors?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-means-clustering-algorithm">k-Means (Clustering Algorithm)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-nearest-neighbors-k-nn">k-Nearest Neighbors (k-NN)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-you-select-the-value-of-k-for-k-nearest-neighbors">2. How do you select the value of K for k-Nearest Neighbors?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-disadvantages-of-k-nearest-neighbors">3. Advantages &amp; Disadvantages of k-Nearest Neighbors</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages">‚úÖ Advantages</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#disadvantages">‚ùå Disadvantages</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-of-k-means-clustering">4. Applications of k-Means Clustering</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#steps-of-k-means-clustering-algorithm">5. Steps of k-Means Clustering Algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-objective-function-of-k-means">6. What is the Objective Function of k-Means?</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-trees">Decision Trees</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-decision-trees">7. What are Decision Trees?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Example:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-of-decision-trees">8. Advantages of Decision Trees</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-pure-node">9. What is a Pure Node?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-handle-overfitting-in-decision-trees">10. How to Handle Overfitting in Decision Trees?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solution-pruning-reducing-complexity-of-the-tree">Solution: <strong>Pruning</strong> (Reducing complexity of the tree)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bottom-up-pruning">üîΩ Bottom-Up Pruning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#top-down-pruning">üîº Top-Down Pruning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reduced-error-pruning">üîß Reduced Error Pruning</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-concepts-structured-guide">üìå Machine Learning Concepts: Structured Guide</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#greedy-splitting">1Ô∏è‚É£ Greedy Splitting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy">2Ô∏è‚É£ Entropy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tree-bagging-bootstrap-aggregating">3Ô∏è‚É£ Tree Bagging (Bootstrap Aggregating)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tree-boosting">4Ô∏è‚É£ Tree Boosting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#handling-outliers-in-logistic-regression">5Ô∏è‚É£ Handling Outliers in Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#entropy-in-decision-trees">6Ô∏è‚É£ Entropy in Decision Trees</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forest">7Ô∏è‚É£ Random Forest</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#does-random-forest-require-pruning">8Ô∏è‚É£ Does Random Forest Require Pruning?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ensemble-methods">9Ô∏è‚É£ Ensemble Methods</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#random-forest-a-comprehensive-guide">Random Forest: A Comprehensive Guide</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameters-in-random-forest">1. Hyperparameters in Random Forest</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#is-cross-validation-necessary-in-random-forest">2. Is Cross-Validation Necessary in Random Forest?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#is-random-forest-an-ensemble-algorithm">3. Is Random Forest an Ensemble Algorithm?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#handling-missing-values-in-random-forest">4. Handling Missing Values in Random Forest</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variable-selection-in-random-forest">5. Variable Selection in Random Forest</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-random-forest-considered-non-interpretable">6. Why is Random Forest Considered Non-Interpretable?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#advantages-of-random-forest">7. Advantages of Random Forest</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#drawbacks-of-random-forest">8. Drawbacks of Random Forest</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-bagging-bootstrap-aggregating">9. Understanding BAGGing (Bootstrap Aggregating)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#difference-between-oob-score-and-validation-score">10. Difference Between OOB Score and Validation Score</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-concepts-random-forest-support-vector-machines-svm">Machine Learning Concepts: Random Forest &amp; Support Vector Machines (SVM)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-proximities-in-random-forests">31. What are Proximities in Random Forests?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-define-the-criteria-to-split-at-each-node-of-the-trees">32. How to Define the Criteria to Split at Each Node of the Trees?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-adaboost-algorithm">33. What is the AdaBoost Algorithm?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-logistic-regression-handle-outliers">34. How Does Logistic Regression Handle Outliers?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-support-vector-machine-svm">35. What is a Support Vector Machine (SVM)?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-hyperplane-in-svm">36. What is a Hyperplane in SVM?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-support-vectors-in-svm">37. What are Support Vectors in SVM?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-svm-kernels">38. Types of SVM Kernels</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-the-kernel-trick">39. Why Use the Kernel Trick?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-of-svms">40. Applications of SVMs</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#face-detection"><strong>1. Face Detection</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#text-hypertext-categorization"><strong>2. Text &amp; Hypertext Categorization</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#image-classification"><strong>3. Image Classification</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bioinformatics"><strong>4. Bioinformatics</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#protein-fold-homology-detection"><strong>5. Protein Fold &amp; Homology Detection</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#handwriting-recognition"><strong>6. Handwriting Recognition</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generalized-predictive-control-gpc"><strong>7. Generalized Predictive Control (GPC)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion"><strong>Conclusion</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#support-vector-machine-svm-and-anomaly-detection-guide">Support Vector Machine (SVM) and Anomaly Detection Guide</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#role-of-c-hyperparameter-in-svm">41. Role of C Hyperparameter in SVM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-support-vectors">42. What are Support Vectors?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-anomaly-detection">43. What is Anomaly Detection?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-care-about-anomalies">44. Why Do We Care About Anomalies?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">45. Normalization vs. Standardization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-68-95-99-7-rule-for-normal-distribution">46. The 68-95-99.7 Rule for Normal Distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-is-iqr-interquartile-range-used-in-time-series-forecasting">47. How is IQR (Interquartile Range) Used in Time Series Forecasting?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-standard-deviation-for-anomaly-detection">48. Using Standard Deviation for Anomaly Detection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#can-you-find-outliers-using-k-means">49. Can You Find Outliers Using k-Means?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-handle-outliers-in-a-dataset">50. How to Handle Outliers in a Dataset?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approaches">Approaches:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-interview-questions-answers">Machine Learning Interview Questions &amp; Answers</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-bias-in-machine-learning">1. What is Bias in Machine Learning?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#characteristics-of-bias">Characteristics of Bias:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-bias-variance-tradeoff">2. What is the Bias-Variance Tradeoff?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solution">Solution:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-identify-and-fix-a-high-bias-model">3. How to Identify and Fix a High-Bias Model?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#identifying-high-bias">Identifying High Bias:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fixing-high-bias">Fixing High Bias:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-types-of-classification-algorithms-exist">4. What Types of Classification Algorithms Exist?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-the-adaboost-algorithm-work">5. How Does the AdaBoost Algorithm Work?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-confusion-matrix">6. What is a Confusion Matrix?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-roc-curve-auc-measure-model-performance">7. How Do ROC Curve &amp; AUC Measure Model Performance?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-difference-between-cost-function-and-gradient-descent">8. What is the Difference Between Cost Function and Gradient Descent?</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-interview-guide-data-preprocessing-optimization">Machine Learning Interview Guide: Data Preprocessing &amp; Optimization</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-data-preprocessing-what-steps-are-involved">1Ô∏è‚É£ What is Data Preprocessing? What Steps are Involved?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-preprocessing-steps">Key Preprocessing Steps:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-feature-engineering">2Ô∏è‚É£ What is Feature Engineering?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-feature-engineering-techniques">Common Feature Engineering Techniques:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-some-recommended-choices-for-imputation-values">3Ô∏è‚É£ What are Some Recommended Choices for Imputation Values?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#is-it-a-good-idea-to-clean-data-automatically">4Ô∏è‚É£ Is it a Good Idea to Clean Data Automatically?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-you-check-the-quality-of-your-dataset">5Ô∏è‚É£ How Do You Check the Quality of Your Dataset?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-anova">6Ô∏è‚É£ What is ANOVA?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-ensemble-learning">7Ô∏è‚É£ What is Ensemble Learning?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-the-differences-between-bagging-and-boosting">8Ô∏è‚É£ What are the Differences Between Bagging and Boosting?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-difference-between-cost-function-vs-gradient-descent">9Ô∏è‚É£ What is the Difference Between Cost Function vs Gradient Descent?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-the-idea-behind-gradient-descent">üîü What is the Idea Behind Gradient Descent?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-considerations">Key Considerations:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-gradient-descent-in-linear-regression">üìâ Understanding Gradient Descent in Linear Regression</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-gradient-descent-work">üîπ How Does Gradient Descent Work?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-by-step-process">üîÑ <strong>Step-by-Step Process:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-gradient-descent">üìä <strong>Visualizing Gradient Descent</strong></a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gajanesh
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright ¬© 2025 Gajanesh. All rights reserved..
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>