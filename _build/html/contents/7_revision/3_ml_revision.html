
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>1. Introduction to Machine Learning &#8212; Data Science Books</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-dropdown.css?v=995e94df" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-bootstrap.min.css?v=21c0b90a" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=d567e03f" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'contents/7_revision/3_ml_revision';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Data Science Books</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Part I ‚Äî Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../0_maths/0_essential.html">Essential Mathematics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_maths/4_linear_algebra.html">Linear Algebra</a></li>







<li class="toctree-l1"><a class="reference internal" href="../0_maths/2_probability.html">Probability Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_maths/1_descriptive.html">Descriptive Statistics</a></li>








<li class="toctree-l1"><a class="reference internal" href="../0_maths/3_inferential.html">Inferential Statistics</a></li>



<li class="toctree-l1"><a class="reference internal" href="../0_maths/5_calculus.html">Calculus</a></li>







<li class="toctree-l1"><a class="reference internal" href="../0_maths/6_regression_analysis.html">Explanatory and Response Variables</a></li>


<li class="toctree-l1"><a class="reference internal" href="../1_python/1_basics.html">Python Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/2_advanced.html">Advanced Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/3_data_structures.html">Data Structures</a></li>





<li class="toctree-l1"><a class="reference internal" href="../1_python/4_modules_packages.html">Modules &amp; Packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/5_functions.html">Functions &amp; Modular Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/6_oop.html">Object-Oriented Programming</a></li>





<li class="toctree-l1"><a class="reference internal" href="../1_python/8_exceptions.html">Exception Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/9_regex.html">Regular Expressions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_numpy/1_numpy.html">NumPy (<strong>Numerical Python</strong>)</a></li>



<li class="toctree-l1"><a class="reference internal" href="../2_pandas/1_series.html">Pandas Series</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_pandas/2_dataframes.html">Pandas DataFrame</a></li>
















<li class="toctree-l1"><a class="reference internal" href="../2_pandas/3_visualization.html"><strong>What is Data Visualization in Data Science?</strong></a></li>


<li class="toctree-l1"><a class="reference internal" href="../2_pandas/4_eda.html">Exploratory Data Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_pandas/5_feature_engineering.html"><strong>What is Feature Engineering?</strong></a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part II ‚Äî Classical Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../3_ml/1_foundations.html">ML Foundational</a></li>





<li class="toctree-l1"><a class="reference internal" href="../3_ml/2_data_preparation.html">2Ô∏è‚É£ Data Handling</a></li>





<li class="toctree-l1"><a class="reference internal" href="../3_ml/2_train_test_split.html">Train‚ÄìTest Split</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/5_model_evaluation.html">4Ô∏è‚É£ Model Evaluation</a></li>




<li class="toctree-l1"><a class="reference internal" href="../3_ml/11_supervised_learning.html"><strong>Supervised Learning</strong></a></li>



<li class="toctree-l1"><a class="reference internal" href="../3_ml/12_regression.html">Regression Algorithms</a></li>







<li class="toctree-l1"><a class="reference internal" href="../3_ml/13_classification.html">Classification Algorithms</a></li>

<li class="toctree-l1"><a class="reference internal" href="../3_ml/10_core_algo.html">Core ML Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/14_decision_trees.html">Decision Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/15_ensemble_methods.html">Ensemble Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/16_svm.html">Support Vector Machine (SVM) in Detail</a></li>


<li class="toctree-l1"><a class="reference internal" href="../3_ml/17_knn.html">k-Nearest Neighbors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/18_naive_bayes.html">Naive Bayes</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part III ‚Äî Advanced Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../3_ml/20_unsupervised_learning.html">Unsupervised Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../3_ml/21_clustering.html">Clustering Techniques</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/7_optimization_and_training.html">5Ô∏è‚É£ Optimization &amp; Training</a></li>







<li class="toctree-l1 has-children"><a class="reference internal" href="../3_ml/6_ml_lifecycle.html">ML Lifecycle</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../3_ml/6_training.html">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3_ml/6_evaluation.html">Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3_ml/6_deployment.html">Deployment</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part IV ‚Äî Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl1_Introduction.html">Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl2_Neuron.html">Neuron</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl3_Libraries.html">Deep Learning Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl4_Terminology.html">Terminology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl5_multi_layer.html">Multi-Layer Perceptron</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl6_first_nn.html">First Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl7_evaluating_model.html">Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl8_multiclass_classification.html">Multiclass Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl9_multiclass_classification_hand.html">Handwritten Digit Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl10_saving_and_loading.html">Saving &amp; Loading Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl11_checkpointing.html">Model Checkpointing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl12_visualizing_model_training.html"><strong>Visualizing Model Training History in Deep Learning</strong></a></li>

<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl13_loss_functions_activation_functions_and_optimizers.html">Loss Functions &amp; Optimizers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part V ‚Äî NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp1.html">NLP Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp2.html">Text Cleaning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp3.html">Text Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp4.html">NLP Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp5.html">Bag of Words, TF-IDF</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp6.html">Sentiment Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp7.html">NLP with SpaCy</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part VI ‚Äî Career &amp; MLOps</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../6_interview/self%20introduction.html">Self Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6_interview/py.html">Python: Interview Guide</a></li>





<li class="toctree-l1"><a class="reference internal" href="../6_interview/pd.html">üìö Pandas: Interview Guide</a></li>


<li class="toctree-l1"><a class="reference internal" href="../6_interview/ml.html">Machine Learning: Interview Guide</a></li>













<li class="toctree-l1"><a class="reference internal" href="../6_interview/git.html">Git</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6_interview/dvc.html">DVC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6_interview/mlflow.html">MLflow</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/coding-fame/book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/coding-fame/book/edit/main/contents/7_revision/3_ml_revision.md" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/coding-fame/book/issues/new?title=Issue%20on%20page%20%2Fcontents/7_revision/3_ml_revision.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/contents/7_revision/3_ml_revision.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>1. Introduction to Machine Learning</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">1. Introduction to Machine Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-machine-learning">What is Machine Learning?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-program">What is a Program?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-an-algorithm">What is an Algorithm?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-machine-learning-algorithm">What is a Machine Learning Algorithm?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#differences-between-machine-learning-and-normal-algorithms">Differences Between Machine Learning and Normal Algorithms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-machine-learning">Why Machine Learning?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-use-cases">Key Use Cases:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#definitions-of-machine-learning">Definitions of Machine Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#arthur-samuel-s-definition-1959">Arthur Samuel‚Äôs Definition (1959)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#artificial-intelligence-vs-machine-learning-vs-deep-learning">Artificial Intelligence vs Machine Learning vs Deep Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning">Machine Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning">Deep Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#artificial-intelligence-ai">Artificial Intelligence (AI)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#human-vs-computer-decision-making">Human vs Computer Decision-Making</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-machines-think">How Do Machines ‚ÄúThink‚Äù?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#real-time-examples-of-machine-learning">Real-Time Examples of Machine Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gmail-spam-filter">Gmail Spam Filter</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#banking-loan-application-credit-card-approval">Banking Loan Application / Credit Card Approval</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-terminology">2. Machine Learning Terminology</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Machine Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model">Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-a-model">Training a Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-the-model">Testing the Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-deployment">Model Deployment</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normal-vs-smart-applications">Normal vs Smart Applications</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normal-application">Normal Application</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#smart-application">Smart Application</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-use-case">Example Use Case:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy-testing">Accuracy Testing</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#data-machine-learning-algorithm-terminology">3. Data &amp; Machine Learning Algorithm Terminology</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data">Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-in-tables">Data in Tables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-components-of-a-table">Key Components of a Table</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-learning-perspective">Statistical Learning Perspective</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#input-and-output-variables">Input and Output Variables</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#terminology-in-statistics">Terminology in Statistics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computer-science-perspective">Computer Science Perspective</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#models-and-algorithms">Models and Algorithms</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-learning-a-function">4. Machine Learning: Learning a Function</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-a-function">Learning a Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#purpose-of-the-learning-function">Purpose of the Learning Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-algorithms">Machine Learning Algorithms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-machine-learning-models">5. Types of Machine Learning Models</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#features-and-labels">Features and Labels</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#features">Features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#label">Label</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#label-example">Label Example</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#labelled-and-unlabelled-data">Labelled and Unlabelled Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#labelled-data">Labelled Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unlabelled-data">Unlabelled Data</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supervised-vs-unsupervised-learning">Supervised vs Unsupervised Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#supervised-learning">Supervised Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unsupervised-learning">Unsupervised Learning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-supervised-learning-models">Types of Supervised Learning Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-models">1. Regression Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-models">2. Classification Models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-unsupervised-learning-models">Types of Unsupervised Learning Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering">1. Clustering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensionality-reduction">2. Dimensionality Reduction</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-life-cycle">6. Machine Learning Life Cycle</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-collection">1. Data Collection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preparation">2. Data Preparation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-wrangling">3. Data Wrangling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-the-model">4. Train the Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#test-the-model">5. Test the Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">6. Model Deployment</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Summary</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#train-test-datasets-in-machine-learning">7. Train &amp; Test Datasets in Machine Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-datasets">Types of Datasets</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-dataset">1. Train Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#test-dataset">2. Test Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#validation-dataset-optional">3. Validation Dataset (Optional)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deciding-the-size-of-datasets">Deciding the Size of Datasets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-train-test-split-p-function">Using <code class="docutils literal notranslate"><span class="pre">train_test_split(p)</span></code> Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-train-test-split-p-random-state-0-function">Using <code class="docutils literal notranslate"><span class="pre">train_test_split(p,</span> <span class="pre">random_state=0)</span></code> Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Summary</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#r-value-and-regression-analysis">8. R-Value and Regression Analysis</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-analysis">Regression Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-a-line-in-regression">Understanding a Line in Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#goal-of-linear-regression">Goal of Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#can-we-use-regression-everywhere">Can We Use Regression Everywhere?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#r-value-understanding-the-strength-of-relationship">R-Value: Understanding the Strength of Relationship</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#r-value-range">R-Value Range</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-the-r-value">Calculating the R-Value</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-r-value-results">Interpreting R-Value Results</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Summary</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-linear-regression">Simple Linear Regression</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-linear-regression">What is Linear Regression?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-linear-regression">Types of Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">1. Simple Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-linear-regression">2. Multiple Linear Regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-formula">Linear Regression Formula</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-simple-linear-regression-in-python">Example: Simple Linear Regression in Python</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-calculation">Example Calculation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-best-fitted-line">Visualizing the Best Fitted Line</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Summary</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-example">Linear Regression Example</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scenario">Scenario</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-mathematics-behind-it">Understanding the Mathematics Behind It</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-linear-regression-works">How Linear Regression Works:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-linear-regression-in-python">Implementing Linear Regression in Python</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#important-information">Important Information</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#making-predictions">Making Predictions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-y-pred">What is <code class="docutils literal notranslate"><span class="pre">y_pred</span></code>?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-actual-vs-predicted-values">Comparing Actual vs. Predicted Values</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-the-model">Evaluating the Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-evaluation-metrics-for-regression">Common Evaluation Metrics for Regression:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Summary</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-example-salary-prediction">9 Linear Regression Example - Salary Prediction</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-importing-required-libraries">Step 1: Importing Required Libraries</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-loading-the-dataset">Step 2: Loading the Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-splitting-the-dataset">Step 3: Splitting the Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-training-the-model">Step 4: Training the Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-making-predictions">Step 5: Making Predictions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-6-visualizing-the-results">Step 6: Visualizing the Results</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-dataset-visualization"><strong>Training Dataset Visualization</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#test-dataset-visualization"><strong>Test Dataset Visualization</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Summary</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">10. Multiple Linear Regression</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-statement">Problem Statement</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#given-the-following-home-details-we-need-to-predict-their-prices">Given the following home details, we need to predict their prices:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#formula-for-multiple-linear-regression">Formula for Multiple Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-by-step-implementation">Step-by-Step Implementation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-import-required-libraries"><strong>Step 1: Import Required Libraries</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-load-the-dataset"><strong>Step 2: Load the Dataset</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-handling-missing-values"><strong>Step 3: Handling Missing Values</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-preparing-data-for-model-training"><strong>Step 4: Preparing Data for Model Training</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-train-the-model"><strong>Step 5: Train the Model</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-6-retrieve-model-coefficients"><strong>Step 6: Retrieve Model Coefficients</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Summary</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#polynomial-features-in-machine-learning">11. Polynomial Features in Machine Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">Introduction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-polynomial-features"><strong>What Are Polynomial Features?</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-polynomial-features"><strong>Why Do We Need Polynomial Features?</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-models-vs-non-linear-data"><strong>Linear Models vs. Non-Linear Data</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-equations"><strong>Mathematical Equations</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13"><strong>Simple Linear Regression</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14"><strong>Multiple Linear Regression</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id15"><strong>Step-by-Step Implementation</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16"><strong>Step 1: Import Required Libraries</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17"><strong>Step 2: Load the Dataset</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-data-preparation"><strong>Step 3: Data Preparation</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-visualizing-the-data"><strong>Step 4: Visualizing the Data</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-training-a-linear-regression-model"><strong>Step 5: Training a Linear Regression Model</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-6-applying-polynomial-features"><strong>Step 6: Applying Polynomial Features</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-7-plotting-polynomial-regression"><strong>Step 7: Plotting Polynomial Regression</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predictions"><strong>Predictions</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id18"><strong>Conclusion</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-functions-in-machine-learning">12. Cost Functions in Machine Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id19"><strong>Introduction</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-cost-functions"><strong>Why Do We Need Cost Functions?</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-a-cost-function-work"><strong>How Does a Cost Function Work?</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#goal-of-training"><strong>Goal of Training</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-functions-for-regression">13. <strong>Cost Functions for Regression</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id20"><strong>Introduction</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-regression-metrics"><strong>Types of Regression Metrics</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distance-based-error"><strong>Distance-Based Error</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error-mse"><strong>1. Mean Squared Error (MSE)</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation"><strong>Implementation:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#root-mean-squared-error-rmse"><strong>2. Root Mean Squared Error (RMSE)</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#formula"><strong>Formula:</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id21"><strong>Implementation:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-absolute-error-mae"><strong>3. Mean Absolute Error (MAE)</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id22"><strong>Implementation:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-the-right-regression-metric"><strong>Choosing the Right Regression Metric</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#handling-categorical-data-dummy-variables-one-hot-encoding">14. <strong>Handling Categorical Data: Dummy Variables &amp; One-Hot Encoding</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id23"><strong>Introduction</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-can-t-we-directly-convert-text-to-numbers"><strong>Why Can‚Äôt We Directly Convert Text to Numbers?</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-dummy-variables"><strong>Using Dummy Variables</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id24"><strong>Implementation:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id25"><strong>Making Predictions</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-house-price-in-vijayawada"><strong>Predicting House Price in Vijayawada</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-house-price-in-guntur"><strong>Predicting House Price in Guntur</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-house-price-in-gudiwada"><strong>Predicting House Price in Gudiwada</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id26"><strong>Conclusion</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-in-machine-learning">15. <strong>Gradient Descent in Machine Learning</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id27"><strong>Introduction</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-gradient-descent-works"><strong>How Gradient Descent Works</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-convergence"><strong>Understanding Convergence</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#process-behind-gradient-descent"><strong>Process Behind Gradient Descent</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-gradient-descent-process"><strong>Visualizing the Gradient Descent Process</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#steps-in-gradient-descent"><strong>Steps in Gradient Descent</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-gradient-descent-approaches"><strong>Types of Gradient Descent Approaches</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fixed-step-size-approach"><strong>Fixed Step Size Approach</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-reaching-minimum-error"><strong>Learning Rate: Reaching Minimum Error</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#effect-of-small-large-learning-rates"><strong>Effect of Small &amp; Large Learning Rates</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id28"><strong>Conclusion</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-in-machine-learning">16. <strong>Logistic Regression in Machine Learning</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id29"><strong>Introduction</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-logistic-regression"><strong>Types of Logistic Regression</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-classification"><strong>1. Binary Classification</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multiclass-classification"><strong>2. Multiclass Classification</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-problem"><strong>Understanding the Problem</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-overview"><strong>Dataset Overview</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id30"><strong>Problem Statement</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-function-sigmoid"><strong>Logistic Function (Sigmoid)</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-in-python"><strong>Implementation in Python</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#load-the-dataset"><strong>1. Load the Dataset</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#splitting-the-data"><strong>2. Splitting the Data</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-the-model"><strong>3. Training the Model</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id31"><strong>4. Making Predictions</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-performance"><strong>5. Model Performance</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-insights"><strong>Prediction Insights</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id32"><strong>Conclusion</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-multiclass-classification"><strong>Logistic Regression - Multiclass Classification</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id33"><strong>Introduction</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-example-handwritten-digit-recognition"><strong>Practical Example: Handwritten Digit Recognition</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id34"><strong>Problem Statement</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id35"><strong>Implementation in Python</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id36"><strong>1. Load the Dataset</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-digits"><strong>2. Visualizing the Digits</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#check-target-labels"><strong>3. Check Target Labels</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id37"><strong>Training the Model</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#splitting-the-dataset"><strong>1. Splitting the Dataset</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-the-logistic-regression-model"><strong>2. Training the Logistic Regression Model</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-evaluation-predictions"><strong>Model Evaluation &amp; Predictions</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checking-accuracy"><strong>1. Checking Accuracy</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id38"><strong>2. Making Predictions</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id39"><strong>Conclusion</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-decision-tree">18. Machine Learning: Decision Tree</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-decision-trees">1. Introduction to Decision Trees</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-decision-tree">2. What is a Decision Tree?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#components-of-a-decision-tree">Components of a Decision Tree:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cart-algorithm">3. CART Algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-decision-trees">4. Why Use Decision Trees?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-decision-tree-terminologies">5. Key Decision Tree Terminologies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-the-decision-tree-algorithm-work">6. How Does the Decision Tree Algorithm Work?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-a-decision-tree">7. Example of a Decision Tree</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-implementation-decision-tree-in-python">8. Practical Implementation: Decision Tree in Python</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id40">9. Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-confusion-matrix">19. Machine Learning: Confusion Matrix</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-confusion-matrix">1. Introduction to Confusion Matrix</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-confusion-matrix">2. What is a Confusion Matrix?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-terms">3. Understanding the Terms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#type-i-and-type-ii-errors">4. Type I and Type II Errors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#representing-the-confusion-matrix">5. Representing the Confusion Matrix</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-metrics">6. Performance Metrics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy">Accuracy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recall-sensitivity-true-positive-rate">Recall (Sensitivity / True Positive Rate)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#specificity-true-negative-rate">Specificity (True Negative Rate)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#precision">Precision</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#f1-score">F1 Score</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id41">7. Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-bias-variance-trade-off">20. Machine Learning: Bias-Variance Trade-Off</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id42">1. Introduction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-definitions">Key Definitions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-goal-of-supervised-learning">2. The Goal of Supervised Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-prediction-error">3. Understanding Prediction Error</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-in-machine-learning">4. Bias in Machine Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-low-and-high-bias-models">Examples of Low and High Bias Models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variance-in-machine-learning">5. Variance in Machine Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-low-and-high-variance-models">Examples of Low and High Variance Models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-variance-trade-off">6. Bias-Variance Trade-Off</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#configuring-the-bias-variance-trade-off">7. Configuring the Bias-Variance Trade-Off</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1-k-nearest-neighbors-k-nn">Example 1: k-Nearest Neighbors (K-NN)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-2-support-vector-machines-svm">Example 2: Support Vector Machines (SVM)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-relationship-between-bias-and-variance">8. The Relationship Between Bias and Variance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-error">9. Types of Error</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scenarios-of-bias-and-variance">Scenarios of Bias and Variance:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-bias-variance-decomposition-with-python">10. Example: Bias-Variance Decomposition with Python</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-example">Code Example:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id43">11. Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-random-forest-algorithm">21. Machine Learning: Random Forest Algorithm</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-random-forest">1. Introduction to Random Forest</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-points">Key Points:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-ensemble-learning">2. What is Ensemble Learning?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-random-forest-work">3. How Does Random Forest Work?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-steps-in-random-forest">Key Steps in Random Forest:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-random-forest">Why Use Random Forest?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#steps-in-random-forest-algorithm">4. Steps in Random Forest Algorithm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-by-step-process">Step-by-Step Process:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-use-case-iris-flower-classification">5. Example Use Case: Iris Flower Classification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#goal">Goal:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-code">Example Code:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-parameters">Key Parameters:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id44">6. Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-support-vector-machine-svm">22. Machine Learning: Support Vector Machine (SVM)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-support-vector-machine-svm">1. Introduction to Support Vector Machine (SVM)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id45">Key Points:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-boundary-in-svm">2. Decision Boundary in SVM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-support-vector-machine">3. Why ‚ÄúSupport Vector Machine‚Äù?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-uses-of-svm">4. Common Uses of SVM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-svm">5. Types of SVM</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-svm">Linear SVM</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#non-linear-svm">Non-linear SVM</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperplane-and-dimensions">6. Hyperplane and Dimensions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-svm-work">7. How Does SVM Work?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-svm-example">Linear SVM Example:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperplane-and-support-vectors">Hyperplane and Support Vectors</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#use-case-iris-flower-classification">8. Use Case: Iris Flower Classification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id46">Example Code:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id47">Key Parameters:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id48">9. Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-overfitting-and-underfitting">23. Machine Learning: Overfitting and Underfitting</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id49">1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts">2. Key Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#noise">Noise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bias">Bias</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variance">Variance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting">3. Overfitting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#underfitting">4. Underfitting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#good-fit-model">5. Good Fit Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#good-fit-example">6. Good Fit Example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison">Comparison:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization">7. Regularization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-regularization">Types of Regularization:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pros-and-cons-of-regularization">Pros and Cons of Regularization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id50">8. Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-lasso-ridge-regression">24. Machine Learning: Lasso &amp; Ridge Regression</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-linear-regression">1. Introduction to Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-regression">2. Lasso Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id51">Key Points:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-avoid-overfitting-with-lasso">How to Avoid Overfitting with Lasso:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-regression-in-practice">Lasso Regression in Practice:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regression">3. Ridge Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id52">Key Points:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regression-in-practice">Ridge Regression in Practice:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-lasso-and-ridge-regression">4. Comparing Lasso and Ridge Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-melbourne-housing-market">5. Dataset: Melbourne Housing Market</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-details">Dataset Details:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preprocessing">Data Preprocessing:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id53">Splitting the Data:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-model">Linear Regression Model:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-interpretation">Model Interpretation:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id54">6. Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-k-means-clustering">25. Machine Learning: K-Means Clustering</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-clustering">1. What is Clustering?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-means-clustering-algorithm">2. K-Means Clustering Algorithm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id55">Key Concepts:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#steps-in-k-means-algorithm">Steps in K-Means Algorithm:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scenario-k-means-in-action">3. Scenario: K-Means in Action</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-determine-the-correct-number-of-clusters">4. How to Determine the Correct Number of Clusters?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#elbow-method">Elbow Method:</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-in-python">Example in Python:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cluster-scaling">5. Cluster Scaling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering-performance-metrics">6. Clustering Performance Metrics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id56">7. Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-k-nearest-neighbor-k-nn">Machine Learning: K-Nearest Neighbor (K-NN)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-k-nearest-neighbor">1. What is K-Nearest Neighbor?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-characteristics">Key Characteristics:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-k-nn-works">2. How K-NN Works</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-distance-metrics">Types of Distance Metrics:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-scenario">Example Scenario</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id57">3. Use Case: Iris Flower Classification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id58">Goal:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id59">Dataset:</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id60">Example in Python:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#results">Results:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id61">4. Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-naive-bayes-classifier">27. Machine Learning: Na√Øve Bayes Classifier</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-naive-bayes-classifier">1. What is Na√Øve Bayes Classifier?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id62">Key Use Cases:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-it-called-naive-bayes">2. Why is it called Na√Øve Bayes?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem">3. Bayes‚Äô Theorem</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-probability">Conditional Probability:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scenario-coin-flip-example">4. Scenario: Coin Flip Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#use-case-titanic-survival-prediction">5. Use Case: Titanic Survival Prediction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem">Problem:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#steps">Steps:</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id63">Example in Python:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id64">Key Points:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id65">6. Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-gridsearchcv-randomizedsearchcv">29. Machine Learning: GridSearchCV &amp; RandomizedSearchCV</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id66">1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameter-tuning">2. Hyperparameter Tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-hyperparameter-tuning-approaches">Common Hyperparameter Tuning Approaches:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-tune-hyperparameters">Why Tune Hyperparameters?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-functions-for-svm">3. Kernel Functions (for SVM)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ways-to-tune-hyperparameters">4. Ways to Tune Hyperparameters</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approach-1-manual-tuning-with-train-test-split">Approach 1: Manual Tuning with <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approach-2-k-fold-cross-validation">Approach 2: K-Fold Cross Validation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approach-3-gridsearchcv">Approach 3: GridSearchCV</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#randomizedsearchcv">5. RandomizedSearchCV</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-use-randomizedsearchcv">When to Use RandomizedSearchCV?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-benefits-of-randomizedsearchcv">Key Benefits of RandomizedSearchCV:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id67">6. Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-xgboost">30. Machine Learning: XGBoost</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-xgboost">1. Introduction to XGBoost</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-features">Key Features:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-xgboost">2. Why Use XGBoost?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-xgboost">3. Installing XGBoost</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id68">4. Dataset Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-description">Dataset Description:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id69">Features:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id70">5. Input and Output Variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-variables-x">Input Variables (X):</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#output-variable-y">Output Variable (y):</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preprocessing-and-model-training">6. Data Preprocessing and Model Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-the-dataset">Loading the Dataset:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation">Explanation:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#label-encoding-for-categorical-data">7. Label Encoding for Categorical Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-label-encoding">Example of Label Encoding:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id71">Explanation:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-importance-with-xgboost">8. Feature Importance with XGBoost</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-feature-importance">Example of Feature Importance:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id72">Explanation:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id73">9. Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#pickling-and-unpickling-in-python">31. Pickling and Unpickling in Python</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pickling">1. Pickling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id74">Key Points:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-pickling-an-object">Example: Pickling an Object</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unpickling">2. Unpickling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id75">Key Points:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-unpickling-an-object">Example: Unpickling an Object</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#saving-models-in-machine-learning">Saving Models in Machine Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#saving-models-with-pickling">3. Saving Models with Pickling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-saving-and-loading-a-model-using-pickling">Example: Saving and Loading a Model using Pickling</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#saving-models-with-joblib">4. Saving Models with Joblib</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-saving-and-loading-a-model-using-joblib">Example: Saving and Loading a Model using Joblib</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id76">Summary</a></li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="introduction-to-machine-learning">
<h1>1. Introduction to Machine Learning<a class="headerlink" href="#introduction-to-machine-learning" title="Link to this heading">#</a></h1>
<p>Machine learning is a powerful technique that enables computers to automatically learn from past data, identify patterns, and make predictions or decisions without being explicitly programmed.</p>
<hr class="docutils" />
<section id="what-is-machine-learning">
<h2>What is Machine Learning?<a class="headerlink" href="#what-is-machine-learning" title="Link to this heading">#</a></h2>
<p>Machine learning is a branch of artificial intelligence that focuses on creating algorithms that can automatically learn from data and improve over time.</p>
<ul class="simple">
<li><p><strong>Purpose</strong>: Machine learning helps make predictions based on data by identifying patterns and relationships in it.</p></li>
<li><p><strong>Example</strong>: Predicting future values based on past data.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="what-is-a-program">
<h2>What is a Program?<a class="headerlink" href="#what-is-a-program" title="Link to this heading">#</a></h2>
<p>A program is a set of instructions that a computer can execute to perform a specific task.</p>
<ul class="simple">
<li><p><strong>Purpose</strong>: Used for automating tasks.</p></li>
<li><p><strong>Limitation</strong>: Works well for simple tasks but lacks the ability to improve or adapt on its own.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="what-is-an-algorithm">
<h2>What is an Algorithm?<a class="headerlink" href="#what-is-an-algorithm" title="Link to this heading">#</a></h2>
<p>An algorithm is a set of instructions designed to perform a task.</p>
<ul class="simple">
<li><p><strong>Formula</strong>: Input + Logic = Output</p></li>
<li><p><strong>Example</strong>: Data + Program = Result</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="what-is-a-machine-learning-algorithm">
<h2>What is a Machine Learning Algorithm?<a class="headerlink" href="#what-is-a-machine-learning-algorithm" title="Link to this heading">#</a></h2>
<p>A machine learning algorithm is a type of algorithm that allows a model to learn from data and make predictions or decisions without being explicitly programmed.</p>
<ul class="simple">
<li><p><strong>Formula</strong>: Data + Result = Program (Model)</p></li>
<li><p><strong>Difference</strong>: Unlike normal algorithms, machine learning algorithms can learn from data and improve their performance over time.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="differences-between-machine-learning-and-normal-algorithms">
<h2>Differences Between Machine Learning and Normal Algorithms<a class="headerlink" href="#differences-between-machine-learning-and-normal-algorithms" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Machine Learning Algorithm</strong>: Learns from the data and adjusts its model based on patterns.</p></li>
<li><p><strong>Normal Algorithm</strong>: Executes predefined instructions and does not adapt or learn from data.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="why-machine-learning">
<h2>Why Machine Learning?<a class="headerlink" href="#why-machine-learning" title="Link to this heading">#</a></h2>
<p>With the increasing amount of data being generated by companies, machine learning is essential for extracting valuable insights and making predictions.</p>
<section id="key-use-cases">
<h3>Key Use Cases:<a class="headerlink" href="#key-use-cases" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Creating models</strong> to solve complex problems.</p></li>
<li><p><strong>Extracting deep insights</strong> from large datasets.</p></li>
<li><p><strong>Predicting future trends</strong> based on historical data.</p></li>
</ul>
<p>Machine learning is poised to be a central element in all industries, shaping the future of technology.</p>
</section>
</section>
<hr class="docutils" />
<section id="definitions-of-machine-learning">
<h2>Definitions of Machine Learning<a class="headerlink" href="#definitions-of-machine-learning" title="Link to this heading">#</a></h2>
<section id="arthur-samuel-s-definition-1959">
<h3>Arthur Samuel‚Äôs Definition (1959)<a class="headerlink" href="#arthur-samuel-s-definition-1959" title="Link to this heading">#</a></h3>
<p>Arthur Samuel introduced the term ‚Äúmachine learning‚Äù in 1959 and defined it as follows:</p>
<ul class="simple">
<li><p><strong>Machine Learning</strong>: Enables machines to learn automatically from data, improve performance from experiences, and make predictions without being explicitly programmed.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="artificial-intelligence-vs-machine-learning-vs-deep-learning">
<h2>Artificial Intelligence vs Machine Learning vs Deep Learning<a class="headerlink" href="#artificial-intelligence-vs-machine-learning-vs-deep-learning" title="Link to this heading">#</a></h2>
<section id="machine-learning">
<h3>Machine Learning<a class="headerlink" href="#machine-learning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Part of AI</strong>: Machine learning is a subset of artificial intelligence.</p></li>
<li><p><strong>Purpose</strong>: It allows systems to learn from data, apply that learning, and make decisions.</p></li>
<li><p><strong>Examples</strong>:</p>
<ul>
<li><p><strong>Amazon</strong>: Uses machine learning to recommend products based on customer preferences.</p></li>
<li><p><strong>Netflix</strong>: Uses machine learning to suggest movies, TV series, or shows to users.</p></li>
</ul>
</li>
</ul>
</section>
<section id="deep-learning">
<h3>Deep Learning<a class="headerlink" href="#deep-learning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Subset of Machine Learning</strong>: Deep learning is a subset of machine learning that mimics the way humans think.</p></li>
<li><p><strong>Difference</strong>: While machine learning requires some human guidance, deep learning models improve by themselves.</p></li>
<li><p><strong>Example</strong>: <strong>Self-driving cars</strong> use deep learning algorithms to make decisions and drive without human intervention.</p></li>
</ul>
</section>
<section id="artificial-intelligence-ai">
<h3>Artificial Intelligence (AI)<a class="headerlink" href="#artificial-intelligence-ai" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>AI Definition</strong>: The ability of computer systems to function like the human brain.</p></li>
<li><p><strong>Relationship</strong>: Machine learning and deep learning are subsets of AI.</p></li>
<li><p><strong>Goal</strong>: Replicating human-like thinking and decision-making processes.</p></li>
<li><p><strong>Example</strong>: <strong>Sophia</strong>, the most advanced AI model, represents the state of current AI technology.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="human-vs-computer-decision-making">
<h2>Human vs Computer Decision-Making<a class="headerlink" href="#human-vs-computer-decision-making" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Humans</strong>: Make decisions based on experience.</p></li>
<li><p><strong>Computers</strong>: Use data to create models and make predictions.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="how-do-machines-think">
<h2>How Do Machines ‚ÄúThink‚Äù?<a class="headerlink" href="#how-do-machines-think" title="Link to this heading">#</a></h2>
<p>The goal of machine learning is to make computers think like humans using the ‚ÄúRemember - Formulate - Predict‚Äù framework:</p>
<ol class="arabic simple">
<li><p><strong>Remember</strong>: Process large amounts of data.</p></li>
<li><p><strong>Formulate</strong>: Use rules and formulas to analyze the data.</p></li>
<li><p><strong>Predict</strong>: Use the formulated rules to predict future outcomes.</p></li>
</ol>
</section>
<hr class="docutils" />
<section id="real-time-examples-of-machine-learning">
<h2>Real-Time Examples of Machine Learning<a class="headerlink" href="#real-time-examples-of-machine-learning" title="Link to this heading">#</a></h2>
<section id="gmail-spam-filter">
<h3>Gmail Spam Filter<a class="headerlink" href="#gmail-spam-filter" title="Link to this heading">#</a></h3>
<p>Machine learning is used to filter spam emails:</p>
<ul class="simple">
<li><p>The algorithm learns from previous emails (data).</p></li>
<li><p>It creates models based on this data.</p></li>
<li><p>The model is applied to incoming emails to predict whether they are spam or not.</p></li>
</ul>
</section>
<section id="banking-loan-application-credit-card-approval">
<h3>Banking Loan Application / Credit Card Approval<a class="headerlink" href="#banking-loan-application-credit-card-approval" title="Link to this heading">#</a></h3>
<p>Machine learning is applied to predict whether a loan or credit card should be sanctioned:</p>
<ul class="simple">
<li><p>The algorithm analyzes the applicant‚Äôs data (e.g., pay slip, bank statements).</p></li>
<li><p>Based on this data, the machine learning model predicts whether the loan or credit card will be approved.</p></li>
</ul>
</section>
</section>
</section>
<hr class="docutils" />
<section id="machine-learning-terminology">
<h1>2. Machine Learning Terminology<a class="headerlink" href="#machine-learning-terminology" title="Link to this heading">#</a></h1>
<p>Understanding the key terminology in machine learning is essential for anyone starting in the field. Here‚Äôs an organized overview of the most common terms.</p>
<hr class="docutils" />
<section id="id1">
<h2>Machine Learning<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Definition</strong>: Machine learning is a technique that enables computers to automatically learn from past data.</p></li>
<li><p><strong>Purpose</strong>: It is used to train models that can predict future values based on historical data.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="model">
<h2>Model<a class="headerlink" href="#model" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Definition</strong>: A model is a representation of reality created to perform a specific task.</p></li>
<li><p><strong>Types of Models</strong>:</p>
<ul>
<li><p>A <strong>piece of code</strong> or program.</p></li>
<li><p>A <strong>mathematical formula</strong>.</p></li>
<li><p>A combination of both a <strong>program</strong> and <strong>mathematical formula</strong>.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="training-a-model">
<h2>Training a Model<a class="headerlink" href="#training-a-model" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Definition</strong>: Training a model involves using data to teach the model to recognize patterns and make predictions.</p></li>
<li><p><strong>Process</strong>:</p>
<ul>
<li><p>The model learns patterns from the data during training.</p></li>
<li><p><strong>Before training</strong>: The model doesn‚Äôt know the patterns.</p></li>
<li><p><strong>After training</strong>: The model understands and applies the learned patterns.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="testing-the-model">
<h2>Testing the Model<a class="headerlink" href="#testing-the-model" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Definition</strong>: After the model has been trained, it needs to be tested to check its accuracy.</p></li>
<li><p><strong>Goal</strong>: If the model‚Äôs accuracy is satisfactory, it indicates that the model works well.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="model-deployment">
<h2>Model Deployment<a class="headerlink" href="#model-deployment" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Definition</strong>: Deploying the model involves using the knowledge the model has learned in real-world applications.</p></li>
<li><p><strong>Other Term</strong>: This process is sometimes referred to as making the model ‚Äúfit‚Äù for use.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="normal-vs-smart-applications">
<h2>Normal vs Smart Applications<a class="headerlink" href="#normal-vs-smart-applications" title="Link to this heading">#</a></h2>
<section id="normal-application">
<h3>Normal Application<a class="headerlink" href="#normal-application" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Definition</strong>: A normal application is a software that operates based on a set of instructions.</p></li>
<li><p><strong>Limitation</strong>: Normal applications cannot make decisions on their own or think like a human.</p></li>
</ul>
</section>
<section id="smart-application">
<h3>Smart Application<a class="headerlink" href="#smart-application" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Definition</strong>: A smart application is one that can make decisions and think like a human.</p></li>
<li><p><strong>Intelligence-Based</strong>: These applications use machine learning algorithms to simulate human decision-making.</p></li>
</ul>
<section id="example-use-case">
<h4>Example Use Case:<a class="headerlink" href="#example-use-case" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Gmail Spam Filter</strong>: When you send an email, Gmail uses machine learning to predict whether the email is spam or not.</p></li>
</ul>
</section>
</section>
</section>
<hr class="docutils" />
<section id="accuracy-testing">
<h2>Accuracy Testing<a class="headerlink" href="#accuracy-testing" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Definition</strong>: Before deploying a model, it must be tested in various conditions to ensure it performs well.</p></li>
<li><p><strong>Goal</strong>: If the model‚Äôs predictions are accurate, it is ready to be deployed.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="data-machine-learning-algorithm-terminology">
<h1>3. Data &amp; Machine Learning Algorithm Terminology<a class="headerlink" href="#data-machine-learning-algorithm-terminology" title="Link to this heading">#</a></h1>
<p>Understanding data and how machine learning algorithms use it is crucial for anyone starting in the field of machine learning. Here‚Äôs a structured overview of key terms.</p>
<hr class="docutils" />
<section id="data">
<h2>Data<a class="headerlink" href="#data" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Definition</strong>: Data is a collection of facts, which can be in various forms:</p>
<ul>
<li><p>Alphabets</p></li>
<li><p>Numbers</p></li>
<li><p>Alphanumeric values</p></li>
<li><p>Symbols</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="data-in-tables">
<h2>Data in Tables<a class="headerlink" href="#data-in-tables" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Structure</strong>: Data is often organized into tables with rows, columns, and cells.</p></li>
<li><p><strong>Benefits</strong>: Tabular data is easy to understand and analyze.</p></li>
</ul>
<section id="key-components-of-a-table">
<h3>Key Components of a Table<a class="headerlink" href="#key-components-of-a-table" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Row</strong>:</p>
<ul>
<li><p>Represents a single entity or observation.</p></li>
<li><p>Also known as a record.</p></li>
</ul>
</li>
<li><p><strong>Column</strong>:</p>
<ul>
<li><p>A vertical group of cells within a table.</p></li>
<li><p>Each column contains values of the same type (e.g., weights, heights, prices).</p></li>
</ul>
</li>
<li><p><strong>Cell</strong>:</p>
<ul>
<li><p>A single value at the intersection of a row and column.</p></li>
<li><p>Values can be real numbers, integers, or categories (e.g., 1.5, 2, or red).</p></li>
</ul>
</li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="statistical-learning-perspective">
<h2>Statistical Learning Perspective<a class="headerlink" href="#statistical-learning-perspective" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Data in Statistical Learning</strong>: In this context, data represents the input-output relationship that the machine learning algorithm is trying to learn.</p></li>
<li><p><strong>Formula Example</strong>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">Rice_yield</span> <span class="pre">=</span> <span class="pre">land_size</span> <span class="pre">x</span> <span class="pre">10</span></code></p></li>
</ul>
</li>
<li><p><strong>Why Machine Learning Algorithms Learn this Function</strong>:</p>
<ul>
<li><p>The goal is to predict output for a given input.</p></li>
<li><p>The function relationship can be expressed as: <code class="docutils literal notranslate"><span class="pre">Output</span> <span class="pre">=</span> <span class="pre">f(Input)</span></code></p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="input-and-output-variables">
<h2>Input and Output Variables<a class="headerlink" href="#input-and-output-variables" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Input Variables</strong>:</p>
<ul>
<li><p>Also called <strong>Independent Variables</strong>.</p></li>
<li><p>Represented by <code class="docutils literal notranslate"><span class="pre">X</span></code>.</p></li>
<li><p>These are the factors the model uses to make predictions.</p></li>
</ul>
</li>
<li><p><strong>Output Variable</strong>:</p>
<ul>
<li><p>Also called the <strong>Dependent Variable</strong>.</p></li>
<li><p>Represented by <code class="docutils literal notranslate"><span class="pre">y</span></code>.</p></li>
<li><p>The predicted result that depends on the input variables.</p></li>
</ul>
</li>
<li><p><strong>Multiple Input Variables</strong>:</p>
<ul>
<li><p>When there are multiple input variables, they are collectively called an <strong>Input Vector</strong>.</p></li>
<li><p>Example: <code class="docutils literal notranslate"><span class="pre">Output</span> <span class="pre">Variable</span> <span class="pre">=</span> <span class="pre">f(Input</span> <span class="pre">Vector)</span></code></p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="terminology-in-statistics">
<h2>Terminology in Statistics<a class="headerlink" href="#terminology-in-statistics" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Independent Variables</strong>: Input variables that affect the output. Represented as <code class="docutils literal notranslate"><span class="pre">X</span></code>.</p></li>
<li><p><strong>Dependent Variable</strong>: The output that depends on the function of input variables. Represented as <code class="docutils literal notranslate"><span class="pre">y</span></code>.</p>
<ul>
<li><p>Formula: <code class="docutils literal notranslate"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">f(X)</span></code></p></li>
</ul>
</li>
<li><p><strong>Multiple Input Variables</strong>: If there are multiple input variables, they are represented as <code class="docutils literal notranslate"><span class="pre">x1,</span> <span class="pre">x2,</span> <span class="pre">x3</span></code>, etc.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="computer-science-perspective">
<h2>Computer Science Perspective<a class="headerlink" href="#computer-science-perspective" title="Link to this heading">#</a></h2>
<ul>
<li><p><strong>Row</strong>: Represents an entity, observation, instance, or object in a table.</p></li>
<li><p><strong>Column</strong>: Known as an <strong>attribute</strong> in this context.</p></li>
<li><p><strong>Modelling &amp; Prediction</strong>:</p>
<ul class="simple">
<li><p><strong>Input</strong>: Referred to as an <strong>input attribute</strong>.</p></li>
<li><p><strong>Output</strong>: Referred to as an <strong>output attribute</strong>.</p></li>
</ul>
<p>Formula: <code class="docutils literal notranslate"><span class="pre">Output</span> <span class="pre">Attribute</span> <span class="pre">=</span> <span class="pre">Program(Input</span> <span class="pre">Attributes)</span></code></p>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="models-and-algorithms">
<h2>Models and Algorithms<a class="headerlink" href="#models-and-algorithms" title="Link to this heading">#</a></h2>
<ul>
<li><p><strong>Model</strong>: A specific representation learned from data.</p></li>
<li><p><strong>Algorithm</strong>: The process used to learn the model.</p>
<p>Formula: <code class="docutils literal notranslate"><span class="pre">Model</span> <span class="pre">=</span> <span class="pre">Algorithm(Data)</span></code></p>
</li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="machine-learning-learning-a-function">
<h1>4. Machine Learning: Learning a Function<a class="headerlink" href="#machine-learning-learning-a-function" title="Link to this heading">#</a></h1>
<p>Machine learning algorithms aim to estimate a function that can map input variables to output variables. Understanding how this function is learned and how errors are handled is key to grasping machine learning models.</p>
<hr class="docutils" />
<section id="learning-a-function">
<h2>Learning a Function<a class="headerlink" href="#learning-a-function" title="Link to this heading">#</a></h2>
<ul>
<li><p>Machine learning algorithms focus on learning a <strong>target function</strong> <code class="docutils literal notranslate"><span class="pre">f</span></code>.</p></li>
<li><p>This function maps <strong>input variables (X)</strong> to <strong>output variable (y)</strong>, expressed as:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Output</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">Input</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>The function may include some <strong>error</strong> term, independent of the input data:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="o">+</span> <span class="n">error</span>
</pre></div>
</div>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="purpose-of-the-learning-function">
<h2>Purpose of the Learning Function<a class="headerlink" href="#purpose-of-the-learning-function" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>The main goal of learning this function is to <strong>make predictions</strong>.</p></li>
<li><p>The accuracy of these predictions depends on minimizing the error.</p></li>
<li><p>A function with <strong>less error</strong> will make more <strong>accurate predictions</strong>. This process is called <strong>predictive modelling</strong>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="machine-learning-algorithms">
<h2>Machine Learning Algorithms<a class="headerlink" href="#machine-learning-algorithms" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Machine learning algorithms are techniques used to estimate the target function <code class="docutils literal notranslate"><span class="pre">f</span></code>.</p></li>
<li><p>The function helps predict the output variable <code class="docutils literal notranslate"><span class="pre">y</span></code> based on the input variables <code class="docutils literal notranslate"><span class="pre">X</span></code>.</p></li>
<li><p>Different machine learning algorithms make <strong>different assumptions</strong> about the form of the function, such as:</p>
<ul>
<li><p>Whether it is <strong>linear</strong> or <strong>nonlinear</strong>.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>Machine learning is all about learning functions that map inputs to outputs. The more accurately this function is learned (with minimal error), the better the model will make predictions. Different algorithms approach this task in varied ways based on the assumptions they make about the function.</p>
</section>
</section>
<hr class="docutils" />
<section id="types-of-machine-learning-models">
<h1>5. Types of Machine Learning Models<a class="headerlink" href="#types-of-machine-learning-models" title="Link to this heading">#</a></h1>
<p>In machine learning, data plays a crucial role in training models. Understanding the distinction between features, labels, and types of data is key to choosing the right model for your task.</p>
<hr class="docutils" />
<section id="features-and-labels">
<h2>Features and Labels<a class="headerlink" href="#features-and-labels" title="Link to this heading">#</a></h2>
<section id="features">
<h3>Features<a class="headerlink" href="#features" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Features</strong> are simply the <strong>columns</strong> of a dataset.</p></li>
<li><p>They describe the characteristics or properties of the data.</p></li>
<li><p>Example: In a dataset containing personal information, features might include <strong>Age</strong>, <strong>Gender</strong>, <strong>Experience</strong>, and <strong>Salary</strong>.</p></li>
</ul>
</section>
<section id="label">
<h3>Label<a class="headerlink" href="#label" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The <strong>label</strong> is the <strong>output</strong> the model predicts after training.</p></li>
<li><p>Example: If you‚Äôre predicting the <strong>salary</strong> of someone with 6 years of experience, <strong>salary</strong> is the label.</p></li>
</ul>
<section id="label-example">
<h4>Label Example<a class="headerlink" href="#label-example" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>If predicting the type of pet (e.g., <strong>cat</strong> or <strong>dog</strong>) based on certain information, <strong>pet type</strong> is the label.</p></li>
<li><p>If predicting whether a pet is <strong>sick</strong> or <strong>healthy</strong>, the <strong>health status</strong> is the label.</p></li>
<li><p>If predicting the <strong>age</strong> of a pet, <strong>age</strong> is the label.</p></li>
</ul>
</section>
</section>
</section>
<hr class="docutils" />
<section id="labelled-and-unlabelled-data">
<h2>Labelled and Unlabelled Data<a class="headerlink" href="#labelled-and-unlabelled-data" title="Link to this heading">#</a></h2>
<section id="labelled-data">
<h3>Labelled Data<a class="headerlink" href="#labelled-data" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Labelled data comes with a <strong>tag</strong> or <strong>label</strong>, like a name, type, or number.</p></li>
<li><p>Example: A dataset of <strong>students‚Äô marks</strong> with labels indicating whether the students passed or failed.</p></li>
</ul>
</section>
<section id="unlabelled-data">
<h3>Unlabelled Data<a class="headerlink" href="#unlabelled-data" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Unlabelled data <strong>does not have any tags or labels</strong>.</p></li>
<li><p>Example: A dataset of customer behavior without knowing whether each customer made a purchase or not.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="supervised-vs-unsupervised-learning">
<h2>Supervised vs Unsupervised Learning<a class="headerlink" href="#supervised-vs-unsupervised-learning" title="Link to this heading">#</a></h2>
<section id="supervised-learning">
<h3>Supervised Learning<a class="headerlink" href="#supervised-learning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Supervised learning</strong> involves training models using both <strong>input features</strong> and <strong>labels</strong>.</p></li>
<li><p>Example: Predicting the <strong>salary</strong> of an employee based on <strong>experience</strong> (input feature) and using historical salary data (label).</p></li>
</ul>
</section>
<section id="unsupervised-learning">
<h3>Unsupervised Learning<a class="headerlink" href="#unsupervised-learning" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Unsupervised learning</strong> involves training models using only <strong>input features</strong>, without any labels.</p></li>
<li><p>Example: Grouping data points based on their <strong>similarities</strong> without knowing the labels.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="types-of-supervised-learning-models">
<h2>Types of Supervised Learning Models<a class="headerlink" href="#types-of-supervised-learning-models" title="Link to this heading">#</a></h2>
<p>Supervised learning models are primarily divided into two categories:</p>
<section id="regression-models">
<h3>1. Regression Models<a class="headerlink" href="#regression-models" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Regression models</strong> predict continuous numerical values.</p></li>
<li><p>The output is <strong>continuous</strong>, meaning it can take any real value.</p></li>
<li><p>Examples:</p>
<ul>
<li><p>Weight of an animal</p></li>
<li><p>Employee salary</p></li>
<li><p>Students‚Äô marks</p></li>
<li><p>Stock market prediction</p></li>
<li><p>Number of sales</p></li>
<li><p>House price prediction</p></li>
</ul>
</li>
</ul>
</section>
<section id="classification-models">
<h3>2. Classification Models<a class="headerlink" href="#classification-models" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Classification models</strong> predict discrete categories or states (classes).</p></li>
<li><p>Examples:</p>
<ul>
<li><p>Type of animal (e.g., <strong>cat</strong> or <strong>dog</strong>)</p></li>
<li><p>Gender (e.g., <strong>male</strong> or <strong>female</strong>)</p></li>
<li><p>Biryani taste (e.g., <strong>good</strong> or <strong>bad</strong>)</p></li>
<li><p>Email classification (e.g., <strong>spam</strong> or <strong>ham</strong>)</p></li>
</ul>
</li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="types-of-unsupervised-learning-models">
<h2>Types of Unsupervised Learning Models<a class="headerlink" href="#types-of-unsupervised-learning-models" title="Link to this heading">#</a></h2>
<p>Unsupervised learning models are divided into two primary types:</p>
<section id="clustering">
<h3>1. Clustering<a class="headerlink" href="#clustering" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Clustering</strong> involves grouping data based on similarities.</p></li>
<li><p>Data points in each group (or <strong>cluster</strong>) share similar characteristics.</p></li>
</ul>
</section>
<section id="dimensionality-reduction">
<h3>2. Dimensionality Reduction<a class="headerlink" href="#dimensionality-reduction" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Dimensionality reduction</strong> simplifies data by reducing the number of features, while retaining as much information as possible.</p></li>
<li><p>It makes data easier to visualize and analyze without losing general trends.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="summary">
<h2>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Features</strong> are the inputs, and the <strong>label</strong> is the predicted output.</p></li>
<li><p>In <strong>supervised learning</strong>, models are trained using both <strong>features</strong> and <strong>labels</strong>.</p></li>
<li><p>In <strong>unsupervised learning</strong>, models are trained only with <strong>features</strong> and are used to group or reduce the data‚Äôs dimensions.</p></li>
<li><p><strong>Regression models</strong> predict continuous values, while <strong>classification models</strong> predict categorical outcomes.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="machine-learning-life-cycle">
<h1>6. Machine Learning Life Cycle<a class="headerlink" href="#machine-learning-life-cycle" title="Link to this heading">#</a></h1>
<p>The machine learning life cycle consists of six key steps, each playing a critical role in the development and deployment of a model. These steps ensure the model learns effectively and provides reliable predictions.</p>
<hr class="docutils" />
<section id="data-collection">
<h2>1. Data Collection<a class="headerlink" href="#data-collection" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Data gathering</strong> is the first step in the machine learning life cycle.</p></li>
<li><p>The goal is to identify and collect relevant data from various sources.</p></li>
<li><p>Data can be collected from different sources such as:</p>
<ul>
<li><p>Files</p></li>
<li><p>Databases</p></li>
<li><p>Web scraping</p></li>
<li><p>APIs, and more.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="data-preparation">
<h2>2. Data Preparation<a class="headerlink" href="#data-preparation" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>In this step, we gain a deep understanding of the data.</p></li>
<li><p>This includes assessing the <strong>data format</strong> and <strong>data quality</strong>.</p></li>
<li><p>A thorough understanding helps in identifying:</p>
<ul>
<li><p><strong>Correlations</strong> between features.</p></li>
<li><p><strong>General trends</strong> in the data.</p></li>
<li><p><strong>Outliers</strong> that might skew results.</p></li>
</ul>
</li>
<li><p>Proper data preparation leads to more effective outcomes.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="data-wrangling">
<h2>3. Data Wrangling<a class="headerlink" href="#data-wrangling" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Data wrangling</strong> is the process of cleaning and converting raw data into a usable format.</p></li>
<li><p>This step includes:</p>
<ul>
<li><p><strong>Cleaning the data</strong>: Removing inconsistencies and errors.</p></li>
<li><p>Handling <strong>missing values</strong>: Ensuring there are no gaps in the data.</p></li>
<li><p>Removing <strong>duplicate data</strong>: Ensuring each data point is unique.</p></li>
<li><p>Correcting <strong>invalid data</strong>: Fixing errors or outliers that do not make sense.</p></li>
<li><p><strong>Selecting relevant variables</strong>: Identifying the features that will be used for model training.</p></li>
</ul>
</li>
<li><p>Not all collected data will be useful, so this step ensures only the relevant data is used for modeling.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="train-the-model">
<h2>4. Train the Model<a class="headerlink" href="#train-the-model" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>In the <strong>training phase</strong>, the model learns from the data.</p></li>
<li><p>The model identifies patterns, rules, and relationships between features and the output.</p></li>
<li><p>A <strong>training dataset</strong> is used to teach the model how to make predictions based on input features.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="test-the-model">
<h2>5. Test the Model<a class="headerlink" href="#test-the-model" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>After training, the model needs to be <strong>tested</strong> to evaluate its performance.</p></li>
<li><p>A <strong>test dataset</strong> is used to check the model‚Äôs accuracy.</p></li>
<li><p>Testing helps verify whether the model has learned effectively and can make accurate predictions on new, unseen data.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id2">
<h2>6. Model Deployment<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Once the model is trained and tested, and if it shows satisfactory results, it can be deployed.</p></li>
<li><p><strong>Model deployment</strong> involves integrating the model into real-world applications where it can make predictions in real time.</p></li>
<li><p>This step brings the machine learning model into production, allowing it to provide valuable insights and predictions for business or operational needs.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id3">
<h2>Summary<a class="headerlink" href="#id3" title="Link to this heading">#</a></h2>
<p>The machine learning life cycle is an iterative process that begins with <strong>data collection</strong> and continues through <strong>data preparation</strong>, <strong>wrangling</strong>, and training before testing the model‚Äôs performance. Once successful, the model is deployed for real-world use.</p>
</section>
</section>
<hr class="docutils" />
<section id="train-test-datasets-in-machine-learning">
<h1>7. Train &amp; Test Datasets in Machine Learning<a class="headerlink" href="#train-test-datasets-in-machine-learning" title="Link to this heading">#</a></h1>
<p>In machine learning, data is typically divided into different datasets for training and evaluation purposes. The main types of datasets are:</p>
<ul class="simple">
<li><p><strong>Train Dataset</strong></p></li>
<li><p><strong>Test Dataset</strong></p></li>
<li><p><strong>Validation Dataset</strong> (optional)</p></li>
</ul>
<hr class="docutils" />
<section id="types-of-datasets">
<h2>Types of Datasets<a class="headerlink" href="#types-of-datasets" title="Link to this heading">#</a></h2>
<section id="train-dataset">
<h3>1. Train Dataset<a class="headerlink" href="#train-dataset" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Purpose</strong>: Used to <strong>train the model</strong>.</p></li>
<li><p>It represents the data the model will learn from.</p></li>
<li><p>Typically, the <strong>train dataset</strong> makes up <strong>60-70%</strong> of the total dataset.</p></li>
<li><p>During training, the model learns the underlying patterns, parameters, and concepts from this data.</p></li>
</ul>
</section>
<section id="test-dataset">
<h3>2. Test Dataset<a class="headerlink" href="#test-dataset" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Purpose</strong>: Used to <strong>test the model</strong> after training.</p></li>
<li><p>Once the model is trained, the test dataset evaluates how well the model performs.</p></li>
<li><p>The <strong>test dataset</strong> typically constitutes <strong>15-30%</strong> of the total dataset.</p></li>
<li><p>It helps assess the <strong>accuracy</strong> and <strong>generalization</strong> capability of the model.</p></li>
</ul>
</section>
<section id="validation-dataset-optional">
<h3>3. Validation Dataset (Optional)<a class="headerlink" href="#validation-dataset-optional" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Purpose</strong>: Used to fine-tune the model, often in hyperparameter tuning.</p></li>
<li><p>This dataset is <strong>optional</strong>, unlike the training and test datasets, which are mandatory.</p></li>
<li><p>The validation dataset helps check the performance of the model while adjusting its parameters.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="deciding-the-size-of-datasets">
<h2>Deciding the Size of Datasets<a class="headerlink" href="#deciding-the-size-of-datasets" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>There is no strict rule for splitting the data, but a common guideline is:</p>
<ul>
<li><p><strong>70% for training</strong></p></li>
<li><p><strong>30% for testing</strong> (or <strong>60% for training</strong> and <strong>40% for testing</strong>).</p></li>
</ul>
</li>
<li><p>The exact ratio depends on the size and nature of the dataset, but these proportions are often a good starting point.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="using-train-test-split-p-function">
<h2>Using <code class="docutils literal notranslate"><span class="pre">train_test_split(p)</span></code> Function<a class="headerlink" href="#using-train-test-split-p-function" title="Link to this heading">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">train_test_split(p)</span></code> function in <strong>scikit-learn</strong> allows easy splitting of datasets into train and test sets.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Example 1: Splitting a dataset into train and test sets</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Example 2: Splitting feature data (X) and target data (y)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">20</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="using-train-test-split-p-random-state-0-function">
<h2>Using <code class="docutils literal notranslate"><span class="pre">train_test_split(p,</span> <span class="pre">random_state=0)</span></code> Function<a class="headerlink" href="#using-train-test-split-p-random-state-0-function" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>This variation of <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> ensures that the split is <strong>reproducible</strong> across different runs.</p></li>
<li><p>By setting the <strong>random_state</strong> parameter to a fixed value (e.g., <code class="docutils literal notranslate"><span class="pre">random_state=0</span></code>), we ensure that the same training and testing datasets are used every time the function is called.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Example with random_state to ensure reproducibility</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="id4">
<h2>Summary<a class="headerlink" href="#id4" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Training Dataset</strong>: Used to train the model, typically 60-70% of the data.</p></li>
<li><p><strong>Testing Dataset</strong>: Used to evaluate the model‚Äôs performance, typically 15-30% of the data.</p></li>
<li><p><strong>Validation Dataset</strong>: Optional, used for hyperparameter tuning.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> to easily split datasets into train and test sets with scikit-learn.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="r-value-and-regression-analysis">
<h1>8. R-Value and Regression Analysis<a class="headerlink" href="#r-value-and-regression-analysis" title="Link to this heading">#</a></h1>
<section id="regression-analysis">
<h2>Regression Analysis<a class="headerlink" href="#regression-analysis" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Purpose</strong>: Regression analysis is used to explain the relationship between a dependent variable and one or more independent variables.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="understanding-a-line-in-regression">
<h2>Understanding a Line in Regression<a class="headerlink" href="#understanding-a-line-in-regression" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>When two variables have a relationship, it can be visualized as a <strong>straight line</strong> on a 2D graph.</p></li>
<li><p><strong>Linear Regression</strong> aims to find the line that best represents the relationship between the variables.</p>
<ul>
<li><p>Given a set of data points, the regression line is the one that passes as close as possible to these points.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="goal-of-linear-regression">
<h2>Goal of Linear Regression<a class="headerlink" href="#goal-of-linear-regression" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>The <strong>goal</strong> of linear regression is to draw the <strong>best fitted line</strong>, i.e., a line that best approximates the relationship between the dependent and independent variables.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="can-we-use-regression-everywhere">
<h2>Can We Use Regression Everywhere?<a class="headerlink" href="#can-we-use-regression-everywhere" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Linear Regression</strong> can only be applied when there is a clear relationship between the variables.</p>
<ul>
<li><p>If no relationship exists between the variables, linear regression is not appropriate.</p></li>
<li><p>It‚Äôs important to assess the relationship first to determine if regression is suitable.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="r-value-understanding-the-strength-of-relationship">
<h2>R-Value: Understanding the Strength of Relationship<a class="headerlink" href="#r-value-understanding-the-strength-of-relationship" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>The <strong>r value</strong> measures the strength of the relationship between the two variables.</p>
<ul>
<li><p>It quantifies how well the data points fit the regression line.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="r-value-range">
<h2>R-Value Range<a class="headerlink" href="#r-value-range" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>The <strong>r value</strong> ranges from <strong>-1 to 1</strong>:</p>
<ul>
<li><p><strong>r = 1</strong>: Perfect positive relationship.</p></li>
<li><p><strong>r = -1</strong>: Perfect negative relationship.</p></li>
<li><p><strong>r = 0</strong>: No relationship between the variables.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="calculating-the-r-value">
<h2>Calculating the R-Value<a class="headerlink" href="#calculating-the-r-value" title="Link to this heading">#</a></h2>
<p>To calculate the r value, we can use the <strong>scipy</strong> module in Python.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">scipy</span><span class="w"> </span><span class="kn">import</span> <span class="n">stats</span>

<span class="c1"># Calculate r value</span>
<span class="n">slope</span><span class="p">,</span> <span class="n">intercept</span><span class="p">,</span> <span class="n">r_value</span><span class="p">,</span> <span class="n">p_value</span><span class="p">,</span> <span class="n">std_err</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">linregress</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="interpreting-r-value-results">
<h2>Interpreting R-Value Results<a class="headerlink" href="#interpreting-r-value-results" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Example 1</strong>: If the <strong>r value = 1.0</strong>, it indicates a strong positive relationship between the variables.</p>
<ul>
<li><p><strong>Conclusion</strong>: You can confidently apply linear regression to predict future values.</p></li>
</ul>
</li>
<li><p><strong>Example 2</strong>: If the <strong>r value = -0.065</strong>, which is close to 0, it indicates a weak or no relationship between the variables.</p>
<ul>
<li><p><strong>Conclusion</strong>: Linear regression is not suitable, and any predictions would likely be inaccurate.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="id5">
<h2>Summary<a class="headerlink" href="#id5" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Linear regression</strong> is a powerful tool, but it‚Äôs essential to first check if there‚Äôs a <strong>relationship</strong> between the variables.</p></li>
<li><p>The <strong>r value</strong> helps in assessing the strength of this relationship, guiding whether regression is the right approach.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="simple-linear-regression">
<h1>Simple Linear Regression<a class="headerlink" href="#simple-linear-regression" title="Link to this heading">#</a></h1>
<section id="what-is-linear-regression">
<h2>What is Linear Regression?<a class="headerlink" href="#what-is-linear-regression" title="Link to this heading">#</a></h2>
<p>Linear regression is a statistical technique used to model the relationship between a dependent variable and one or more independent variables. It helps in predicting the value of the dependent variable based on the independent variables.</p>
</section>
<hr class="docutils" />
<section id="types-of-linear-regression">
<h2>Types of Linear Regression<a class="headerlink" href="#types-of-linear-regression" title="Link to this heading">#</a></h2>
<p>There are two main types of linear regression:</p>
<ol class="arabic simple">
<li><p><strong>Simple Linear Regression</strong></p></li>
<li><p><strong>Multiple Linear Regression</strong></p></li>
</ol>
<section id="id6">
<h3>1. Simple Linear Regression<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Simple linear regression is used when you have <strong>one independent variable</strong> and <strong>one dependent variable</strong>.</p></li>
</ul>
</section>
<section id="multiple-linear-regression">
<h3>2. Multiple Linear Regression<a class="headerlink" href="#multiple-linear-regression" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Multiple linear regression is used when you have <strong>two or more independent variables</strong> and <strong>one dependent variable</strong>.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="linear-regression-formula">
<h2>Linear Regression Formula<a class="headerlink" href="#linear-regression-formula" title="Link to this heading">#</a></h2>
<p>The general formula for linear regression is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Home</span> <span class="n">Price</span> <span class="o">=</span> <span class="n">m</span> <span class="o">*</span> <span class="p">(</span><span class="n">Area</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
</pre></div>
</div>
<p>Where:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">m</span></code> is the <strong>coefficient</strong> (also called slope)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">b</span></code> is the <strong>intercept</strong>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="example-simple-linear-regression-in-python">
<h2>Example: Simple Linear Regression in Python<a class="headerlink" href="#example-simple-linear-regression-in-python" title="Link to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="c1"># Create a LinearRegression object</span>
<span class="n">reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>

<span class="c1"># Fit the model using the data</span>
<span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">new_df</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">price</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="c1"># Make predictions</span>
<span class="nb">print</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">3300</span><span class="p">]]))</span>

<span class="c1"># Output the coefficients and intercept</span>
<span class="nb">print</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">reg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>Formula</strong>: <code class="docutils literal notranslate"><span class="pre">Y</span> <span class="pre">=</span> <span class="pre">m</span> <span class="pre">*</span> <span class="pre">X</span> <span class="pre">+</span> <span class="pre">b</span></code></p>
<ul>
<li><p>Where:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">m</span></code> is the coefficient (slope)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">b</span></code> is the intercept</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">X</span></code> is the independent variable (Area)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">Y</span></code> is the dependent variable (Price)</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<section id="example-calculation">
<h3>Example Calculation<a class="headerlink" href="#example-calculation" title="Link to this heading">#</a></h3>
<p>Let‚Äôs calculate the price for an area of 3300 sq ft:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Y</span> <span class="o">=</span> <span class="mf">135.78767123</span> <span class="o">*</span> <span class="mi">3300</span> <span class="o">+</span> <span class="mf">180616.43835616432</span>
<span class="n">Y</span> <span class="o">=</span> <span class="mf">628715.75342466</span>
</pre></div>
</div>
<p>The predicted home price for an area of 3300 sq ft is <strong>$628,715.75</strong>.</p>
</section>
</section>
<hr class="docutils" />
<section id="visualizing-the-best-fitted-line">
<h2>Visualizing the Best Fitted Line<a class="headerlink" href="#visualizing-the-best-fitted-line" title="Link to this heading">#</a></h2>
<p>We can visualize the relationship between area and price with a scatter plot and the best fitted line using <strong>Matplotlib</strong>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Label the axes</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Area&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Price&#39;</span><span class="p">)</span>

<span class="c1"># Scatter plot for the data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">area</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">price</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>

<span class="c1"># Plot the best fitted line</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">area</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;area&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">)</span>

<span class="c1"># Show the plot</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<p>This code generates a scatter plot with the data points and overlays the best fitted line in blue.</p>
</section>
<hr class="docutils" />
<section id="id7">
<h2>Summary<a class="headerlink" href="#id7" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Simple Linear Regression</strong> is used when there‚Äôs a linear relationship between one independent variable and a dependent variable.</p></li>
<li><p>The <strong>Linear Regression Formula</strong> is used to predict the dependent variable using the independent variable(s).</p></li>
<li><p><strong>Visualization</strong> helps in understanding the relationship between variables and the accuracy of the model.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="linear-regression-example">
<h1>Linear Regression Example<a class="headerlink" href="#linear-regression-example" title="Link to this heading">#</a></h1>
<section id="scenario">
<h2>Scenario<a class="headerlink" href="#scenario" title="Link to this heading">#</a></h2>
<p>Let‚Äôs find the relationship between <strong>marks</strong> and <strong>number of study hours</strong>.</p>
<ul class="simple">
<li><p>We want to predict the marks a student will score based on the number of hours they study.</p></li>
<li><p>If we plot <strong>study hours (independent variable)</strong> on the x-axis and <strong>percentage (dependent variable)</strong> on the y-axis, <strong>linear regression</strong> will give us a straight line that best fits the data points.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="understanding-the-mathematics-behind-it">
<h2>Understanding the Mathematics Behind It<a class="headerlink" href="#understanding-the-mathematics-behind-it" title="Link to this heading">#</a></h2>
<p>The equation of a straight line is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">mx</span> <span class="o">+</span> <span class="n">b</span>
</pre></div>
</div>
<p>Where:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">y</span></code> = Dependent variable (percentage)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">x</span></code> = Independent variable (hours studied)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">m</span></code> = Slope of the line (coefficient)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">b</span></code> = Intercept</p></li>
</ul>
<section id="how-linear-regression-works">
<h3>How Linear Regression Works:<a class="headerlink" href="#how-linear-regression-works" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The algorithm finds <strong>optimal values</strong> for the intercept (<code class="docutils literal notranslate"><span class="pre">b</span></code>) and slope (<code class="docutils literal notranslate"><span class="pre">m</span></code>).</p></li>
<li><p>There can be multiple possible straight lines depending on these values.</p></li>
<li><p>The <strong>best-fit line</strong> is the one that results in the <strong>least error</strong>.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="implementing-linear-regression-in-python">
<h2>Implementing Linear Regression in Python<a class="headerlink" href="#implementing-linear-regression-in-python" title="Link to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="c1"># Load the dataset</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;student_scores.csv&#39;</span><span class="p">)</span>

<span class="c1"># Plot the dataset</span>
<span class="n">df</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;Hours&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;Scores&#39;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>

<span class="c1"># Preparing the data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>  <span class="c1"># Independent variable (Hours)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>    <span class="c1"># Dependent variable (Scores)</span>

<span class="c1"># Splitting the data into training and testing sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Training the model</span>
<span class="n">regressor</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Print Intercept</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Intercept:&quot;</span><span class="p">,</span> <span class="n">regressor</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>

<span class="c1"># Print Coefficient</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coefficient:&quot;</span><span class="p">,</span> <span class="n">regressor</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="important-information">
<h2>Important Information<a class="headerlink" href="#important-information" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>The coefficient tells us how much the dependent variable (marks) changes when the independent variable (study hours) changes.</p></li>
<li><p><strong>Example Interpretation:</strong></p>
<ul>
<li><p>If the coefficient is <strong>9.91</strong>, it means for every <strong>1 additional hour</strong> of study, the student‚Äôs score increases by <strong>9.91%</strong>.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="making-predictions">
<h2>Making Predictions<a class="headerlink" href="#making-predictions" title="Link to this heading">#</a></h2>
<p>Now that our algorithm is trained, it‚Äôs time to make predictions.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Predicting the test set results</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
<section id="what-is-y-pred">
<h3>What is <code class="docutils literal notranslate"><span class="pre">y_pred</span></code>?<a class="headerlink" href="#what-is-y-pred" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">y_pred</span></code> is a <strong>NumPy array</strong> containing all the predicted values for the input values in <code class="docutils literal notranslate"><span class="pre">X_test</span></code>.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="comparing-actual-vs-predicted-values">
<h2>Comparing Actual vs. Predicted Values<a class="headerlink" href="#comparing-actual-vs-predicted-values" title="Link to this heading">#</a></h2>
<p>To check the accuracy of our model, we compare the <strong>actual</strong> values with the <strong>predicted</strong> values.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Creating a DataFrame to compare Actual vs Predicted values</span>
<span class="n">compare_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;Actual&#39;</span><span class="p">:</span> <span class="n">y_test</span><span class="p">,</span> <span class="s1">&#39;Predicted&#39;</span><span class="p">:</span> <span class="n">y_pred</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="n">compare_df</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Our model is <strong>not 100% precise</strong>, but the predicted values are <strong>close</strong> to the actual ones.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="evaluating-the-model">
<h2>Evaluating the Model<a class="headerlink" href="#evaluating-the-model" title="Link to this heading">#</a></h2>
<p>To measure the accuracy of our regression model, we use <strong>evaluation metrics</strong>.</p>
<section id="common-evaluation-metrics-for-regression">
<h3>Common Evaluation Metrics for Regression:<a class="headerlink" href="#common-evaluation-metrics-for-regression" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Mean Absolute Error (MAE)</strong> - Measures the average absolute difference between actual and predicted values.</p></li>
<li><p><strong>Mean Squared Error (MSE)</strong> - Measures the average squared difference between actual and predicted values.</p></li>
<li><p><strong>Root Mean Squared Error (RMSE)</strong> - Square root of MSE, helps in understanding the error in the same unit as the target variable.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn</span><span class="w"> </span><span class="kn">import</span> <span class="n">metrics</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="c1"># Calculate MAE</span>
<span class="n">mae</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean Absolute Error (MAE):&quot;</span><span class="p">,</span> <span class="n">mae</span><span class="p">)</span>

<span class="c1"># Calculate MSE</span>
<span class="n">mse</span> <span class="o">=</span> <span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean Squared Error (MSE):&quot;</span><span class="p">,</span> <span class="n">mse</span><span class="p">)</span>

<span class="c1"># Calculate RMSE</span>
<span class="n">rmse</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">metrics</span><span class="o">.</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Root Mean Squared Error (RMSE):&quot;</span><span class="p">,</span> <span class="n">rmse</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="id8">
<h2>Summary<a class="headerlink" href="#id8" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>We built a <strong>linear regression model</strong> to predict student marks based on study hours.</p></li>
<li><p>The <strong>coefficient</strong> shows that <strong>more study hours lead to better scores</strong>.</p></li>
<li><p>We <strong>evaluated</strong> the model using <strong>MAE, MSE, and RMSE</strong> to measure its accuracy.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="linear-regression-example-salary-prediction">
<h1>9 Linear Regression Example - Salary Prediction<a class="headerlink" href="#linear-regression-example-salary-prediction" title="Link to this heading">#</a></h1>
<section id="step-1-importing-required-libraries">
<h2>Step 1: Importing Required Libraries<a class="headerlink" href="#step-1-importing-required-libraries" title="Link to this heading">#</a></h2>
<p>Before we begin, let‚Äôs import the necessary libraries.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="step-2-loading-the-dataset">
<h2>Step 2: Loading the Dataset<a class="headerlink" href="#step-2-loading-the-dataset" title="Link to this heading">#</a></h2>
<p>We use the <code class="docutils literal notranslate"><span class="pre">Salary_Data.csv</span></code> file, which contains <strong>years of experience</strong> and <strong>corresponding salaries</strong>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;Salary_Data.csv&#39;</span><span class="p">)</span>

<span class="c1"># Splitting dataset into independent (X) and dependent (y) variables</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>  <span class="c1"># Years of experience (Independent variable)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>    <span class="c1"># Salary (Dependent variable)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="step-3-splitting-the-dataset">
<h2>Step 3: Splitting the Dataset<a class="headerlink" href="#step-3-splitting-the-dataset" title="Link to this heading">#</a></h2>
<p>We divide the dataset into a <strong>training set</strong> and a <strong>test set</strong>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Splitting the dataset (1/3 for testing, 2/3 for training)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="step-4-training-the-model">
<h2>Step 4: Training the Model<a class="headerlink" href="#step-4-training-the-model" title="Link to this heading">#</a></h2>
<p>We train a <strong>Simple Linear Regression</strong> model using the training dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Creating and training the model</span>
<span class="n">regressor</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="step-5-making-predictions">
<h2>Step 5: Making Predictions<a class="headerlink" href="#step-5-making-predictions" title="Link to this heading">#</a></h2>
<p>Now, we use our trained model to predict salaries for the test dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Predicting salaries for test data</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="step-6-visualizing-the-results">
<h2>Step 6: Visualizing the Results<a class="headerlink" href="#step-6-visualizing-the-results" title="Link to this heading">#</a></h2>
<section id="training-dataset-visualization">
<h3><strong>Training Dataset Visualization</strong><a class="headerlink" href="#training-dataset-visualization" title="Link to this heading">#</a></h3>
<p>We plot the actual <strong>training data points</strong> in red and the <strong>best-fit regression line</strong> in blue.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plotting the training set results</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Actual Data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Regression Line&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Years of Experience&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Salary&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Salary vs Experience (Training Set)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="test-dataset-visualization">
<h3><strong>Test Dataset Visualization</strong><a class="headerlink" href="#test-dataset-visualization" title="Link to this heading">#</a></h3>
<p>We now visualize the test dataset, keeping the regression line the same.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plotting the test set results</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Actual Data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">regressor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Regression Line&#39;</span><span class="p">)</span>  <span class="c1"># Same line</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Years of Experience&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Salary&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Salary vs Experience (Test Set)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="id9">
<h2>Summary<a class="headerlink" href="#id9" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>We built a <strong>Simple Linear Regression Model</strong> to predict <strong>salaries</strong> based on <strong>years of experience</strong>.</p></li>
<li><p>The model was trained using <strong>scikit-learn‚Äôs <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> class</strong>.</p></li>
<li><p>The <strong>best-fit line</strong> was plotted to visualize how the model generalizes over the data.</p></li>
<li><p>The trained model can now be used to predict salaries for <strong>new data points</strong>.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id10">
<h1>10. Multiple Linear Regression<a class="headerlink" href="#id10" title="Link to this heading">#</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>Multiple Linear Regression is used to <strong>explain the relationship</strong> between a <strong>dependent continuous variable</strong> (e.g., house price) and <strong>multiple independent variables</strong> (e.g., area, number of bedrooms, and age of the home).</p>
</section>
<hr class="docutils" />
<section id="problem-statement">
<h2>Problem Statement<a class="headerlink" href="#problem-statement" title="Link to this heading">#</a></h2>
<p>We are planning to buy a new house and need to <strong>predict its price</strong> based on:</p>
<ul class="simple">
<li><p><strong>Area</strong> (in square feet)</p></li>
<li><p><strong>Number of bedrooms</strong></p></li>
<li><p><strong>Age of the home</strong> (in years)</p></li>
</ul>
<section id="given-the-following-home-details-we-need-to-predict-their-prices">
<h3>Given the following home details, we need to predict their prices:<a class="headerlink" href="#given-the-following-home-details-we-need-to-predict-their-prices" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>3000 sq ft, 3 bedrooms, 40 years old</strong></p></li>
<li><p><strong>2500 sq ft, 4 bedrooms, 5 years old</strong></p></li>
</ol>
</section>
</section>
<hr class="docutils" />
<section id="dataset">
<h2>Dataset<a class="headerlink" href="#dataset" title="Link to this heading">#</a></h2>
<p>We will use the dataset <strong><code class="docutils literal notranslate"><span class="pre">homeprices1.csv</span></code></strong>, which contains the following columns:</p>
<ul class="simple">
<li><p><strong>Area</strong> (square feet)</p></li>
<li><p><strong>Bedrooms</strong> (number of rooms)</p></li>
<li><p><strong>Age</strong> (years)</p></li>
<li><p><strong>Price</strong> (dependent variable)</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="formula-for-multiple-linear-regression">
<h2>Formula for Multiple Linear Regression<a class="headerlink" href="#formula-for-multiple-linear-regression" title="Link to this heading">#</a></h2>
<p>The mathematical equation for multiple linear regression is:</p>
<p>[
Y = M_1X_1 + M_2X_2 + M_3X_3 + b
]</p>
<p>For this problem:</p>
<p>[
\text{Price} = M_1 \times \text{Area} + M_2 \times \text{Bedrooms} + M_3 \times \text{Age} + b
]</p>
</section>
<hr class="docutils" />
<section id="step-by-step-implementation">
<h2>Step-by-Step Implementation<a class="headerlink" href="#step-by-step-implementation" title="Link to this heading">#</a></h2>
<section id="step-1-import-required-libraries">
<h3><strong>Step 1: Import Required Libraries</strong><a class="headerlink" href="#step-1-import-required-libraries" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="step-2-load-the-dataset">
<h3><strong>Step 2: Load the Dataset</strong><a class="headerlink" href="#step-2-load-the-dataset" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the dataset</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;homeprices1.csv&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="step-3-handling-missing-values">
<h3><strong>Step 3: Handling Missing Values</strong><a class="headerlink" href="#step-3-handling-missing-values" title="Link to this heading">#</a></h3>
<p>If the <strong>bedrooms</strong> column has missing values, we will replace them with the <strong>median</strong> value.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Fill missing bedroom values with median</span>
<span class="n">df</span><span class="o">.</span><span class="n">bedrooms</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">bedrooms</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">bedrooms</span><span class="o">.</span><span class="n">median</span><span class="p">())</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="step-4-preparing-data-for-model-training">
<h3><strong>Step 4: Preparing Data for Model Training</strong><a class="headerlink" href="#step-4-preparing-data-for-model-training" title="Link to this heading">#</a></h3>
<p>We separate the <strong>independent variables</strong> (<code class="docutils literal notranslate"><span class="pre">Area</span></code>, <code class="docutils literal notranslate"><span class="pre">Bedrooms</span></code>, <code class="docutils literal notranslate"><span class="pre">Age</span></code>) from the <strong>dependent variable</strong> (<code class="docutils literal notranslate"><span class="pre">Price</span></code>).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Dropping the price column to create independent variables dataset</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;columns&#39;</span><span class="p">)</span>

<span class="c1"># Target variable</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;price&#39;</span><span class="p">]</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="step-5-train-the-model">
<h3><strong>Step 5: Train the Model</strong><a class="headerlink" href="#step-5-train-the-model" title="Link to this heading">#</a></h3>
<p>We use <strong>scikit-learn‚Äôs</strong> <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> to train our model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create and train the model</span>
<span class="n">reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="step-6-retrieve-model-coefficients">
<h3><strong>Step 6: Retrieve Model Coefficients</strong><a class="headerlink" href="#step-6-retrieve-model-coefficients" title="Link to this heading">#</a></h3>
<p>The model calculates the <strong>intercept</strong> and <strong>coefficients</strong>, which represent the relationship between features and the target variable.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get the intercept</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Intercept:&quot;</span><span class="p">,</span> <span class="n">reg</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>

<span class="c1"># Get the coefficients</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coefficients:&quot;</span><span class="p">,</span> <span class="n">reg</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>
</pre></div>
</div>
<p>The <strong>coefficients</strong> determine how much the price will change for a <strong>unit increase</strong> in each independent variable.</p>
</section>
</section>
<hr class="docutils" />
<section id="id11">
<h2>Summary<a class="headerlink" href="#id11" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>We implemented Multiple Linear Regression to predict house prices.</strong></p></li>
<li><p><strong>The model was trained on real estate data (<code class="docutils literal notranslate"><span class="pre">homeprices1.csv</span></code>).</strong></p></li>
<li><p><strong>We handled missing values by filling them with the median.</strong></p></li>
<li><p><strong>We extracted the model‚Äôs intercept and coefficients to understand the relationships between features.</strong></p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="polynomial-features-in-machine-learning">
<h1>11. Polynomial Features in Machine Learning<a class="headerlink" href="#polynomial-features-in-machine-learning" title="Link to this heading">#</a></h1>
<section id="id12">
<h2>Introduction<a class="headerlink" href="#id12" title="Link to this heading">#</a></h2>
<p>Polynomial Features are a type of <strong>feature engineering</strong> that creates <strong>new input features</strong> based on existing ones. This transformation is useful when dealing with <strong>non-linear datasets</strong>, as it allows a linear model to <strong>capture non-linear relationships</strong>.</p>
<section id="what-are-polynomial-features">
<h3><strong>What Are Polynomial Features?</strong><a class="headerlink" href="#what-are-polynomial-features" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>If a dataset has one input feature <code class="docutils literal notranslate"><span class="pre">X</span></code>, a <strong>polynomial feature</strong> is created by adding a new feature <code class="docutils literal notranslate"><span class="pre">X¬≤</span></code>, <code class="docutils literal notranslate"><span class="pre">X¬≥</span></code>, etc.</p></li>
<li><p>This transformation is done <strong>for each input variable</strong>, expanding the dataset.</p></li>
<li><p>The <strong>degree</strong> of the polynomial controls the number of features added.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="why-do-we-need-polynomial-features">
<h2><strong>Why Do We Need Polynomial Features?</strong><a class="headerlink" href="#why-do-we-need-polynomial-features" title="Link to this heading">#</a></h2>
<section id="linear-models-vs-non-linear-data">
<h3><strong>Linear Models vs. Non-Linear Data</strong><a class="headerlink" href="#linear-models-vs-non-linear-data" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>A <strong>linear model</strong> works well if the dataset has a <strong>linear relationship</strong> (as seen in Simple Linear Regression).</p></li>
<li><p>However, if the dataset is <strong>non-linear</strong>, using a linear model <strong>without modification</strong> will produce <strong>poor results</strong>.</p></li>
<li><p>This leads to <strong>high error rates</strong> and inaccurate predictions.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="mathematical-equations">
<h2><strong>Mathematical Equations</strong><a class="headerlink" href="#mathematical-equations" title="Link to this heading">#</a></h2>
<section id="id13">
<h3><strong>Simple Linear Regression</strong><a class="headerlink" href="#id13" title="Link to this heading">#</a></h3>
<p>[
y = b_0 + b_1x
]</p>
</section>
<section id="id14">
<h3><strong>Multiple Linear Regression</strong><a class="headerlink" href="#id14" title="Link to this heading">#</a></h3>
<p>[
y = b_0 + b_1x + b_2x^2 + b_3x^3 + \dots + b_nx^n
]</p>
</section>
</section>
<hr class="docutils" />
<section id="id15">
<h2><strong>Step-by-Step Implementation</strong><a class="headerlink" href="#id15" title="Link to this heading">#</a></h2>
<section id="id16">
<h3><strong>Step 1: Import Required Libraries</strong><a class="headerlink" href="#id16" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">PolynomialFeatures</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="id17">
<h3><strong>Step 2: Load the Dataset</strong><a class="headerlink" href="#id17" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the dataset</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;poly_dataset.csv&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="step-3-data-preparation">
<h3><strong>Step 3: Data Preparation</strong><a class="headerlink" href="#step-3-data-preparation" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Extracting independent and dependent variables</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>  <span class="c1"># Selecting feature column</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>    <span class="c1"># Target variable</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="step-4-visualizing-the-data">
<h3><strong>Step 4: Visualizing the Data</strong><a class="headerlink" href="#step-4-visualizing-the-data" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Scatter plot to visualize data distribution</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Feature X&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Target y&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Data Distribution&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="step-5-training-a-linear-regression-model">
<h3><strong>Step 5: Training a Linear Regression Model</strong><a class="headerlink" href="#step-5-training-a-linear-regression-model" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train a simple linear regression model</span>
<span class="n">lin_reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lin_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Plot the Linear Regression model</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">lin_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Linear Regression Fit&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="step-6-applying-polynomial-features">
<h3><strong>Step 6: Applying Polynomial Features</strong><a class="headerlink" href="#step-6-applying-polynomial-features" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Create polynomial features with degree 2</span>
<span class="n">poly_reg</span> <span class="o">=</span> <span class="n">PolynomialFeatures</span><span class="p">(</span><span class="n">degree</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X_poly</span> <span class="o">=</span> <span class="n">poly_reg</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Train a polynomial regression model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_poly</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="step-7-plotting-polynomial-regression">
<h3><strong>Step 7: Plotting Polynomial Regression</strong><a class="headerlink" href="#step-7-plotting-polynomial-regression" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot Polynomial Regression results</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_poly</span><span class="p">),</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Polynomial Regression Fit&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="predictions">
<h2><strong>Predictions</strong><a class="headerlink" href="#predictions" title="Link to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Predict output using Linear Regression</span>
<span class="n">linear_pred</span> <span class="o">=</span> <span class="n">lin_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">330</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Linear Regression Predicted Output:&quot;</span><span class="p">,</span> <span class="n">linear_pred</span><span class="p">)</span>

<span class="c1"># Predict output using Polynomial Regression</span>
<span class="n">poly_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">poly_reg</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">([[</span><span class="mi">330</span><span class="p">]]))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Polynomial Regression Predicted Output:&quot;</span><span class="p">,</span> <span class="n">poly_pred</span><span class="p">)</span>
</pre></div>
</div>
</section>
<hr class="docutils" />
<section id="id18">
<h2><strong>Conclusion</strong><a class="headerlink" href="#id18" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Linear Regression predicted output:</strong> <code class="docutils literal notranslate"><span class="pre">[330378.78787879]</span></code></p></li>
<li><p><strong>Polynomial Regression predicted output:</strong> <code class="docutils literal notranslate"><span class="pre">[158862.45265155]</span></code></p></li>
<li><p><strong>Polynomial Regression provides a more accurate prediction</strong> in cases where the dataset exhibits <strong>non-linearity</strong>.</p></li>
</ul>
<p>üìå <strong>Key Takeaway:</strong> Polynomial Regression is a <strong>powerful extension of Linear Regression</strong> that allows models to fit <strong>non-linear data patterns</strong> more effectively.</p>
</section>
</section>
<hr class="docutils" />
<section id="cost-functions-in-machine-learning">
<h1>12. Cost Functions in Machine Learning<a class="headerlink" href="#cost-functions-in-machine-learning" title="Link to this heading">#</a></h1>
<section id="id19">
<h2><strong>Introduction</strong><a class="headerlink" href="#id19" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Cost Functions</strong> (also called <strong>Loss Functions</strong>) are <strong>optimization techniques</strong> used in machine learning.</p></li>
<li><p>They measure the difference between the <strong>actual output</strong> and the <strong>predicted output</strong> of a model.</p></li>
<li><p>Different cost functions exist for <strong>regression</strong> and <strong>classification</strong> problems.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="why-do-we-need-cost-functions">
<h2><strong>Why Do We Need Cost Functions?</strong><a class="headerlink" href="#why-do-we-need-cost-functions" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>During training, the model initializes weights <strong>randomly</strong> and makes predictions on the training data.</p></li>
<li><p>The model needs a way to measure <strong>how far</strong> its predictions are from the actual values.</p></li>
<li><p><strong>Cost functions provide this information</strong>, enabling the model to <strong>adjust weights</strong> using optimization techniques like <strong>gradient descent</strong>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="how-does-a-cost-function-work">
<h2><strong>How Does a Cost Function Work?</strong><a class="headerlink" href="#how-does-a-cost-function-work" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>The <strong>cost function</strong> takes <strong>predicted outputs</strong> and <strong>actual outputs</strong> as input.</p></li>
<li><p>It calculates the <strong>error</strong> (difference between the predicted and actual values).</p></li>
<li><p>The model <strong>adjusts its weights</strong> in the next iteration to <strong>minimize this error</strong>.</p></li>
<li><p>This process continues until the <strong>optimal model weights</strong> are found.</p></li>
</ol>
</section>
<hr class="docutils" />
<section id="goal-of-training">
<h2><strong>Goal of Training</strong><a class="headerlink" href="#goal-of-training" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>The primary goal of training is to <strong>find the optimal weights</strong> that <strong>minimize error</strong> in every iteration.</p></li>
<li><p>This optimization ensures that the model <strong>learns effectively</strong> and makes <strong>accurate predictions</strong>.</p></li>
</ul>
<p>üìå <strong>Key Takeaway:</strong> Cost functions are <strong>crucial</strong> for training machine learning models, as they guide the model in <strong>minimizing error</strong> and improving accuracy.</p>
</section>
</section>
<hr class="docutils" />
<section id="cost-functions-for-regression">
<h1>13. <strong>Cost Functions for Regression</strong><a class="headerlink" href="#cost-functions-for-regression" title="Link to this heading">#</a></h1>
<section id="id20">
<h2><strong>Introduction</strong><a class="headerlink" href="#id20" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>In regression, the model predicts an output value for each training data point.</p></li>
<li><p>Cost functions for regression are calculated based on <strong>distance-based error</strong>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="types-of-regression-metrics">
<h2><strong>Types of Regression Metrics</strong><a class="headerlink" href="#types-of-regression-metrics" title="Link to this heading">#</a></h2>
<p>There are three common regression metrics used to evaluate model performance:</p>
<ol class="arabic simple">
<li><p><strong>Mean Squared Error (MSE)</strong></p></li>
<li><p><strong>Root Mean Squared Error (RMSE)</strong></p></li>
<li><p><strong>Mean Absolute Error (MAE)</strong></p></li>
</ol>
</section>
<hr class="docutils" />
<section id="distance-based-error">
<h2><strong>Distance-Based Error</strong><a class="headerlink" href="#distance-based-error" title="Link to this heading">#</a></h2>
<ul>
<li><p>Given an actual output <code class="docutils literal notranslate"><span class="pre">y</span></code> and a predicted output <code class="docutils literal notranslate"><span class="pre">y'</span></code>, the <strong>error</strong> is calculated as:</p>
<p>[
\text{Error} = y - y‚Äô
]</p>
</li>
<li><p>This error, known as <strong>distance-based error</strong>, forms the basis of cost functions in regression models.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="mean-squared-error-mse">
<h2><strong>1. Mean Squared Error (MSE)</strong><a class="headerlink" href="#mean-squared-error-mse" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>MSE is the <strong>mean of the squared differences</strong> between predicted and actual target values.</p></li>
<li><p>It <strong>penalizes larger errors more heavily</strong> because of squaring.</p></li>
<li><p>MSE <strong>never</strong> takes a negative value.</p></li>
</ul>
<section id="implementation">
<h3><strong>Implementation:</strong><a class="headerlink" href="#implementation" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="n">expected</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]</span>
<span class="n">predicted</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">,</span> <span class="mf">0.6</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">]</span>

<span class="n">mse_value</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mse_value</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="root-mean-squared-error-rmse">
<h2><strong>2. Root Mean Squared Error (RMSE)</strong><a class="headerlink" href="#root-mean-squared-error-rmse" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>RMSE is the <strong>square root</strong> of MSE.</p></li>
<li><p>It is useful when we want to <strong>report errors in the same unit as the target variable</strong>.</p></li>
</ul>
<section id="formula">
<h3><strong>Formula:</strong><a class="headerlink" href="#formula" title="Link to this heading">#</a></h3>
<p>[
\text{RMSE} = \sqrt{\text{MSE}}
]</p>
</section>
<section id="id21">
<h3><strong>Implementation:</strong><a class="headerlink" href="#id21" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_squared_error</span>

<span class="n">rmse_value</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">predicted</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rmse_value</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="mean-absolute-error-mae">
<h2><strong>3. Mean Absolute Error (MAE)</strong><a class="headerlink" href="#mean-absolute-error-mae" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>MAE calculates the <strong>average absolute difference</strong> between predicted and actual values.</p></li>
<li><p>Unlike MSE, MAE <strong>treats all errors equally</strong> and is <strong>less sensitive to outliers</strong>.</p></li>
</ul>
<section id="id22">
<h3><strong>Implementation:</strong><a class="headerlink" href="#id22" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">mean_absolute_error</span>

<span class="n">mae_value</span> <span class="o">=</span> <span class="n">mean_absolute_error</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">predicted</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mae_value</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="choosing-the-right-regression-metric">
<h2><strong>Choosing the Right Regression Metric</strong><a class="headerlink" href="#choosing-the-right-regression-metric" title="Link to this heading">#</a></h2>
<p>The choice of regression metric depends on the <strong>problem and data characteristics</strong>:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>When to Use?</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>MSE</strong></p></td>
<td><p>When you want to <strong>penalize large errors</strong> more. However, it is <strong>sensitive to outliers</strong>.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>RMSE</strong></p></td>
<td><p>When comparing model performance or when you want an <strong>interpretable error in the same unit</strong> as the target variable.</p></td>
</tr>
<tr class="row-even"><td><p><strong>MAE</strong></p></td>
<td><p>When you want to <strong>treat all errors equally</strong>, regardless of magnitude. It is <strong>less sensitive to outliers</strong>.</p></td>
</tr>
</tbody>
</table>
</div>
<p>üìå <strong>Key Takeaway:</strong></p>
<ul class="simple">
<li><p>If you want to <strong>heavily penalize larger errors</strong>, use <strong>MSE</strong>.</p></li>
<li><p>If you need a <strong>more interpretable metric</strong>, use <strong>RMSE</strong>.</p></li>
<li><p>If your data has <strong>outliers</strong>, <strong>MAE</strong> is a better choice.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="handling-categorical-data-dummy-variables-one-hot-encoding">
<h1>14. <strong>Handling Categorical Data: Dummy Variables &amp; One-Hot Encoding</strong><a class="headerlink" href="#handling-categorical-data-dummy-variables-one-hot-encoding" title="Link to this heading">#</a></h1>
<section id="id23">
<h2><strong>Introduction</strong><a class="headerlink" href="#id23" title="Link to this heading">#</a></h2>
<p>When dealing with <strong>categorical data</strong>, such as the ‚Äútown‚Äù column in a dataset, we need to convert text-based data into numerical values so that machine learning models can process them.</p>
</section>
<section id="why-can-t-we-directly-convert-text-to-numbers">
<h2><strong>Why Can‚Äôt We Directly Convert Text to Numbers?</strong><a class="headerlink" href="#why-can-t-we-directly-convert-text-to-numbers" title="Link to this heading">#</a></h2>
<p>If we assign arbitrary numerical values to categories, the model may assume an <strong>incorrect ordinal relationship</strong> between them.</p>
<p>For example:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Town</p></th>
<th class="head"><p>Assigned Number</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Vijayawada</p></td>
<td><p>1</p></td>
</tr>
<tr class="row-odd"><td><p>Guntur</p></td>
<td><p>2</p></td>
</tr>
<tr class="row-even"><td><p>Gudiwada</p></td>
<td><p>3</p></td>
</tr>
</tbody>
</table>
</div>
<p>If we feed this to a model, it may incorrectly assume:</p>
<ul class="simple">
<li><p><strong>Vijayawada &lt; Guntur &lt; Gudiwada</strong>, or</p></li>
<li><p><strong>Vijayawada + Guntur = Gudiwada</strong>, which is incorrect.</p></li>
</ul>
<p>To handle this, we use <strong>dummy variables</strong> or <strong>One-Hot Encoding</strong>.</p>
</section>
<hr class="docutils" />
<section id="using-dummy-variables">
<h2><strong>Using Dummy Variables</strong><a class="headerlink" href="#using-dummy-variables" title="Link to this heading">#</a></h2>
<p>Dummy variables create separate binary columns for each category in the ‚Äútown‚Äù column.</p>
<section id="id24">
<h3><strong>Implementation:</strong><a class="headerlink" href="#id24" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="c1"># Loading dataset</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;homeprices2.csv&quot;</span><span class="p">)</span>

<span class="c1"># Creating dummy variables</span>
<span class="n">dummies</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">town</span><span class="p">)</span>

<span class="c1"># Merging the dummy variables with the dataset</span>
<span class="n">merged</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">df</span><span class="p">,</span> <span class="n">dummies</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;columns&#39;</span><span class="p">)</span>

<span class="c1"># Dropping the original categorical column</span>
<span class="n">final</span> <span class="o">=</span> <span class="n">merged</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;town&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;columns&#39;</span><span class="p">)</span>

<span class="c1"># Splitting into features and target variable</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">final</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;price&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;columns&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">final</span><span class="o">.</span><span class="n">price</span>

<span class="c1"># Training the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Predicting prices for the dataset</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

<span class="c1"># Checking the model score</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model Accuracy:&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="id25">
<h2><strong>Making Predictions</strong><a class="headerlink" href="#id25" title="Link to this heading">#</a></h2>
<p>We can now predict house prices based on location:</p>
<section id="predicting-house-price-in-vijayawada">
<h3><strong>Predicting House Price in Vijayawada</strong><a class="headerlink" href="#predicting-house-price-in-vijayawada" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">3400</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]))</span>
</pre></div>
</div>
</section>
<section id="predicting-house-price-in-guntur">
<h3><strong>Predicting House Price in Guntur</strong><a class="headerlink" href="#predicting-house-price-in-guntur" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">3400</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
</pre></div>
</div>
</section>
<section id="predicting-house-price-in-gudiwada">
<h3><strong>Predicting House Price in Gudiwada</strong><a class="headerlink" href="#predicting-house-price-in-gudiwada" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">3400</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="id26">
<h2><strong>Conclusion</strong><a class="headerlink" href="#id26" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Dummy variables</strong> help us properly encode categorical data <strong>without introducing incorrect relationships</strong>.</p></li>
<li><p><strong>One-Hot Encoding</strong> is another method that works similarly but is often automated in libraries like <code class="docutils literal notranslate"><span class="pre">sklearn.preprocessing.OneHotEncoder</span></code>.</p></li>
<li><p>Always be cautious when encoding categorical variables to <strong>avoid misleading the model</strong>.</p></li>
</ul>
<p>üìå <strong>Key Takeaway:</strong><br />
Dummy variables ensure that our model interprets categories <strong>correctly and without bias</strong> by creating independent binary columns instead of using arbitrary numbers.</p>
</section>
</section>
<hr class="docutils" />
<section id="gradient-descent-in-machine-learning">
<h1>15. <strong>Gradient Descent in Machine Learning</strong><a class="headerlink" href="#gradient-descent-in-machine-learning" title="Link to this heading">#</a></h1>
<section id="id27">
<h2><strong>Introduction</strong><a class="headerlink" href="#id27" title="Link to this heading">#</a></h2>
<p>Gradient Descent is an <strong>optimization algorithm</strong> used to find the values of parameters (coefficients) that <strong>minimize a cost function</strong> in machine learning models.</p>
</section>
<hr class="docutils" />
<section id="how-gradient-descent-works">
<h2><strong>How Gradient Descent Works</strong><a class="headerlink" href="#how-gradient-descent-works" title="Link to this heading">#</a></h2>
<p>Gradient Descent follows an iterative process to optimize the parameters:</p>
<ol class="arabic simple">
<li><p><strong>Initialize</strong> random values for the parameters.</p></li>
<li><p><strong>Predict</strong> the target variable using the current parameters.</p></li>
<li><p><strong>Calculate</strong> the cost (error) associated with the prediction.</p></li>
<li><p><strong>Check if cost is minimized</strong>:</p>
<ul class="simple">
<li><p><strong>If yes</strong>, proceed to step 6.</p></li>
<li><p><strong>If no</strong>, move to step 5.</p></li>
</ul>
</li>
<li><p><strong>Update</strong> the parameter values using the gradient descent formula and go back to step 2.</p></li>
<li><p><strong>Repeat</strong> until the model reaches optimal parameter values.</p></li>
<li><p><strong>Final updated parameters</strong> are obtained, and the model is ready for use.</p></li>
</ol>
</section>
<hr class="docutils" />
<section id="understanding-convergence">
<h2><strong>Understanding Convergence</strong><a class="headerlink" href="#understanding-convergence" title="Link to this heading">#</a></h2>
<p><strong>Convergence</strong> occurs when gradient descent <strong>makes very small updates</strong> to the objective function, indicating it is approaching an optimal solution.<br />
However, reaching convergence <strong>does not always guarantee the absolute best solution</strong>‚Äîit means the algorithm is close to the minimum.</p>
</section>
<hr class="docutils" />
<section id="process-behind-gradient-descent">
<h2><strong>Process Behind Gradient Descent</strong><a class="headerlink" href="#process-behind-gradient-descent" title="Link to this heading">#</a></h2>
<p>Gradient Descent finds the <strong>best-fit line</strong> for a given training dataset by optimizing parameters iteratively.</p>
<section id="visualizing-the-gradient-descent-process">
<h3><strong>Visualizing the Gradient Descent Process</strong><a class="headerlink" href="#visualizing-the-gradient-descent-process" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Consider a scenario where we are optimizing parameters <code class="docutils literal notranslate"><span class="pre">m</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code> for a regression model.</p></li>
<li><p>We calculate the <strong>Mean Squared Error (MSE)</strong> for different values of <code class="docutils literal notranslate"><span class="pre">m</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code>.</p></li>
<li><p>If we plot <code class="docutils literal notranslate"><span class="pre">m</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code> against <strong>MSE</strong>, we get a <strong>bowl-shaped curve</strong> (convex function).</p></li>
<li><p>Our goal is to <strong>start at an initial value</strong> and move <strong>downward</strong> toward the lowest point (minima).</p></li>
</ul>
</section>
<section id="steps-in-gradient-descent">
<h3><strong>Steps in Gradient Descent</strong><a class="headerlink" href="#steps-in-gradient-descent" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Start with <strong>m = 0</strong> and <strong>b = 0</strong>.</p></li>
<li><p>Compute the <strong>initial MSE</strong> (assume it is 1000).</p></li>
<li><p>Adjust the values of <code class="docutils literal notranslate"><span class="pre">m</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code> slightly.</p></li>
<li><p>Check the new <strong>MSE</strong>‚Äîit should decrease.</p></li>
<li><p>Repeat this process until the error reaches a <strong>minimum value (minima)</strong>.</p></li>
<li><p>The <strong>final m and b values</strong> are used for making predictions.</p></li>
</ol>
</section>
</section>
<hr class="docutils" />
<section id="types-of-gradient-descent-approaches">
<h2><strong>Types of Gradient Descent Approaches</strong><a class="headerlink" href="#types-of-gradient-descent-approaches" title="Link to this heading">#</a></h2>
<section id="fixed-step-size-approach">
<h3><strong>Fixed Step Size Approach</strong><a class="headerlink" href="#fixed-step-size-approach" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>If we use a <strong>fixed step size</strong>, we might <strong>overshoot or miss the global minimum</strong>.</p></li>
<li><p>This approach is <strong>not ideal</strong> for convergence.</p></li>
</ul>
</section>
<section id="learning-rate-reaching-minimum-error">
<h3><strong>Learning Rate: Reaching Minimum Error</strong><a class="headerlink" href="#learning-rate-reaching-minimum-error" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The <strong>learning rate (Œ±)</strong> controls the <strong>step size</strong> taken towards minimizing the cost function.</p></li>
<li><p>A well-tuned learning rate helps in reaching the <strong>optimal point efficiently</strong>.</p></li>
</ul>
</section>
<section id="effect-of-small-large-learning-rates">
<h3><strong>Effect of Small &amp; Large Learning Rates</strong><a class="headerlink" href="#effect-of-small-large-learning-rates" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Small Learning Rate</strong> ‚Üí Model takes a long time to converge.</p></li>
<li><p><strong>Large Learning Rate</strong> ‚Üí Model overshoots the minima and fails to converge.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="id28">
<h2><strong>Conclusion</strong><a class="headerlink" href="#id28" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Gradient Descent is crucial</strong> for optimizing machine learning models.</p></li>
<li><p>Choosing the <strong>right learning rate</strong> is essential for <strong>efficient training</strong>.</p></li>
<li><p><strong>Understanding convergence</strong> helps prevent <strong>overfitting or underfitting</strong>.</p></li>
</ul>
<p>üìå <strong>Key Takeaway:</strong><br />
Gradient Descent iteratively <strong>tunes model parameters</strong> to <strong>minimize error</strong>, but <strong>choosing an appropriate learning rate</strong> is critical for successful optimization.</p>
</section>
</section>
<hr class="docutils" />
<section id="logistic-regression-in-machine-learning">
<h1>16. <strong>Logistic Regression in Machine Learning</strong><a class="headerlink" href="#logistic-regression-in-machine-learning" title="Link to this heading">#</a></h1>
<section id="id29">
<h2><strong>Introduction</strong><a class="headerlink" href="#id29" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Logistic Regression is a <strong>supervised learning</strong> technique used for <strong>classification problems</strong>.</p></li>
<li><p>It predicts <strong>categorical dependent variables</strong> based on independent variables.</p></li>
<li><p><strong>Examples of Logistic Regression Applications</strong>:</p>
<ul>
<li><p><strong>Email Classification</strong> ‚Üí Spam or Not Spam</p></li>
<li><p><strong>Customer Behavior</strong> ‚Üí Will buy a product or not</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="types-of-logistic-regression">
<h2><strong>Types of Logistic Regression</strong><a class="headerlink" href="#types-of-logistic-regression" title="Link to this heading">#</a></h2>
<section id="binary-classification">
<h3><strong>1. Binary Classification</strong><a class="headerlink" href="#binary-classification" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The target variable has only <strong>two possible outcomes</strong>:</p>
<ul>
<li><p><strong>0 or 1</strong></p></li>
<li><p><strong>Pass or Fail</strong></p></li>
<li><p><strong>Yes or No</strong></p></li>
</ul>
</li>
</ul>
</section>
<section id="multiclass-classification">
<h3><strong>2. Multiclass Classification</strong><a class="headerlink" href="#multiclass-classification" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The target variable has <strong>three or more unordered categories</strong>:</p>
<ul>
<li><p><strong>Ok, Good, Best</strong></p></li>
<li><p><strong>Cat, Dog, Sheep</strong>, etc.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="understanding-the-problem">
<h2><strong>Understanding the Problem</strong><a class="headerlink" href="#understanding-the-problem" title="Link to this heading">#</a></h2>
<section id="dataset-overview">
<h3><strong>Dataset Overview</strong><a class="headerlink" href="#dataset-overview" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>We use an <strong>insurance dataset</strong> where:</p>
<ul>
<li><p><strong>0</strong> ‚Üí Person <strong>did not buy</strong> insurance.</p></li>
<li><p><strong>1</strong> ‚Üí Person <strong>bought</strong> insurance.</p></li>
</ul>
</li>
<li><p>Observations:</p>
<ul>
<li><p><strong>Younger people</strong> tend <strong>not</strong> to buy insurance.</p></li>
<li><p><strong>Older people</strong> are <strong>more likely</strong> to buy insurance.</p></li>
</ul>
</li>
</ul>
</section>
<section id="id30">
<h3><strong>Problem Statement</strong><a class="headerlink" href="#id30" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Based on a person‚Äôs <strong>age</strong>, predict <strong>whether they will buy insurance or not</strong>.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="logistic-function-sigmoid">
<h2><strong>Logistic Function (Sigmoid)</strong><a class="headerlink" href="#logistic-function-sigmoid" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Logistic Regression</strong> uses the <strong>sigmoid function</strong> to map real values into a range of <strong>0 to 1</strong>.</p></li>
<li><p><strong>Threshold Concept</strong>:</p>
<ul>
<li><p><strong>If output ‚â• 0.5</strong>, classify as <strong>1</strong> (Will buy insurance).</p></li>
<li><p><strong>If output &lt; 0.5</strong>, classify as <strong>0</strong> (Will not buy insurance).</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="implementation-in-python">
<h2><strong>Implementation in Python</strong><a class="headerlink" href="#implementation-in-python" title="Link to this heading">#</a></h2>
<section id="load-the-dataset">
<h3><strong>1. Load the Dataset</strong><a class="headerlink" href="#load-the-dataset" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="c1"># Load the dataset</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;insurance_data.csv&quot;</span><span class="p">)</span>

<span class="c1"># Plot the data</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">age</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">bought_insurance</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;Age&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;Bought Insurance (0 = No, 1 = Yes)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="splitting-the-data">
<h3><strong>2. Splitting the Data</strong><a class="headerlink" href="#splitting-the-data" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s1">&#39;age&#39;</span><span class="p">]]</span>  <span class="c1"># Independent variable</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;bought_insurance&#39;</span><span class="p">]</span>  <span class="c1"># Dependent variable</span>

<span class="c1"># Splitting into training and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">52</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="training-the-model">
<h3><strong>3. Training the Model</strong><a class="headerlink" href="#training-the-model" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Train the Logistic Regression model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id31">
<h3><strong>4. Making Predictions</strong><a class="headerlink" href="#id31" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Predict for new ages</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">50</span><span class="p">]]))</span>  <span class="c1"># Predict for age 50</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">25</span><span class="p">]]))</span>  <span class="c1"># Predict for age 25</span>

<span class="c1"># Predict on test data</span>
<span class="n">y_predicted</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="model-performance">
<h3><strong>5. Model Performance</strong><a class="headerlink" href="#model-performance" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Model accuracy score</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model Accuracy:&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>

<span class="c1"># Prediction probabilities</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prediction Probabilities:</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="prediction-insights">
<h2><strong>Prediction Insights</strong><a class="headerlink" href="#prediction-insights" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>A 19-year-old</strong> is predicted <strong>not</strong> to buy insurance.</p></li>
<li><p><strong>People aged 60 and 61</strong> are predicted <strong>to buy insurance</strong>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id32">
<h2><strong>Conclusion</strong><a class="headerlink" href="#id32" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Logistic Regression</strong> is effective for <strong>binary and multiclass classification</strong>.</p></li>
<li><p><strong>Sigmoid function</strong> ensures predictions fall within a <strong>0 to 1 range</strong>.</p></li>
<li><p><strong>Thresholding helps in classification</strong>:</p>
<ul>
<li><p><strong>‚â• 0.5 ‚Üí Positive Class (1)</strong></p></li>
<li><p><strong>&lt; 0.5 ‚Üí Negative Class (0)</strong></p></li>
</ul>
</li>
<li><p><strong>Performance evaluation</strong> using <strong>accuracy score and probability predictions</strong> helps assess model effectiveness.</p></li>
</ul>
<p>üìå <strong>Key Takeaway:</strong> Logistic Regression is a fundamental classification algorithm that predicts outcomes based on probability, making it widely used in machine learning applications.</p>
</section>
</section>
<hr class="docutils" />
<section id="logistic-regression-multiclass-classification">
<h1><strong>Logistic Regression - Multiclass Classification</strong><a class="headerlink" href="#logistic-regression-multiclass-classification" title="Link to this heading">#</a></h1>
<section id="id33">
<h2><strong>Introduction</strong><a class="headerlink" href="#id33" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Logistic Regression is a <strong>supervised learning</strong> algorithm used for <strong>classification problems</strong>.</p></li>
<li><p>It predicts a <strong>categorical dependent variable</strong> based on independent variables.</p></li>
<li><p><strong>Multiclass Classification</strong> extends Logistic Regression to handle <strong>more than two classes</strong>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="practical-example-handwritten-digit-recognition">
<h2><strong>Practical Example: Handwritten Digit Recognition</strong><a class="headerlink" href="#practical-example-handwritten-digit-recognition" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>The goal is to recognize <strong>handwritten digits</strong> from <strong>0 to 9</strong>.</p></li>
<li><p>Example:</p>
<ul>
<li><p>If the image contains the digit <strong>0</strong>, the output should be <strong>0</strong>.</p></li>
<li><p>If the image contains the digit <strong>1</strong>, the output should be <strong>1</strong>.</p></li>
<li><p>If the image contains the digit <strong>2</strong>, the output should be <strong>2</strong>.</p></li>
<li><p>‚Ä¶</p></li>
<li><p>If the image contains the digit <strong>9</strong>, the output should be <strong>9</strong>.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="id34">
<h2><strong>Problem Statement</strong><a class="headerlink" href="#id34" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Recognizing handwritten digits</strong> from images using <strong>Logistic Regression</strong>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id35">
<h2><strong>Implementation in Python</strong><a class="headerlink" href="#id35" title="Link to this heading">#</a></h2>
<section id="id36">
<h3><strong>1. Load the Dataset</strong><a class="headerlink" href="#id36" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_digits</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>

<span class="c1"># Load the dataset</span>
<span class="n">digits</span> <span class="o">=</span> <span class="n">load_digits</span><span class="p">()</span>

<span class="c1"># Display data sample</span>
<span class="nb">print</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Print feature data of second image</span>
<span class="nb">print</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">)</span>   <span class="c1"># Print raw images</span>
</pre></div>
</div>
</section>
<section id="visualizing-the-digits">
<h3><strong>2. Visualizing the Digits</strong><a class="headerlink" href="#visualizing-the-digits" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Display a single image</span>
<span class="n">plt</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gray</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Display multiple images</span>
<span class="n">plt</span><span class="o">.</span><span class="n">gray</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">images</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="check-target-labels">
<h3><strong>3. Check Target Labels</strong><a class="headerlink" href="#check-target-labels" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>  <span class="c1"># Display target labels (0-9)</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="id37">
<h2><strong>Training the Model</strong><a class="headerlink" href="#id37" title="Link to this heading">#</a></h2>
<section id="splitting-the-dataset">
<h3><strong>1. Splitting the Dataset</strong><a class="headerlink" href="#splitting-the-dataset" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">digits</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="training-the-logistic-regression-model">
<h3><strong>2. Training the Logistic Regression Model</strong><a class="headerlink" href="#training-the-logistic-regression-model" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize and train the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">3000</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
<p>üìå <strong>Note:</strong></p>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">lbfgs</span></code></strong> stands for <strong>Limited-memory Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShanno Algorithm</strong>.</p></li>
<li><p>It is an optimization algorithm used by Scikit-Learn for efficient convergence.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="model-evaluation-predictions">
<h2><strong>Model Evaluation &amp; Predictions</strong><a class="headerlink" href="#model-evaluation-predictions" title="Link to this heading">#</a></h2>
<section id="checking-accuracy">
<h3><strong>1. Checking Accuracy</strong><a class="headerlink" href="#checking-accuracy" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model Accuracy:&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="id38">
<h3><strong>2. Making Predictions</strong><a class="headerlink" href="#id38" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Predict a single digit</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Prediction for image 6:&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">6</span><span class="p">]]))</span>

<span class="c1"># Predict multiple digits</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predictions for first 5 images:&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">digits</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">5</span><span class="p">]))</span>
</pre></div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="id39">
<h2><strong>Conclusion</strong><a class="headerlink" href="#id39" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Logistic Regression is <strong>effective for multiclass classification</strong> problems.</p></li>
<li><p>The <strong>handwritten digit dataset</strong> is a classic example where Logistic Regression is used.</p></li>
<li><p><strong>Performance can be improved</strong> by experimenting with different hyperparameters, solvers, and advanced deep learning techniques.</p></li>
</ul>
<p>üìå <strong>Key Takeaway:</strong> Logistic Regression can classify handwritten digits <strong>accurately</strong>, making it useful for <strong>OCR (Optical Character Recognition) applications</strong>.</p>
</section>
</section>
<hr class="docutils" />
<section id="machine-learning-decision-tree">
<h1>18. Machine Learning: Decision Tree<a class="headerlink" href="#machine-learning-decision-tree" title="Link to this heading">#</a></h1>
<section id="introduction-to-decision-trees">
<h2>1. Introduction to Decision Trees<a class="headerlink" href="#introduction-to-decision-trees" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Example 1</strong>: If the dataset is simple, a regression algorithm can draw a clear separation line.</p></li>
<li><p><strong>Example 2</strong>: In a complex dataset, a single line may not fit the data well. In such cases, a decision tree can help by providing a better fit.</p></li>
</ul>
</section>
<section id="what-is-a-decision-tree">
<h2>2. What is a Decision Tree?<a class="headerlink" href="#what-is-a-decision-tree" title="Link to this heading">#</a></h2>
<p>A <strong>Decision Tree</strong> is a supervised learning technique that can be used for both <strong>classification</strong> and <strong>regression</strong> problems. However, it is predominantly used for classification tasks.</p>
<p>It is called a decision tree because, like a tree, it starts with a <strong>root node</strong> and branches out further, constructing a tree-like structure.</p>
<section id="components-of-a-decision-tree">
<h3>Components of a Decision Tree:<a class="headerlink" href="#components-of-a-decision-tree" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Decision Node</strong>: A node used to make a decision, having multiple branches.</p></li>
<li><p><strong>Leaf Node</strong>: The output of decisions, without further branching.</p></li>
</ul>
</section>
</section>
<section id="cart-algorithm">
<h2>3. CART Algorithm<a class="headerlink" href="#cart-algorithm" title="Link to this heading">#</a></h2>
<p>To build a decision tree, we use the <strong>CART (Classification and Regression Tree)</strong> algorithm. A decision tree asks a question, and based on the answer (Yes/No), it splits further into subtrees.</p>
</section>
<section id="why-use-decision-trees">
<h2>4. Why Use Decision Trees?<a class="headerlink" href="#why-use-decision-trees" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Human-Like Decision-Making</strong>: Decision Trees mimic human thinking ability, making it easy to understand the decision-making process.</p></li>
<li><p><strong>Intuitive Structure</strong>: The logic behind the decisions is clear due to its tree-like structure.</p></li>
</ul>
</section>
<section id="key-decision-tree-terminologies">
<h2>5. Key Decision Tree Terminologies<a class="headerlink" href="#key-decision-tree-terminologies" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Root Node</strong>: The starting point of the tree, representing the entire dataset, which splits into sub-datasets.</p></li>
<li><p><strong>Leaf Node</strong>: The final output node where further splitting is not possible.</p></li>
<li><p><strong>Splitting</strong>: The process of dividing a decision node or root node into sub-nodes based on conditions.</p></li>
<li><p><strong>Branch/Sub-Tree</strong>: A tree formed by splitting the root node or any other node.</p></li>
<li><p><strong>Pruning</strong>: The process of removing unnecessary branches from the tree.</p></li>
<li><p><strong>Parent/Child Node</strong>: The root node is the parent, while the nodes that split off are the child nodes.</p></li>
</ul>
</section>
<section id="how-does-the-decision-tree-algorithm-work">
<h2>6. How Does the Decision Tree Algorithm Work?<a class="headerlink" href="#how-does-the-decision-tree-algorithm-work" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>Step 1</strong>: Begin with the root node, which contains the entire dataset.</p></li>
<li><p><strong>Step 2</strong>: Use the <strong>Attribute Selection Measure (ASM)</strong> to determine the best attribute for splitting the dataset.</p></li>
<li><p><strong>Step 3</strong>: Divide the dataset into subsets based on the values of the selected attribute.</p></li>
<li><p><strong>Step 4</strong>: Create a decision tree node with the chosen attribute.</p></li>
<li><p><strong>Step 5</strong>: Recursively apply the same steps on the subsets until the dataset can no longer be classified, and the final node becomes a leaf node.</p></li>
</ol>
</section>
<section id="example-of-a-decision-tree">
<h2>7. Example of a Decision Tree<a class="headerlink" href="#example-of-a-decision-tree" title="Link to this heading">#</a></h2>
<p>Imagine a candidate who has a job offer and wants to decide whether to accept it. Here‚Äôs how the decision tree would work:</p>
<ol class="arabic simple">
<li><p><strong>Root Node</strong>: The decision tree starts with the <strong>Salary</strong> attribute.</p></li>
<li><p><strong>First Decision Node</strong>: The dataset splits based on <strong>distance from the office</strong>.</p></li>
<li><p><strong>Second Decision Node</strong>: Further splitting occurs based on <strong>cab facility</strong>.</p></li>
<li><p><strong>Leaf Nodes</strong>: The final decision is either ‚ÄúAccepted Offer‚Äù or ‚ÄúDeclined Offer.‚Äù</p></li>
</ol>
</section>
<section id="practical-implementation-decision-tree-in-python">
<h2>8. Practical Implementation: Decision Tree in Python<a class="headerlink" href="#practical-implementation-decision-tree-in-python" title="Link to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.tree</span><span class="w"> </span><span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">LabelEncoder</span>

<span class="c1"># Load dataset</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;salaries.csv&quot;</span><span class="p">)</span>

<span class="c1"># Prepare inputs and target variables</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;salary_more_then_100k&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;columns&#39;</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;salary_more_then_100k&#39;</span><span class="p">]</span>

<span class="c1"># Encode categorical data</span>
<span class="n">le_company</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;company_n&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">le_company</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;company&#39;</span><span class="p">])</span>
<span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;job_n&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">le_company</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;job&#39;</span><span class="p">])</span>
<span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;degree_n&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">le_company</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;degree&#39;</span><span class="p">])</span>

<span class="c1"># Drop original categorical columns</span>
<span class="n">inputs_n</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;company&#39;</span><span class="p">,</span> <span class="s1">&#39;job&#39;</span><span class="p">,</span> <span class="s1">&#39;degree&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;columns&#39;</span><span class="p">)</span>

<span class="c1"># Initialize and train Decision Tree Classifier</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">inputs_n</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

<span class="c1"># Evaluate the model</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">inputs_n</span><span class="p">,</span> <span class="n">target</span><span class="p">))</span>

<span class="c1"># Make predictions</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]))</span>
</pre></div>
</div>
</section>
<section id="id40">
<h2>9. Conclusion<a class="headerlink" href="#id40" title="Link to this heading">#</a></h2>
<p>Decision trees are a powerful tool for classification and regression tasks, providing an intuitive, tree-like structure that makes decision-making easier to understand. They work well with both small and complex datasets, making them a popular choice in machine learning.</p>
</section>
</section>
<hr class="docutils" />
<section id="machine-learning-confusion-matrix">
<h1>19. Machine Learning: Confusion Matrix<a class="headerlink" href="#machine-learning-confusion-matrix" title="Link to this heading">#</a></h1>
<section id="introduction-to-confusion-matrix">
<h2>1. Introduction to Confusion Matrix<a class="headerlink" href="#introduction-to-confusion-matrix" title="Link to this heading">#</a></h2>
<p>A <strong>Confusion Matrix</strong> is a tool used to explain the results of a classification model. It helps to evaluate the performance of classification algorithms by summarizing the predictions against the actual outcomes.</p>
</section>
<section id="what-is-a-confusion-matrix">
<h2>2. What is a Confusion Matrix?<a class="headerlink" href="#what-is-a-confusion-matrix" title="Link to this heading">#</a></h2>
<p>In binary classification, the outcomes are:</p>
<ul class="simple">
<li><p><strong>True</strong> (Correct prediction)</p></li>
<li><p><strong>False</strong> (Incorrect prediction)</p></li>
</ul>
<p>For example, in a binary classifier that predicts whether a person has cancer based on MRI images, the classifier‚Äôs predictions and the actual outcomes can only result in the following four combinations:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Prediction \ Actual</p></th>
<th class="head"><p>Positive (Cancer)</p></th>
<th class="head"><p>Negative (No Cancer)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Positive (Predicted Cancer)</strong></p></td>
<td><p>True Positive (TP)</p></td>
<td><p>False Positive (FP)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Negative (Predicted No Cancer)</strong></p></td>
<td><p>False Negative (FN)</p></td>
<td><p>True Negative (TN)</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="understanding-the-terms">
<h2>3. Understanding the Terms<a class="headerlink" href="#understanding-the-terms" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>True Positive (TP)</strong>: The model predicts that the patient has cancer (positive), and the patient actually has cancer (true prediction).</p></li>
<li><p><strong>False Positive (FP)</strong>: The model predicts that the patient has cancer (positive), but the patient does not have cancer (false prediction).</p></li>
<li><p><strong>False Negative (FN)</strong>: The model predicts that the patient does not have cancer (negative), but the patient actually has cancer (false prediction).</p></li>
<li><p><strong>True Negative (TN)</strong>: The model predicts that the patient does not have cancer (negative), and the patient indeed does not have cancer (true prediction).</p></li>
</ul>
</section>
<section id="type-i-and-type-ii-errors">
<h2>4. Type I and Type II Errors<a class="headerlink" href="#type-i-and-type-ii-errors" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Type I Error (False Positive)</strong>: Occurs when you reject the null hypothesis when it is actually true.</p></li>
<li><p><strong>Type II Error (False Negative)</strong>: Occurs when you fail to reject the null hypothesis when it is actually false.</p></li>
</ul>
</section>
<section id="representing-the-confusion-matrix">
<h2>5. Representing the Confusion Matrix<a class="headerlink" href="#representing-the-confusion-matrix" title="Link to this heading">#</a></h2>
<p>The confusion matrix can be visualized as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>               <span class="n">Actual</span> <span class="n">Positive</span> <span class="o">|</span> <span class="n">Actual</span> <span class="n">Negative</span>
<span class="n">Predicted</span> <span class="n">Positive</span>   <span class="o">|</span>   <span class="n">TP</span>  <span class="o">|</span>   <span class="n">FP</span>
<span class="n">Predicted</span> <span class="n">Negative</span>   <span class="o">|</span>   <span class="n">FN</span>  <span class="o">|</span>   <span class="n">TN</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>True Positive</strong>: Number of true positive predictions.</p></li>
<li><p><strong>False Positive</strong>: Number of false positive predictions.</p></li>
<li><p><strong>False Negative</strong>: Number of false negative predictions.</p></li>
<li><p><strong>True Negative</strong>: Number of true negative predictions.</p></li>
</ul>
<p>The total number of predictions is:<br />
<code class="docutils literal notranslate"><span class="pre">TP</span> <span class="pre">+</span> <span class="pre">FP</span> <span class="pre">+</span> <span class="pre">FN</span> <span class="pre">+</span> <span class="pre">TN</span></code></p>
</section>
<section id="performance-metrics">
<h2>6. Performance Metrics<a class="headerlink" href="#performance-metrics" title="Link to this heading">#</a></h2>
<p>The confusion matrix helps calculate various performance metrics to measure the accuracy of classification models.</p>
<section id="accuracy">
<h3>Accuracy<a class="headerlink" href="#accuracy" title="Link to this heading">#</a></h3>
<p>Accuracy is the fraction of correct predictions (both True Positives and True Negatives) out of all predictions.</p>
<p>Formula:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">TP</span> <span class="o">+</span> <span class="n">TN</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">TP</span> <span class="o">+</span> <span class="n">FP</span> <span class="o">+</span> <span class="n">FN</span> <span class="o">+</span> <span class="n">TN</span><span class="p">)</span>
</pre></div>
</div>
<p>However, accuracy can be misleading, especially in imbalanced datasets. For instance, if the model always predicts the negative class (no cancer), the accuracy may still be high despite the model failing to identify cancer cases.</p>
</section>
<section id="recall-sensitivity-true-positive-rate">
<h3>Recall (Sensitivity / True Positive Rate)<a class="headerlink" href="#recall-sensitivity-true-positive-rate" title="Link to this heading">#</a></h3>
<p>Recall measures the fraction of actual positive cases correctly predicted by the model.</p>
<p>Formula:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Recall</span> <span class="o">=</span> <span class="n">TP</span> <span class="o">/</span> <span class="p">(</span><span class="n">TP</span> <span class="o">+</span> <span class="n">FN</span><span class="p">)</span>
</pre></div>
</div>
<p>In our cancer example, if the model predicts 57 cancer cases correctly out of 80 actual cancer cases (TP = 57, FN = 23), the recall would be 71%.</p>
</section>
<section id="specificity-true-negative-rate">
<h3>Specificity (True Negative Rate)<a class="headerlink" href="#specificity-true-negative-rate" title="Link to this heading">#</a></h3>
<p>Specificity measures the fraction of actual negative cases correctly predicted by the model.</p>
<p>Formula:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Specificity</span> <span class="o">=</span> <span class="n">TN</span> <span class="o">/</span> <span class="p">(</span><span class="n">TN</span> <span class="o">+</span> <span class="n">FP</span><span class="p">)</span>
</pre></div>
</div>
<p>In our cancer example, if the model predicts 171 non-cancer cases correctly (TN) out of 185 total non-cancer cases (TN + FP), the specificity would be 92%.</p>
</section>
<section id="precision">
<h3>Precision<a class="headerlink" href="#precision" title="Link to this heading">#</a></h3>
<p>Precision calculates the fraction of correctly predicted positive cases out of all the predicted positive cases.</p>
<p>Formula:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Precision</span> <span class="o">=</span> <span class="n">TP</span> <span class="o">/</span> <span class="p">(</span><span class="n">TP</span> <span class="o">+</span> <span class="n">FP</span><span class="p">)</span>
</pre></div>
</div>
<p>In the cancer example, if the model predicted 71 cancer cases as positive (TP + FP), and 57 of those predictions were correct (TP), the precision would be 80%.</p>
</section>
<section id="f1-score">
<h3>F1 Score<a class="headerlink" href="#f1-score" title="Link to this heading">#</a></h3>
<p>The F1 Score combines precision and recall into a single metric. It is the harmonic mean of precision and recall.</p>
<p>Formula:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">F1</span> <span class="n">Score</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">Precision</span> <span class="o">*</span> <span class="n">Recall</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">Precision</span> <span class="o">+</span> <span class="n">Recall</span><span class="p">)</span>
</pre></div>
</div>
<p>The F1 Score helps evaluate the performance of the model by balancing the importance of precision and recall.</p>
</section>
</section>
<section id="id41">
<h2>7. Conclusion<a class="headerlink" href="#id41" title="Link to this heading">#</a></h2>
<p>While accuracy is a simple and intuitive metric, it may not be sufficient for evaluating model performance, especially in imbalanced datasets. The confusion matrix, along with performance metrics like recall, specificity, precision, and F1 score, provides a more comprehensive understanding of a model‚Äôs strengths and weaknesses.</p>
</section>
</section>
<hr class="docutils" />
<section id="machine-learning-bias-variance-trade-off">
<h1>20. Machine Learning: Bias-Variance Trade-Off<a class="headerlink" href="#machine-learning-bias-variance-trade-off" title="Link to this heading">#</a></h1>
<section id="id42">
<h2>1. Introduction<a class="headerlink" href="#id42" title="Link to this heading">#</a></h2>
<p>In machine learning, <strong>bias</strong> and <strong>variance</strong> are key factors that influence the performance of a model. Understanding and managing the <strong>bias-variance trade-off</strong> is crucial for building accurate and robust models.</p>
<section id="key-definitions">
<h3>Key Definitions<a class="headerlink" href="#key-definitions" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Bias</strong>: The error introduced by simplifying assumptions made by the model to make the target function easier to learn.</p></li>
<li><p><strong>Variance</strong>: The error that occurs due to small changes in the training set, causing the model to change its predictions significantly.</p></li>
</ul>
</section>
</section>
<section id="the-goal-of-supervised-learning">
<h2>2. The Goal of Supervised Learning<a class="headerlink" href="#the-goal-of-supervised-learning" title="Link to this heading">#</a></h2>
<p>In supervised learning, the algorithm learns from a given set of training data. The goal is to find the best mapping function ( f ) for the output variable ( Y ) based on the input data ( X ).</p>
</section>
<section id="understanding-prediction-error">
<h2>3. Understanding Prediction Error<a class="headerlink" href="#understanding-prediction-error" title="Link to this heading">#</a></h2>
<p><strong>Prediction error</strong> is the difference between the predicted values and the actual values. This error can be split into two components:</p>
<ul class="simple">
<li><p><strong>Bias error</strong>: The error due to incorrect assumptions made by the model.</p></li>
<li><p><strong>Variance error</strong>: The error that occurs due to changes in the training data.</p></li>
</ul>
</section>
<section id="bias-in-machine-learning">
<h2>4. Bias in Machine Learning<a class="headerlink" href="#bias-in-machine-learning" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Low Bias</strong>: Implies that the model makes fewer assumptions about the target function, leading to more complexity.</p></li>
<li><p><strong>High Bias</strong>: Implies that the model makes more assumptions about the target function, simplifying the model.</p></li>
</ul>
<section id="examples-of-low-and-high-bias-models">
<h3>Examples of Low and High Bias Models<a class="headerlink" href="#examples-of-low-and-high-bias-models" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Low-Bias Algorithms</strong>:</p>
<ul>
<li><p>Decision Trees</p></li>
<li><p>k-Nearest Neighbors</p></li>
<li><p>Support Vector Machines</p></li>
</ul>
</li>
<li><p><strong>High-Bias Algorithms</strong>:</p>
<ul>
<li><p>Linear Regression</p></li>
<li><p>Linear Discriminant Analysis</p></li>
<li><p>Logistic Regression</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="variance-in-machine-learning">
<h2>5. Variance in Machine Learning<a class="headerlink" href="#variance-in-machine-learning" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Low Variance</strong>: Indicates that small changes to the training data have little impact on the model‚Äôs predictions.</p></li>
<li><p><strong>High Variance</strong>: Indicates that small changes to the training data can significantly alter the model‚Äôs predictions.</p></li>
</ul>
<section id="examples-of-low-and-high-variance-models">
<h3>Examples of Low and High Variance Models<a class="headerlink" href="#examples-of-low-and-high-variance-models" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Low-Variance Algorithms</strong>:</p>
<ul>
<li><p>Linear Regression</p></li>
<li><p>Linear Discriminant Analysis</p></li>
<li><p>Logistic Regression</p></li>
</ul>
</li>
<li><p><strong>High-Variance Algorithms</strong>:</p>
<ul>
<li><p>Decision Trees</p></li>
<li><p>k-Nearest Neighbors</p></li>
<li><p>Support Vector Machines</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="bias-variance-trade-off">
<h2>6. Bias-Variance Trade-Off<a class="headerlink" href="#bias-variance-trade-off" title="Link to this heading">#</a></h2>
<p>The goal of any supervised machine learning algorithm is to find the <strong>sweet spot</strong> of <strong>low bias</strong> and <strong>low variance</strong> for optimal performance. This means achieving good predictions while avoiding overfitting and underfitting.</p>
<ul class="simple">
<li><p><strong>Parametric or Linear Models</strong>: Often have <strong>high bias</strong> but <strong>low variance</strong>.</p></li>
<li><p><strong>Nonparametric or Nonlinear Models</strong>: Often have <strong>low bias</strong> but <strong>high variance</strong>.</p></li>
</ul>
</section>
<section id="configuring-the-bias-variance-trade-off">
<h2>7. Configuring the Bias-Variance Trade-Off<a class="headerlink" href="#configuring-the-bias-variance-trade-off" title="Link to this heading">#</a></h2>
<section id="example-1-k-nearest-neighbors-k-nn">
<h3>Example 1: k-Nearest Neighbors (K-NN)<a class="headerlink" href="#example-1-k-nearest-neighbors-k-nn" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>K-NN has <strong>low bias</strong> and <strong>high variance</strong>. Increasing the number of neighbors ( k ) can reduce variance by increasing bias, as more neighbors are involved in the prediction.</p></li>
</ul>
</section>
<section id="example-2-support-vector-machines-svm">
<h3>Example 2: Support Vector Machines (SVM)<a class="headerlink" href="#example-2-support-vector-machines-svm" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>SVM has <strong>low bias</strong> and <strong>high variance</strong>. By adjusting the ( C ) parameter, you can control the trade-off, where increasing ( C ) increases bias but decreases variance.</p></li>
</ul>
</section>
</section>
<section id="the-relationship-between-bias-and-variance">
<h2>8. The Relationship Between Bias and Variance<a class="headerlink" href="#the-relationship-between-bias-and-variance" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Increasing bias</strong> leads to <strong>decreasing variance</strong>.</p></li>
<li><p><strong>Increasing variance</strong> leads to <strong>decreasing bias</strong>.</p></li>
</ul>
</section>
<section id="types-of-error">
<h2>9. Types of Error<a class="headerlink" href="#types-of-error" title="Link to this heading">#</a></h2>
<p>The error is the difference between the predicted value and the actual value.</p>
<section id="scenarios-of-bias-and-variance">
<h3>Scenarios of Bias and Variance:<a class="headerlink" href="#scenarios-of-bias-and-variance" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Low Bias, Low Variance</strong>: The model is very accurate and performs well in predictions.</p></li>
<li><p><strong>Low Bias, High Variance</strong>: The model has lower accuracy due to overfitting.</p></li>
<li><p><strong>High Bias, Low Variance</strong>: The model has lower accuracy due to underfitting.</p></li>
<li><p><strong>High Bias, High Variance</strong>: The model has lower accuracy and is prone to both underfitting and overfitting.</p></li>
</ul>
</section>
</section>
<section id="example-bias-variance-decomposition-with-python">
<h2>10. Example: Bias-Variance Decomposition with Python<a class="headerlink" href="#example-bias-variance-decomposition-with-python" title="Link to this heading">#</a></h2>
<p>You can calculate the bias and variance of a model using tools like the <code class="docutils literal notranslate"><span class="pre">mlxtend</span></code> package.</p>
<section id="code-example">
<h3>Code Example:<a class="headerlink" href="#code-example" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">mlxtend</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">mlxtend.evaluate</span><span class="w"> </span><span class="kn">import</span> <span class="n">bias_variance_decomp</span>

<span class="c1"># Load the dataset</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;student_scores.csv&#39;</span><span class="p">)</span>

<span class="c1"># Prepare data</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="c1"># Split the data</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Create and train the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>

<span class="c1"># Perform bias-variance decomposition</span>
<span class="n">mse</span><span class="p">,</span> <span class="n">bias</span><span class="p">,</span> <span class="n">var</span> <span class="o">=</span> <span class="n">bias_variance_decomp</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span> <span class="n">num_rounds</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Print results</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Bias: </span><span class="si">{</span><span class="n">bias</span><span class="si">}</span><span class="s1">, Variance: </span><span class="si">{</span><span class="n">var</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>In this case, the model has a <strong>high bias</strong> and <strong>low variance</strong>, indicating that it may be underfitting the data.</p>
</section>
</section>
<section id="id43">
<h2>11. Conclusion<a class="headerlink" href="#id43" title="Link to this heading">#</a></h2>
<p>Understanding the <strong>bias-variance trade-off</strong> is essential for selecting and configuring machine learning models. By managing bias and variance, you can improve model performance and achieve better predictions.</p>
</section>
</section>
<hr class="docutils" />
<section id="machine-learning-random-forest-algorithm">
<h1>21. Machine Learning: Random Forest Algorithm<a class="headerlink" href="#machine-learning-random-forest-algorithm" title="Link to this heading">#</a></h1>
<section id="introduction-to-random-forest">
<h2>1. Introduction to Random Forest<a class="headerlink" href="#introduction-to-random-forest" title="Link to this heading">#</a></h2>
<p><strong>Random Forest</strong> is a supervised learning algorithm used for both <strong>Classification</strong> and <strong>Regression</strong> tasks in machine learning. It is an example of <strong>ensemble learning</strong>, where multiple models are combined to improve performance.</p>
<section id="key-points">
<h3>Key Points:<a class="headerlink" href="#key-points" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Supervised learning</strong> technique.</p></li>
<li><p>Can be used for both <strong>Classification</strong> and <strong>Regression</strong>.</p></li>
<li><p>Part of the <strong>ensemble learning</strong> family, where the output is determined by combining multiple models.</p></li>
</ul>
</section>
</section>
<section id="what-is-ensemble-learning">
<h2>2. What is Ensemble Learning?<a class="headerlink" href="#what-is-ensemble-learning" title="Link to this heading">#</a></h2>
<p><strong>Ensemble learning</strong> involves combining multiple classifiers to solve complex problems and improve the overall performance of the model. By aggregating the results from several models, ensemble methods tend to be more accurate and robust.</p>
</section>
<section id="how-does-random-forest-work">
<h2>3. How Does Random Forest Work?<a class="headerlink" href="#how-does-random-forest-work" title="Link to this heading">#</a></h2>
<p>Random Forest works by creating multiple decision trees on different subsets of the data, and then combining their results to make predictions.</p>
<section id="key-steps-in-random-forest">
<h3>Key Steps in Random Forest:<a class="headerlink" href="#key-steps-in-random-forest" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Create Random Subsets</strong>: Select random subsets of the training data.</p></li>
<li><p><strong>Build Decision Trees</strong>: For each subset, a decision tree is built.</p></li>
<li><p><strong>Combine Trees‚Äô Results</strong>: The predictions from each tree are aggregated (majority voting for classification, averaging for regression).</p></li>
<li><p><strong>Final Prediction</strong>: The final output is determined by the majority vote from all trees.</p></li>
</ol>
</section>
<section id="why-use-random-forest">
<h3>Why Use Random Forest?<a class="headerlink" href="#why-use-random-forest" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Faster Training</strong>: It takes less time to train compared to other algorithms.</p></li>
<li><p><strong>High Accuracy</strong>: It predicts with high accuracy, especially on large datasets.</p></li>
<li><p><strong>Handles Missing Data</strong>: It can maintain accuracy even if a significant portion of the data is missing.</p></li>
</ul>
</section>
</section>
<section id="steps-in-random-forest-algorithm">
<h2>4. Steps in Random Forest Algorithm<a class="headerlink" href="#steps-in-random-forest-algorithm" title="Link to this heading">#</a></h2>
<p>Here is a step-by-step breakdown of how the Random Forest algorithm works:</p>
<section id="step-by-step-process">
<h3>Step-by-Step Process:<a class="headerlink" href="#step-by-step-process" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Select Random Data Points</strong>: Choose <strong>K random data points</strong> from the training dataset.</p></li>
<li><p><strong>Build Decision Trees</strong>: For each random subset, build a decision tree.</p></li>
<li><p><strong>Set the Number of Trees</strong>: Define the number of trees, <strong>N</strong>, that you want in the forest.</p></li>
<li><p><strong>Repeat Steps 1 &amp; 2</strong>: Repeat the process to create multiple decision trees.</p></li>
<li><p><strong>Make Predictions</strong>: For new data, predict using each decision tree and assign the final prediction based on the <strong>majority vote</strong>.</p></li>
</ol>
</section>
</section>
<section id="example-use-case-iris-flower-classification">
<h2>5. Example Use Case: Iris Flower Classification<a class="headerlink" href="#example-use-case-iris-flower-classification" title="Link to this heading">#</a></h2>
<p>Let‚Äôs assume <strong>Abhi</strong> is interested in classifying iris flowers into species based on specific measurements. He has the following data:</p>
<ul class="simple">
<li><p><strong>Features</strong>: Petal length, petal width, sepal length, sepal width (in cm).</p></li>
<li><p><strong>Labels</strong>: The known species are <strong>Setosa</strong>, <strong>Versicolor</strong>, and <strong>Virginica</strong>.</p></li>
</ul>
<section id="goal">
<h3>Goal:<a class="headerlink" href="#goal" title="Link to this heading">#</a></h3>
<p>Create a machine learning model to predict the species of new iris flowers based on the measurements.</p>
</section>
<section id="example-code">
<h3>Example Code:<a class="headerlink" href="#example-code" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.ensemble</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomForestClassifier</span>

<span class="c1"># Load the iris dataset</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>

<span class="c1"># Convert to DataFrame</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="c1"># Map target labels to species names</span>
<span class="n">a</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">x</span><span class="p">]</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;flower_name&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">a</span><span class="p">)</span>

<span class="c1"># Features and target variable</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;target&#39;</span><span class="p">,</span> <span class="s1">&#39;flower_name&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;columns&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">target</span>

<span class="c1"># Split data into training and testing sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="c1"># Create and train the Random Forest model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Evaluate the model&#39;s performance</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>

<span class="c1"># Predict the species of a new iris</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">4.8</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]]))</span>
</pre></div>
</div>
</section>
<section id="key-parameters">
<h3>Key Parameters:<a class="headerlink" href="#key-parameters" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>n_estimators=40</strong>: This parameter defines the number of trees in the forest.</p></li>
</ul>
</section>
</section>
<section id="id44">
<h2>6. Conclusion<a class="headerlink" href="#id44" title="Link to this heading">#</a></h2>
<p>The Random Forest algorithm is a powerful tool for both classification and regression tasks. It works by combining multiple decision trees to improve accuracy and robustness. With its ability to handle missing data and large datasets efficiently, it‚Äôs widely used in various real-world applications.</p>
</section>
</section>
<hr class="docutils" />
<section id="machine-learning-support-vector-machine-svm">
<h1>22. Machine Learning: Support Vector Machine (SVM)<a class="headerlink" href="#machine-learning-support-vector-machine-svm" title="Link to this heading">#</a></h1>
<section id="introduction-to-support-vector-machine-svm">
<h2>1. Introduction to Support Vector Machine (SVM)<a class="headerlink" href="#introduction-to-support-vector-machine-svm" title="Link to this heading">#</a></h2>
<p><strong>Support Vector Machine (SVM)</strong> is a popular <strong>supervised learning</strong> algorithm primarily used for <strong>classification</strong> tasks, though it can also be applied to regression problems. SVM aims to find the optimal decision boundary that best separates data points of different classes.</p>
<section id="id45">
<h3>Key Points:<a class="headerlink" href="#id45" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Supervised learning</strong> algorithm.</p></li>
<li><p>Used for <strong>Classification</strong> and <strong>Regression</strong>.</p></li>
<li><p>Primarily used for <strong>Classification</strong> problems in machine learning.</p></li>
</ul>
</section>
</section>
<section id="decision-boundary-in-svm">
<h2>2. Decision Boundary in SVM<a class="headerlink" href="#decision-boundary-in-svm" title="Link to this heading">#</a></h2>
<p>The goal of SVM is to create the best line (or hyperplane) that can separate n-dimensional space into distinct classes. This line or boundary is called the <strong>hyperplane</strong>, and the algorithm‚Äôs task is to find the optimal hyperplane that maximizes the margin between the classes.</p>
</section>
<section id="why-support-vector-machine">
<h2>3. Why ‚ÄúSupport Vector Machine‚Äù?<a class="headerlink" href="#why-support-vector-machine" title="Link to this heading">#</a></h2>
<p>The name ‚ÄúSupport Vector Machine‚Äù comes from the algorithm‚Äôs reliance on <strong>support vectors</strong>. Support vectors are the extreme data points that help define the optimal hyperplane. These points are critical in forming the decision boundary, and hence, the term ‚Äúsupport vector‚Äù is used.</p>
</section>
<section id="common-uses-of-svm">
<h2>4. Common Uses of SVM<a class="headerlink" href="#common-uses-of-svm" title="Link to this heading">#</a></h2>
<p>SVM can be applied to various machine learning problems, including:</p>
<ul class="simple">
<li><p><strong>Face detection</strong></p></li>
<li><p><strong>Image classification</strong></p></li>
<li><p><strong>Text categorization</strong></p></li>
</ul>
</section>
<section id="types-of-svm">
<h2>5. Types of SVM<a class="headerlink" href="#types-of-svm" title="Link to this heading">#</a></h2>
<p>There are two main types of SVMs based on how the data can be separated:</p>
<ol class="arabic simple">
<li><p><strong>Linear SVM</strong>: Used when the data is linearly separable (can be separated by a straight line).</p></li>
<li><p><strong>Non-linear SVM</strong>: Used when the data cannot be separated by a straight line, and a more complex decision boundary is required.</p></li>
</ol>
<section id="linear-svm">
<h3>Linear SVM<a class="headerlink" href="#linear-svm" title="Link to this heading">#</a></h3>
<p>When the data can be classified into two classes using a straight line, the data is considered <strong>linearly separable</strong>, and a <strong>Linear SVM</strong> classifier is used.</p>
</section>
<section id="non-linear-svm">
<h3>Non-linear SVM<a class="headerlink" href="#non-linear-svm" title="Link to this heading">#</a></h3>
<p>When the data cannot be separated by a straight line, the dataset is considered <strong>non-linearly separable</strong>, and a <strong>Non-linear SVM</strong> classifier is used.</p>
</section>
</section>
<section id="hyperplane-and-dimensions">
<h2>6. Hyperplane and Dimensions<a class="headerlink" href="#hyperplane-and-dimensions" title="Link to this heading">#</a></h2>
<p>The dimensions of the hyperplane depend on the number of features in the dataset:</p>
<ul class="simple">
<li><p><strong>2 features</strong>: The hyperplane is a straight line.</p></li>
<li><p><strong>3 features</strong>: The hyperplane is a 2-dimensional plane.</p></li>
</ul>
</section>
<section id="how-does-svm-work">
<h2>7. How Does SVM Work?<a class="headerlink" href="#how-does-svm-work" title="Link to this heading">#</a></h2>
<section id="linear-svm-example">
<h3>Linear SVM Example:<a class="headerlink" href="#linear-svm-example" title="Link to this heading">#</a></h3>
<p>Let‚Äôs assume we have a dataset with two classes (green and blue) and two features <strong>x1</strong> and <strong>x2</strong>. The task is to classify each pair (x1, x2) as either green or blue.</p>
<p>While this is a 2-dimensional space, we use a straight line to separate the two classes.</p>
</section>
<section id="hyperplane-and-support-vectors">
<h3>Hyperplane and Support Vectors<a class="headerlink" href="#hyperplane-and-support-vectors" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Hyperplane</strong>: The decision boundary that separates the classes.</p></li>
<li><p><strong>Support Vectors</strong>: The data points closest to the hyperplane. These are the key points that help in defining the margin.</p></li>
<li><p><strong>Margin</strong>: The distance between the support vectors and the hyperplane. SVM‚Äôs goal is to maximize this margin.</p></li>
<li><p><strong>Optimal Hyperplane</strong>: The hyperplane that maximizes the margin between the support vectors from both classes.</p></li>
</ul>
</section>
</section>
<section id="use-case-iris-flower-classification">
<h2>8. Use Case: Iris Flower Classification<a class="headerlink" href="#use-case-iris-flower-classification" title="Link to this heading">#</a></h2>
<p>Let‚Äôs assume <strong>Abhi</strong> is interested in classifying iris flowers based on certain features:</p>
<ul class="simple">
<li><p><strong>Features</strong>: Petal length, petal width, sepal length, and sepal width (all measured in centimeters).</p></li>
<li><p><strong>Labels</strong>: The species are already known: <strong>Setosa</strong>, <strong>Versicolor</strong>, and <strong>Virginica</strong>.</p></li>
</ul>
<p>The goal is to create a machine learning model that can predict the species of new irises based on these measurements.</p>
<section id="id46">
<h3>Example Code:<a class="headerlink" href="#id46" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.svm</span><span class="w"> </span><span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>

<span class="c1"># Load the iris dataset</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>

<span class="c1"># Convert to DataFrame</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;flower_name&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">x</span><span class="p">])</span>

<span class="c1"># Split the data for visualization</span>
<span class="n">df0</span> <span class="o">=</span> <span class="n">df</span><span class="p">[:</span><span class="mi">50</span><span class="p">]</span>
<span class="n">df1</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="mi">50</span><span class="p">:</span><span class="mi">100</span><span class="p">]</span>

<span class="c1"># Plot Sepal Length vs Sepal Width (Setosa vs Versicolor)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Sepal Length&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Sepal Width&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df0</span><span class="p">[</span><span class="s1">&#39;sepal length (cm)&#39;</span><span class="p">],</span> <span class="n">df0</span><span class="p">[</span><span class="s1">&#39;sepal width (cm)&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;green&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df1</span><span class="p">[</span><span class="s1">&#39;sepal length (cm)&#39;</span><span class="p">],</span> <span class="n">df1</span><span class="p">[</span><span class="s1">&#39;sepal width (cm)&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Split the data into training and test sets</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;target&#39;</span><span class="p">,</span> <span class="s1">&#39;flower_name&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;columns&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">target</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="c1"># Create and train the SVM model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Evaluate the model&#39;s performance</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model Accuracy: &quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>

<span class="c1"># Predict the species of a new iris</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predicted Species: &quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">4.8</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]]))</span>
</pre></div>
</div>
</section>
<section id="id47">
<h3>Key Parameters:<a class="headerlink" href="#id47" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>C</strong>: The penalty parameter that controls the margin width.</p></li>
<li><p><strong>kernel</strong>: The kernel function used to transform the data for non-linear SVM (e.g., linear, polynomial, radial basis function).</p></li>
</ul>
</section>
</section>
<section id="id48">
<h2>9. Conclusion<a class="headerlink" href="#id48" title="Link to this heading">#</a></h2>
<p>Support Vector Machines are powerful tools for classification and regression tasks. They aim to find the optimal hyperplane that maximizes the margin between classes. SVM is highly effective, especially for high-dimensional data, and is used in various real-world applications such as face detection, image classification, and text categorization.</p>
</section>
</section>
<hr class="docutils" />
<section id="machine-learning-overfitting-and-underfitting">
<h1>23. Machine Learning: Overfitting and Underfitting<a class="headerlink" href="#machine-learning-overfitting-and-underfitting" title="Link to this heading">#</a></h1>
<section id="id49">
<h2>1. Introduction<a class="headerlink" href="#id49" title="Link to this heading">#</a></h2>
<p>The main goal of every <strong>machine learning (ML)</strong> model is to <strong>generalize well</strong>. A well-generalized model can produce reliable and accurate outputs on unseen data. Overfitting and underfitting are two common problems that impact the performance and accuracy of ML models.</p>
<ul class="simple">
<li><p><strong>Generalization</strong>: The ability of a model to perform well on unseen data.</p></li>
<li><p><strong>Overfitting and Underfitting</strong>: These are key issues that can reduce a model‚Äôs ability to generalize effectively.</p></li>
</ul>
</section>
<section id="key-concepts">
<h2>2. Key Concepts<a class="headerlink" href="#key-concepts" title="Link to this heading">#</a></h2>
<section id="noise">
<h3>Noise<a class="headerlink" href="#noise" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Noise</strong> refers to irrelevant data that can reduce the performance of the model. It adds unpredictability and reduces model accuracy.</p></li>
</ul>
</section>
<section id="bias">
<h3>Bias<a class="headerlink" href="#bias" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Bias</strong> is the difference between the predicted values and the actual values. High bias can lead to underfitting.</p></li>
</ul>
</section>
<section id="variance">
<h3>Variance<a class="headerlink" href="#variance" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Variance</strong> is the model‚Äôs ability to perform well with the training dataset but fail with the test dataset. High variance can lead to overfitting.</p></li>
</ul>
</section>
</section>
<section id="overfitting">
<h2>3. Overfitting<a class="headerlink" href="#overfitting" title="Link to this heading">#</a></h2>
<p><strong>Overfitting</strong> occurs when a model learns the details and noise in the training data to the extent that it negatively impacts the performance on unseen data.</p>
<ul class="simple">
<li><p><strong>Characteristics of Overfitting</strong>:</p>
<ul>
<li><p><strong>Low bias</strong>: The model performs well on the training data.</p></li>
<li><p><strong>High variance</strong>: The model performs poorly on the test data.</p></li>
</ul>
</li>
<li><p><strong>Causes</strong>:</p>
<ul>
<li><p>The model is too complex relative to the data.</p></li>
<li><p>The model fits noise or outliers in the data.</p></li>
</ul>
</li>
<li><p><strong>How to Avoid Overfitting</strong>:</p>
<ul>
<li><p>Use <strong>cross-validation</strong>.</p></li>
<li><p>Train with <strong>more data</strong>.</p></li>
<li><p>Remove irrelevant <strong>features</strong>.</p></li>
<li><p>Apply <strong>regularization</strong>.</p></li>
<li><p>Use <strong>ensemble methods</strong> (e.g., random forests, gradient boosting).</p></li>
</ul>
</li>
</ul>
</section>
<section id="underfitting">
<h2>4. Underfitting<a class="headerlink" href="#underfitting" title="Link to this heading">#</a></h2>
<p><strong>Underfitting</strong> occurs when a model is too simple to capture the underlying patterns in the data. The model fails to learn properly during training and thus performs poorly.</p>
<ul class="simple">
<li><p><strong>Characteristics of Underfitting</strong>:</p>
<ul>
<li><p><strong>High bias</strong>: The model performs poorly on both the training and test data.</p></li>
<li><p><strong>Low variance</strong>: The model doesn‚Äôt vary much with changes in the data.</p></li>
</ul>
</li>
<li><p><strong>Causes</strong>:</p>
<ul>
<li><p>The model is too simple.</p></li>
<li><p>The model doesn‚Äôt learn the relevant patterns in the data.</p></li>
</ul>
</li>
</ul>
</section>
<section id="good-fit-model">
<h2>5. Good Fit Model<a class="headerlink" href="#good-fit-model" title="Link to this heading">#</a></h2>
<p>A <strong>good fit model</strong> is one that performs well on both the training dataset and unseen (test) dataset. It balances both bias and variance to generalize well.</p>
</section>
<section id="good-fit-example">
<h2>6. Good Fit Example<a class="headerlink" href="#good-fit-example" title="Link to this heading">#</a></h2>
<p>Consider three students preparing for a mathematics exam:</p>
<ul class="simple">
<li><p><strong>First student</strong>:</p>
<ul>
<li><p>Prepared only addition operations and skipped other topics.</p></li>
<li><p><strong>Result</strong>: Can only answer addition-related questions. (Underfitting)</p></li>
</ul>
</li>
<li><p><strong>Second student</strong>:</p>
<ul>
<li><p>Prepared only topics from the textbook.</p></li>
<li><p><strong>Result</strong>: Can answer only questions from the textbook. (Overfitting)</p></li>
</ul>
</li>
<li><p><strong>Third student</strong>:</p>
<ul>
<li><p>Prepared all topics from the textbook and practiced additional topics from other resources.</p></li>
<li><p><strong>Result</strong>: Can answer questions from all covered material. (Good fit)</p></li>
</ul>
</li>
</ul>
<section id="comparison">
<h3>Comparison:<a class="headerlink" href="#comparison" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>First student</strong>: Underfitting.</p></li>
<li><p><strong>Second student</strong>: Overfitting.</p></li>
<li><p><strong>Third student</strong>: Good fit.</p></li>
</ul>
</section>
</section>
<section id="regularization">
<h2>7. Regularization<a class="headerlink" href="#regularization" title="Link to this heading">#</a></h2>
<p>Regularization is a technique used to prevent overfitting by penalizing more complex models.</p>
<section id="types-of-regularization">
<h3>Types of Regularization:<a class="headerlink" href="#types-of-regularization" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>L1 Regularization</strong> (Lasso): Adds a penalty based on the absolute value of coefficients.</p></li>
<li><p><strong>L2 Regularization</strong> (Ridge): Adds a penalty based on the square of coefficients.</p></li>
<li><p><strong>Elastic Net</strong>: Combines L1 and L2 regularization.</p></li>
</ol>
</section>
<section id="pros-and-cons-of-regularization">
<h3>Pros and Cons of Regularization<a class="headerlink" href="#pros-and-cons-of-regularization" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Pros</strong>:</p>
<ul>
<li><p>Helps in solving overfitting.</p></li>
<li><p>Lowers model variance.</p></li>
<li><p>Computationally efficient.</p></li>
</ul>
</li>
<li><p><strong>Cons</strong>:</p>
<ul>
<li><p>Low interpretability of the model after applying regularization.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="id50">
<h2>8. Conclusion<a class="headerlink" href="#id50" title="Link to this heading">#</a></h2>
<p>To ensure a model performs well on new, unseen data, it is crucial to strike a balance between <strong>bias</strong> and <strong>variance</strong>. <strong>Overfitting</strong> and <strong>underfitting</strong> represent the extremes that can harm a model‚Äôs ability to generalize. By applying techniques like regularization and cross-validation, one can improve the model‚Äôs ability to generalize, ensuring it performs reliably across different datasets.</p>
</section>
</section>
<hr class="docutils" />
<section id="machine-learning-lasso-ridge-regression">
<h1>24. Machine Learning: Lasso &amp; Ridge Regression<a class="headerlink" href="#machine-learning-lasso-ridge-regression" title="Link to this heading">#</a></h1>
<section id="introduction-to-linear-regression">
<h2>1. Introduction to Linear Regression<a class="headerlink" href="#introduction-to-linear-regression" title="Link to this heading">#</a></h2>
<p><strong>Linear Regression</strong> is a standard algorithm for regression tasks, used to explain the relationship between a dependent variable and one or more independent variables.</p>
<ul class="simple">
<li><p>In simple terms, it finds the best-fitting straight line (or hyperplane in higher dimensions) to represent the relationship.</p></li>
</ul>
</section>
<section id="lasso-regression">
<h2>2. Lasso Regression<a class="headerlink" href="#lasso-regression" title="Link to this heading">#</a></h2>
<p><strong>Lasso Regression</strong> is a regularized version of linear regression that includes an L1 penalty. It aims to minimize the error by shrinking the coefficients of the model.</p>
<section id="id51">
<h3>Key Points:<a class="headerlink" href="#id51" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>L1 Penalty</strong>: The L1 penalty in Lasso regression penalizes the sum of the absolute values of the coefficients, which encourages sparsity (i.e., some coefficients are driven to zero, effectively removing certain features).</p></li>
<li><p><strong>Objective Function</strong>:</p>
<ul>
<li><p><strong>Minimization Objective</strong>:<br />
[
\text{LS Objective} + \alpha \times \left(\sum |\text{coefficients}|\right)
]</p></li>
</ul>
</li>
<li><p><strong>Benefits</strong>: Lasso helps prevent overfitting by reducing the complexity of the model, leading to a simpler and more interpretable model.</p></li>
</ul>
</section>
<section id="how-to-avoid-overfitting-with-lasso">
<h3>How to Avoid Overfitting with Lasso:<a class="headerlink" href="#how-to-avoid-overfitting-with-lasso" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Regularization techniques, such as Lasso, are key to reducing overfitting by adding a penalty term to the cost function.</p></li>
</ul>
</section>
<section id="lasso-regression-in-practice">
<h3>Lasso Regression in Practice:<a class="headerlink" href="#lasso-regression-in-practice" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">Lasso</span>

<span class="c1"># Creating and training the Lasso model</span>
<span class="n">lasso_reg</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">lasso_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>

<span class="c1"># Model Evaluation</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training Score:&quot;</span><span class="p">,</span> <span class="n">lasso_reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Testing Score:&quot;</span><span class="p">,</span> <span class="n">lasso_reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_X</span><span class="p">,</span> <span class="n">test_y</span><span class="p">))</span>
</pre></div>
</div>
</section>
</section>
<section id="ridge-regression">
<h2>3. Ridge Regression<a class="headerlink" href="#ridge-regression" title="Link to this heading">#</a></h2>
<p><strong>Ridge Regression</strong> is another regularized version of linear regression that includes an L2 penalty. This method penalizes large coefficients by adding a cost proportional to the sum of the squared coefficients.</p>
<section id="id52">
<h3>Key Points:<a class="headerlink" href="#id52" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>L2 Penalty</strong>: The L2 penalty in Ridge regression penalizes the sum of the squared coefficients, which helps prevent the model from becoming too complex.</p></li>
<li><p><strong>Objective Function</strong>:</p>
<ul>
<li><p><strong>Minimization Objective</strong>:<br />
[
\text{LS Objective} + \alpha \times \left(\sum \text{coefficients}^2\right)
]</p></li>
</ul>
</li>
<li><p><strong>Benefits</strong>: Ridge regression helps improve the stability of the model by shrinking large coefficients, but it does not necessarily reduce the number of features like Lasso.</p></li>
</ul>
</section>
<section id="ridge-regression-in-practice">
<h3>Ridge Regression in Practice:<a class="headerlink" href="#ridge-regression-in-practice" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">Ridge</span>

<span class="c1"># Creating and training the Ridge model</span>
<span class="n">ridge_reg</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">ridge_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>

<span class="c1"># Model Evaluation</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training Score:&quot;</span><span class="p">,</span> <span class="n">ridge_reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Testing Score:&quot;</span><span class="p">,</span> <span class="n">ridge_reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_X</span><span class="p">,</span> <span class="n">test_y</span><span class="p">))</span>
</pre></div>
</div>
</section>
</section>
<section id="comparing-lasso-and-ridge-regression">
<h2>4. Comparing Lasso and Ridge Regression<a class="headerlink" href="#comparing-lasso-and-ridge-regression" title="Link to this heading">#</a></h2>
<p>Both Lasso and Ridge regression are effective methods to prevent overfitting, but they serve slightly different purposes:</p>
<ul class="simple">
<li><p><strong>Lasso</strong> (L1 regularization) performs <strong>feature selection</strong> by driving some coefficients to zero.</p></li>
<li><p><strong>Ridge</strong> (L2 regularization) focuses on <strong>shrinking</strong> the coefficients without reducing their count.</p></li>
</ul>
<p>In practice, the choice between Lasso and Ridge depends on whether you need feature selection (Lasso) or if you simply want to shrink coefficients (Ridge).</p>
</section>
<section id="dataset-melbourne-housing-market">
<h2>5. Dataset: Melbourne Housing Market<a class="headerlink" href="#dataset-melbourne-housing-market" title="Link to this heading">#</a></h2>
<p>We will use a dataset containing information about Melbourne house sale prices to demonstrate regression techniques.</p>
<section id="dataset-details">
<h3>Dataset Details:<a class="headerlink" href="#dataset-details" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Price</strong>: The sale price in dollars.</p></li>
<li><p><strong>Rooms</strong>: Number of rooms in the property.</p></li>
<li><p><strong>Method</strong>: The method of sale (e.g., sold, sold prior, etc.).</p></li>
<li><p><strong>Type</strong>: Type of property (house, townhouse, etc.).</p></li>
<li><p><strong>Other Features</strong>: Includes variables like distance from CBD, land size, building area, number of bathrooms, etc.</p></li>
</ul>
</section>
<section id="data-preprocessing">
<h3>Data Preprocessing:<a class="headerlink" href="#data-preprocessing" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load and clean dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;Melbourne_housing_FULL.csv&#39;</span><span class="p">)</span>

<span class="c1"># Selecting relevant columns</span>
<span class="n">cols_to_use</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Suburb&#39;</span><span class="p">,</span> <span class="s1">&#39;Rooms&#39;</span><span class="p">,</span> <span class="s1">&#39;Type&#39;</span><span class="p">,</span> <span class="s1">&#39;Method&#39;</span><span class="p">,</span> <span class="s1">&#39;SellerG&#39;</span><span class="p">,</span> <span class="s1">&#39;Regionname&#39;</span><span class="p">,</span> <span class="s1">&#39;Propertycount&#39;</span><span class="p">,</span> <span class="s1">&#39;Distance&#39;</span><span class="p">,</span> <span class="s1">&#39;CouncilArea&#39;</span><span class="p">,</span> <span class="s1">&#39;Bedroom2&#39;</span><span class="p">,</span> <span class="s1">&#39;Bathroom&#39;</span><span class="p">,</span> <span class="s1">&#39;Car&#39;</span><span class="p">,</span> <span class="s1">&#39;Landsize&#39;</span><span class="p">,</span> <span class="s1">&#39;BuildingArea&#39;</span><span class="p">,</span> <span class="s1">&#39;Price&#39;</span><span class="p">]</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="n">cols_to_use</span><span class="p">]</span>

<span class="c1"># Fill missing values and create dummy variables</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">drop_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id53">
<h3>Splitting the Data:<a class="headerlink" href="#id53" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Creating features and labels</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Price&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[</span><span class="s1">&#39;Price&#39;</span><span class="p">]</span>

<span class="c1"># Splitting into training and testing datasets</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">train_X</span><span class="p">,</span> <span class="n">test_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">test_y</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="linear-regression-model">
<h3>Linear Regression Model:<a class="headerlink" href="#linear-regression-model" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="c1"># Training the Linear Regression model</span>
<span class="n">reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>

<span class="c1"># Model Evaluation</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training Score:&quot;</span><span class="p">,</span> <span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Testing Score:&quot;</span><span class="p">,</span> <span class="n">reg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">test_X</span><span class="p">,</span> <span class="n">test_y</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="model-interpretation">
<h3>Model Interpretation:<a class="headerlink" href="#model-interpretation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>If the <strong>training score</strong> is very high but the <strong>test score</strong> is significantly lower, the model is <strong>overfitting</strong>.</p></li>
</ul>
</section>
</section>
<section id="id54">
<h2>6. Conclusion<a class="headerlink" href="#id54" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Lasso and Ridge Regression</strong> are both regularization techniques designed to handle overfitting by penalizing large coefficients.</p></li>
<li><p><strong>Lasso</strong> is ideal when you want to perform feature selection, while <strong>Ridge</strong> is helpful when you want to shrink coefficients without eliminating them.</p></li>
<li><p>Regularization techniques like these are essential for building stable and generalized models that perform well on unseen data.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="machine-learning-k-means-clustering">
<h1>25. Machine Learning: K-Means Clustering<a class="headerlink" href="#machine-learning-k-means-clustering" title="Link to this heading">#</a></h1>
<section id="what-is-clustering">
<h2>1. What is Clustering?<a class="headerlink" href="#what-is-clustering" title="Link to this heading">#</a></h2>
<p><strong>Clustering</strong> is an unsupervised machine learning technique used to group similar data points into clusters based on shared properties or characteristics. Unlike classification, clustering works on unlabelled data and tries to uncover inherent patterns in the data.</p>
<ul class="simple">
<li><p><strong>Goal</strong>: Group similar data points into clusters, where points within a cluster are more similar to each other than to those in other clusters.</p></li>
</ul>
</section>
<section id="k-means-clustering-algorithm">
<h2>2. K-Means Clustering Algorithm<a class="headerlink" href="#k-means-clustering-algorithm" title="Link to this heading">#</a></h2>
<p><strong>K-Means</strong> is a widely used clustering algorithm that divides an unlabelled dataset into <code class="docutils literal notranslate"><span class="pre">K</span></code> predefined clusters.</p>
<section id="id55">
<h3>Key Concepts:<a class="headerlink" href="#id55" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>K</strong>: Represents the number of clusters we want to create.</p></li>
<li><p><strong>Centroid-based</strong>: The algorithm works by assigning each data point to the nearest centroid (the center of a cluster).</p></li>
<li><p><strong>Unsupervised Learning</strong>: No labels are required, and the goal is to find structure within the data.</p></li>
</ul>
</section>
<section id="steps-in-k-means-algorithm">
<h3>Steps in K-Means Algorithm:<a class="headerlink" href="#steps-in-k-means-algorithm" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Step 1</strong>: Select the number <code class="docutils literal notranslate"><span class="pre">K</span></code> to define the number of clusters.</p></li>
<li><p><strong>Step 2</strong>: Randomly select <code class="docutils literal notranslate"><span class="pre">K</span></code> initial centroids.</p></li>
<li><p><strong>Step 3</strong>: Assign each data point to the nearest centroid. This forms <code class="docutils literal notranslate"><span class="pre">K</span></code> clusters.</p></li>
<li><p><strong>Step 4</strong>: Recalculate the centroids based on the newly assigned points (center of gravity of each cluster).</p></li>
<li><p><strong>Step 5</strong>: Reassign each data point to the nearest updated centroid.</p></li>
<li><p><strong>Step 6</strong>: Repeat steps 4 and 5 until the centroids no longer change.</p></li>
<li><p><strong>Step 7</strong>: The model is ready.</p></li>
</ol>
</section>
</section>
<section id="scenario-k-means-in-action">
<h2>3. Scenario: K-Means in Action<a class="headerlink" href="#scenario-k-means-in-action" title="Link to this heading">#</a></h2>
<p>Let‚Äôs consider a dataset with two variables (X and Y) scattered on a 2D plot:</p>
<ul class="simple">
<li><p><strong>Step 1</strong>: We randomly choose two centroids (<code class="docutils literal notranslate"><span class="pre">K=2</span></code>).</p></li>
<li><p><strong>Step 2</strong>: Assign each data point to the nearest centroid.</p></li>
<li><p><strong>Step 3</strong>: Recalculate the centroids based on the new assignments.</p></li>
<li><p><strong>Step 4</strong>: Repeat the process until the centroids stabilize.</p></li>
</ul>
<p>This process helps group data points into clusters based on their proximity to the centroids.</p>
</section>
<section id="how-to-determine-the-correct-number-of-clusters">
<h2>4. How to Determine the Correct Number of Clusters?<a class="headerlink" href="#how-to-determine-the-correct-number-of-clusters" title="Link to this heading">#</a></h2>
<section id="elbow-method">
<h3>Elbow Method:<a class="headerlink" href="#elbow-method" title="Link to this heading">#</a></h3>
<p>The <strong>Elbow Method</strong> is a popular technique for determining the optimal number of clusters (<code class="docutils literal notranslate"><span class="pre">K</span></code>). It works by plotting the <strong>Sum of Squared Errors (SSE)</strong> for different values of <code class="docutils literal notranslate"><span class="pre">K</span></code> and looking for the ‚Äúelbow,‚Äù where the rate of decrease in SSE slows down.</p>
<section id="example-in-python">
<h4>Example in Python:<a class="headerlink" href="#example-in-python" title="Link to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.cluster</span><span class="w"> </span><span class="kn">import</span> <span class="n">KMeans</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>

<span class="c1"># Load dataset</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;income.csv&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">Age</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Income($)&#39;</span><span class="p">])</span>

<span class="c1"># Apply KMeans clustering with K=3</span>
<span class="n">km</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">y_predicted</span> <span class="o">=</span> <span class="n">km</span><span class="o">.</span><span class="n">fit_predict</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;Age&#39;</span><span class="p">,</span> <span class="s1">&#39;Income($)&#39;</span><span class="p">]])</span>

<span class="c1"># Plot clusters</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">Age</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Income($)&#39;</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_predicted</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">km</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">km</span><span class="o">.</span><span class="n">cluster_centers_</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Centroid&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Age&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Income ($)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="c1"># Elbow method to find optimal K</span>
<span class="n">sse</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">k_range</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_range</span><span class="p">:</span>
    <span class="n">km</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="n">k</span><span class="p">)</span>
    <span class="n">km</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;Age&#39;</span><span class="p">,</span> <span class="s1">&#39;Income($)&#39;</span><span class="p">]])</span>
    <span class="n">sse</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">km</span><span class="o">.</span><span class="n">inertia_</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">k_range</span><span class="p">,</span> <span class="n">sse</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Number of Clusters (K)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Sum of Squared Error&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="cluster-scaling">
<h2>5. Cluster Scaling<a class="headerlink" href="#cluster-scaling" title="Link to this heading">#</a></h2>
<p>Sometimes, features (such as income or age) have different scales, which could bias the clustering algorithm. To prevent this, it‚Äôs recommended to scale the data before clustering.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">MinMaxScaler</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span>

<span class="c1"># Scaling the data</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Income($)&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;Income($)&#39;</span><span class="p">]])</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;Age&#39;</span><span class="p">]])</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">Age</span><span class="p">,</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Income($)&#39;</span><span class="p">])</span>
</pre></div>
</div>
</section>
<section id="clustering-performance-metrics">
<h2>6. Clustering Performance Metrics<a class="headerlink" href="#clustering-performance-metrics" title="Link to this heading">#</a></h2>
<p>For unsupervised models like K-Means, evaluating the quality of clusters can be challenging. Some common metrics used to evaluate clustering performance include:</p>
<ul class="simple">
<li><p><strong>Precision</strong>: Measures the similarity of the data points within each cluster and the dissimilarity between different clusters.</p></li>
<li><p><strong>Homogeneity</strong>: Measures whether each cluster contains only members of a single class.</p></li>
<li><p><strong>Silhouette Score</strong>: Measures how similar each point is to its own cluster compared to other clusters.</p></li>
<li><p><strong>Completeness</strong>: Measures whether all members of a cluster are assigned to the same class.</p></li>
</ul>
</section>
<section id="id56">
<h2>7. Conclusion<a class="headerlink" href="#id56" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>K-Means Clustering</strong> is a simple and powerful clustering algorithm that groups similar data points into <code class="docutils literal notranslate"><span class="pre">K</span></code> clusters based on their distance to centroids.</p></li>
<li><p>The <strong>Elbow Method</strong> is commonly used to determine the optimal number of clusters by analyzing the Sum of Squared Errors.</p></li>
<li><p><strong>Data scaling</strong> is important to ensure that all features contribute equally to the clustering process.</p></li>
</ul>
<p>By following these steps, you can effectively implement and evaluate K-Means clustering in a variety of real-world scenarios.</p>
</section>
</section>
<hr class="docutils" />
<section id="machine-learning-k-nearest-neighbor-k-nn">
<h1>Machine Learning: K-Nearest Neighbor (K-NN)<a class="headerlink" href="#machine-learning-k-nearest-neighbor-k-nn" title="Link to this heading">#</a></h1>
<section id="what-is-k-nearest-neighbor">
<h2>1. What is K-Nearest Neighbor?<a class="headerlink" href="#what-is-k-nearest-neighbor" title="Link to this heading">#</a></h2>
<p><strong>K-Nearest Neighbor (K-NN)</strong> is a <strong>supervised learning</strong> algorithm used for both classification and regression tasks. It is based on the simple principle: ‚ÄúSimilar things are near to each other.‚Äù</p>
<section id="key-characteristics">
<h3>Key Characteristics:<a class="headerlink" href="#key-characteristics" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Lazy Learning</strong>: K-NN is a lazy learner, meaning it doesn‚Äôt train a model immediately but instead stores the entire dataset. The classification or prediction happens during the testing phase.</p></li>
<li><p><strong>Classification Rule</strong>: It classifies a new data point based on the majority class of its nearest neighbors.</p></li>
</ul>
</section>
</section>
<section id="how-k-nn-works">
<h2>2. How K-NN Works<a class="headerlink" href="#how-k-nn-works" title="Link to this heading">#</a></h2>
<p>The algorithm works by following these steps:</p>
<ol class="arabic simple">
<li><p><strong>Calculate Distance</strong>: For a new data point, calculate the distance to all other points in the training dataset.</p></li>
<li><p><strong>Find K Nearest Neighbors</strong>: Select the <code class="docutils literal notranslate"><span class="pre">K</span></code> nearest neighbors based on the smallest distances.</p></li>
<li><p><strong>Classify the Data Point</strong>: Assign the class that is most common among the <code class="docutils literal notranslate"><span class="pre">K</span></code> nearest neighbors.</p></li>
</ol>
<section id="types-of-distance-metrics">
<h3>Types of Distance Metrics:<a class="headerlink" href="#types-of-distance-metrics" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Euclidean Distance</strong></p></li>
<li><p><strong>Manhattan Distance</strong></p></li>
<li><p><strong>Minkowski Distance</strong></p></li>
</ul>
</section>
<section id="example-scenario">
<h3>Example Scenario<a class="headerlink" href="#example-scenario" title="Link to this heading">#</a></h3>
<p>Suppose you have a dataset with two variables, and you want to classify a new data point <code class="docutils literal notranslate"><span class="pre">X</span></code> into one of two classes: ‚ÄúBlue‚Äù or ‚ÄúRed‚Äù.</p>
<ol class="arabic simple">
<li><p><strong>Step 1</strong>: Calculate the distance between <code class="docutils literal notranslate"><span class="pre">X</span></code> and all other points.</p></li>
<li><p><strong>Step 2</strong>: Find the 3 nearest points to <code class="docutils literal notranslate"><span class="pre">X</span></code>. These are marked in the diagram.</p></li>
<li><p><strong>Step 3</strong>: Classify the new point <code class="docutils literal notranslate"><span class="pre">X</span></code> based on the majority class of the 3 nearest points.</p></li>
</ol>
<p>If two out of the three nearest points belong to the ‚ÄúRed‚Äù class, and one belongs to the ‚ÄúBlue‚Äù class, then the new data point <code class="docutils literal notranslate"><span class="pre">X</span></code> will be classified as ‚ÄúRed‚Äù.</p>
</section>
</section>
<section id="id57">
<h2>3. Use Case: Iris Flower Classification<a class="headerlink" href="#id57" title="Link to this heading">#</a></h2>
<p>Let‚Äôs apply K-NN to classify the species of iris flowers based on their measurements (petal and sepal lengths and widths).</p>
<section id="id58">
<h3>Goal:<a class="headerlink" href="#id58" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Predict the species of new iris flowers based on their measurements (e.g., petal length, petal width, etc.).</p></li>
</ul>
</section>
<section id="id59">
<h3>Dataset:<a class="headerlink" href="#id59" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Known Species</strong>: Setosa, Versicolor, and Virginica.</p></li>
<li><p><strong>Features</strong>: Length and width of petals and sepals.</p></li>
</ul>
<section id="id60">
<h4>Example in Python:<a class="headerlink" href="#id60" title="Link to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import necessary libraries</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.neighbors</span><span class="w"> </span><span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">confusion_matrix</span>

<span class="c1"># Load Iris dataset</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;flower_name&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">[</span><span class="n">x</span><span class="p">])</span>

<span class="c1"># Prepare features and labels</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;target&#39;</span><span class="p">,</span> <span class="s1">&#39;flower_name&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">target</span>

<span class="c1"># Split dataset into training and testing sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="c1"># Create K-NN model with k=5</span>
<span class="n">classifier</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">classifier</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Test the model</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model accuracy:&quot;</span><span class="p">,</span> <span class="n">classifier</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>

<span class="c1"># Predict species of a new flower</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Predicted flower species:&quot;</span><span class="p">,</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">4.8</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]]))</span>

<span class="c1"># Evaluate model performance</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">classifier</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">))</span>
</pre></div>
</div>
</section>
</section>
<section id="results">
<h3>Results:<a class="headerlink" href="#results" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Accuracy</strong>: The accuracy of the model on the test set is printed.</p></li>
<li><p><strong>Prediction</strong>: Predict the species of a new iris with specific features.</p></li>
<li><p><strong>Evaluation</strong>: The performance is evaluated using a confusion matrix and classification report.</p></li>
</ul>
</section>
</section>
<section id="id61">
<h2>4. Conclusion<a class="headerlink" href="#id61" title="Link to this heading">#</a></h2>
<p>K-Nearest Neighbor is a simple yet powerful algorithm used for classification tasks. It:</p>
<ul class="simple">
<li><p>Does not require explicit training and is referred to as a <strong>lazy learner</strong>.</p></li>
<li><p>Classifies new data based on the majority class of the nearest <code class="docutils literal notranslate"><span class="pre">K</span></code> points.</p></li>
<li><p>Can be used in real-world applications such as flower species classification, handwriting recognition, and more.</p></li>
</ul>
<p>By adjusting <code class="docutils literal notranslate"><span class="pre">K</span></code> (the number of neighbors), you can tune the model for better accuracy in different scenarios.</p>
</section>
</section>
<hr class="docutils" />
<section id="machine-learning-naive-bayes-classifier">
<h1>27. Machine Learning: Na√Øve Bayes Classifier<a class="headerlink" href="#machine-learning-naive-bayes-classifier" title="Link to this heading">#</a></h1>
<section id="what-is-naive-bayes-classifier">
<h2>1. What is Na√Øve Bayes Classifier?<a class="headerlink" href="#what-is-naive-bayes-classifier" title="Link to this heading">#</a></h2>
<p><strong>Na√Øve Bayes</strong> is a <strong>supervised learning</strong> algorithm used primarily for solving <strong>classification problems</strong>. It is based on <strong>Bayes‚Äô Theorem</strong> and is particularly effective in scenarios where the features are independent of each other.</p>
<section id="id62">
<h3>Key Use Cases:<a class="headerlink" href="#id62" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Text Classification</strong>: Spam email filtering, sentiment analysis, article classification.</p></li>
<li><p><strong>Other Applications</strong>: Document classification, medical diagnosis, and more.</p></li>
</ul>
</section>
</section>
<section id="why-is-it-called-naive-bayes">
<h2>2. Why is it called Na√Øve Bayes?<a class="headerlink" href="#why-is-it-called-naive-bayes" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Na√Øve</strong>: The algorithm is called ‚Äúnaive‚Äù because it assumes that the features (or variables) are <strong>independent</strong> of each other. This assumption is often not true in real-world data, but it simplifies the calculation and still gives surprisingly good results in many cases.</p>
<ul>
<li><p>Example: In fruit classification, features like color, shape, and taste are considered independent to identify the fruit, such as recognizing an apple based on these characteristics without any dependencies between them.</p></li>
</ul>
</li>
<li><p><strong>Bayes</strong>: The ‚ÄúBayes‚Äù part comes from <strong>Bayes‚Äô Theorem</strong>, which the algorithm is based on. Bayes‚Äô Theorem helps in calculating the conditional probability of an event based on prior knowledge.</p></li>
</ul>
</section>
<section id="bayes-theorem">
<h2>3. Bayes‚Äô Theorem<a class="headerlink" href="#bayes-theorem" title="Link to this heading">#</a></h2>
<p><strong>Bayes‚Äô Theorem</strong> (or Bayes‚Äô Rule) is a fundamental concept in probability theory used to calculate the probability of a hypothesis based on prior information.</p>
<p>The formula for Bayes‚Äô Theorem is:</p>
<p>[
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
]</p>
<p>Where:</p>
<ul class="simple">
<li><p>( P(A|B) ) is the probability of hypothesis (A) given the evidence (B).</p></li>
<li><p>( P(B|A) ) is the likelihood of observing evidence (B) given hypothesis (A).</p></li>
<li><p>( P(A) ) is the prior probability of hypothesis (A).</p></li>
<li><p>( P(B) ) is the total probability of observing the evidence.</p></li>
</ul>
<section id="conditional-probability">
<h3>Conditional Probability:<a class="headerlink" href="#conditional-probability" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Conditional probability helps determine the likelihood of an event occurring, given the occurrence of another related event.</p></li>
</ul>
</section>
</section>
<section id="scenario-coin-flip-example">
<h2>4. Scenario: Coin Flip Example<a class="headerlink" href="#scenario-coin-flip-example" title="Link to this heading">#</a></h2>
<p>When flipping a coin:</p>
<ul class="simple">
<li><p>The probability of getting <strong>head</strong> or <strong>tail</strong> is 50% each.</p></li>
<li><p>This is an example of <strong>conditional probability</strong>, where each outcome depends on the coin flip.</p></li>
</ul>
</section>
<section id="use-case-titanic-survival-prediction">
<h2>5. Use Case: Titanic Survival Prediction<a class="headerlink" href="#use-case-titanic-survival-prediction" title="Link to this heading">#</a></h2>
<p>Let‚Äôs consider predicting the survival of passengers on the Titanic based on their class, sex, age, and fare.</p>
<section id="problem">
<h3>Problem:<a class="headerlink" href="#problem" title="Link to this heading">#</a></h3>
<p>Given data about passengers on the Titanic, we want to predict whether they survived or not.</p>
</section>
<section id="steps">
<h3>Steps:<a class="headerlink" href="#steps" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Load Data</strong>: Load the Titanic dataset, focusing on relevant features like passenger class, sex, age, fare, and survival status.</p></li>
<li><p><strong>Preprocessing</strong>:</p>
<ul class="simple">
<li><p>Convert categorical variables like sex into dummy variables (binary).</p></li>
<li><p>Fill missing values for the ‚ÄúAge‚Äù column with the mean value.</p></li>
</ul>
</li>
<li><p><strong>Model Training</strong>:</p>
<ul class="simple">
<li><p>Split the dataset into training and test sets.</p></li>
<li><p>Use the <strong>Gaussian Na√Øve Bayes</strong> model for classification.</p></li>
</ul>
</li>
<li><p><strong>Model Evaluation</strong>:</p>
<ul class="simple">
<li><p>Use <strong>cross-validation</strong> to evaluate the model‚Äôs performance.</p></li>
</ul>
</li>
</ol>
<section id="id63">
<h4>Example in Python:<a class="headerlink" href="#id63" title="Link to this heading">#</a></h4>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import necessary libraries</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">cross_val_score</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.naive_bayes</span><span class="w"> </span><span class="kn">import</span> <span class="n">GaussianNB</span>

<span class="c1"># Load Titanic dataset</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;titanic.csv&#39;</span><span class="p">,</span> <span class="n">usecols</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Pclass&#39;</span><span class="p">,</span> <span class="s1">&#39;Sex&#39;</span><span class="p">,</span> <span class="s1">&#39;Age&#39;</span><span class="p">,</span> <span class="s1">&#39;Fare&#39;</span><span class="p">,</span> <span class="s1">&#39;Survived&#39;</span><span class="p">])</span>

<span class="c1"># Prepare features and target variable</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s1">&#39;Survived&#39;</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;columns&#39;</span><span class="p">)</span>
<span class="n">target</span> <span class="o">=</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Survived&#39;</span><span class="p">]</span>

<span class="c1"># Convert &#39;Sex&#39; column to dummy variables</span>
<span class="n">dummies</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;Sex&#39;</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">inputs</span><span class="p">,</span> <span class="n">dummies</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;columns&#39;</span><span class="p">)</span>

<span class="c1"># Drop unnecessary columns</span>
<span class="n">inputs</span><span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s1">&#39;Sex&#39;</span><span class="p">,</span> <span class="s1">&#39;male&#39;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="s1">&#39;columns&#39;</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Fill missing &#39;Age&#39; values with mean</span>
<span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">fillna</span><span class="p">(</span><span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>

<span class="c1"># Split data into training and testing sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>

<span class="c1"># Train Gaussian Naive Bayes model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Evaluate model using cross-validation</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">GaussianNB</span><span class="p">(),</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">))</span>
</pre></div>
</div>
</section>
</section>
<section id="id64">
<h3>Key Points:<a class="headerlink" href="#id64" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Gaussian Na√Øve Bayes</strong>: A variant of Na√Øve Bayes used when features follow a normal (Gaussian) distribution.</p></li>
<li><p><strong>Cross-validation</strong>: Used to assess the model‚Äôs performance by splitting the dataset into multiple parts and training/testing the model multiple times.</p></li>
</ul>
</section>
</section>
<section id="id65">
<h2>6. Conclusion<a class="headerlink" href="#id65" title="Link to this heading">#</a></h2>
<p>The <strong>Na√Øve Bayes Classifier</strong> is a simple yet powerful algorithm for solving classification problems. It:</p>
<ul class="simple">
<li><p>Works well for text classification tasks such as spam filtering and sentiment analysis.</p></li>
<li><p>Assumes that the features are independent, which simplifies calculations but may not always be accurate in practice.</p></li>
<li><p>Can be easily implemented using libraries like <strong>scikit-learn</strong>, making it an ideal choice for many classification problems.</p></li>
</ul>
<p>Despite its simplicity, Na√Øve Bayes often performs surprisingly well, especially in applications involving large amounts of data with independent features.</p>
</section>
</section>
<hr class="docutils" />
<section id="machine-learning-gridsearchcv-randomizedsearchcv">
<h1>29. Machine Learning: GridSearchCV &amp; RandomizedSearchCV<a class="headerlink" href="#machine-learning-gridsearchcv-randomizedsearchcv" title="Link to this heading">#</a></h1>
<section id="id66">
<h2>1. Introduction<a class="headerlink" href="#id66" title="Link to this heading">#</a></h2>
<p>In machine learning, we deal with two types of model settings:</p>
<ul class="simple">
<li><p><strong>Model Parameters</strong>: Internal parameters that are automatically learned from the data.</p>
<ul>
<li><p>Example: Support Vector Machine (SVM) algorithm automatically computes its parameters during training.</p></li>
</ul>
</li>
<li><p><strong>Model Hyperparameters</strong>: These are parameters that the programmer can set before training the model. They influence the model‚Äôs performance and can be manually adjusted to improve results.</p>
<ul>
<li><p>Example: Learning rate, number of trees in a Random Forest, or the regularization parameter <code class="docutils literal notranslate"><span class="pre">C</span></code> in SVM.</p></li>
</ul>
</li>
</ul>
</section>
<section id="hyperparameter-tuning">
<h2>2. Hyperparameter Tuning<a class="headerlink" href="#hyperparameter-tuning" title="Link to this heading">#</a></h2>
<p><strong>Hyperparameter tuning</strong> is the process of finding the optimal hyperparameters for a machine learning model. Tuning these parameters correctly can significantly improve the model‚Äôs performance.</p>
<section id="common-hyperparameter-tuning-approaches">
<h3>Common Hyperparameter Tuning Approaches:<a class="headerlink" href="#common-hyperparameter-tuning-approaches" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>GridSearchCV</strong></p></li>
<li><p><strong>RandomizedSearchCV</strong></p></li>
</ol>
</section>
<section id="why-tune-hyperparameters">
<h3>Why Tune Hyperparameters?<a class="headerlink" href="#why-tune-hyperparameters" title="Link to this heading">#</a></h3>
<p>Tuning hyperparameters can result in better generalization, leading to a model that performs better on unseen data.</p>
</section>
<section id="example">
<h3>Example:<a class="headerlink" href="#example" title="Link to this heading">#</a></h3>
<p>For a Random Forest classifier, you might want to tune the number of trees (<code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>) and the maximum depth (<code class="docutils literal notranslate"><span class="pre">max_depth</span></code>) of each tree.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="kernel-functions-for-svm">
<h2>3. Kernel Functions (for SVM)<a class="headerlink" href="#kernel-functions-for-svm" title="Link to this heading">#</a></h2>
<p>SVM algorithms use mathematical functions called <strong>kernels</strong> to transform data into a required form. Common kernel types include:</p>
<ul class="simple">
<li><p><strong>Linear</strong>: Used for linearly separable data.</p></li>
<li><p><strong>Nonlinear</strong>: Used for complex data structures.</p></li>
<li><p><strong>Polynomial</strong>: Models complex, curved decision boundaries.</p></li>
<li><p><strong>Radial Basis Function (RBF)</strong>: A popular choice for non-linear data.</p></li>
<li><p><strong>Sigmoid</strong>: A function used in neural networks.</p></li>
</ul>
</section>
<section id="ways-to-tune-hyperparameters">
<h2>4. Ways to Tune Hyperparameters<a class="headerlink" href="#ways-to-tune-hyperparameters" title="Link to this heading">#</a></h2>
<section id="approach-1-manual-tuning-with-train-test-split">
<h3>Approach 1: Manual Tuning with <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code><a class="headerlink" href="#approach-1-manual-tuning-with-train-test-split" title="Link to this heading">#</a></h3>
<p>You can manually adjust hyperparameters by trial and error using <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code>.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.svm</span><span class="w"> </span><span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Create and train model with manually selected parameters</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="approach-2-k-fold-cross-validation">
<h3>Approach 2: K-Fold Cross Validation<a class="headerlink" href="#approach-2-k-fold-cross-validation" title="Link to this heading">#</a></h3>
<p>K-fold cross-validation splits the data into K parts, using each part for testing while training on the remaining parts. This helps evaluate the model‚Äôs performance and avoid overfitting.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.svm</span><span class="w"> </span><span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_iris</span>

<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>

<span class="c1"># Try different kernel and C values</span>
<span class="n">sc1</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">),</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">sc2</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">),</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">sc3</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">),</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="approach-3-gridsearchcv">
<h3>Approach 3: GridSearchCV<a class="headerlink" href="#approach-3-gridsearchcv" title="Link to this heading">#</a></h3>
<p><strong>GridSearchCV</strong> automates the process of hyperparameter tuning by exhaustively searching through a predefined set of hyperparameters. It evaluates every combination of hyperparameters and selects the best performing one.</p>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.svm</span><span class="w"> </span><span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_iris</span>

<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>

<span class="c1"># Define the hyperparameter grid</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span>
    <span class="s1">&#39;kernel&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="s1">&#39;linear&#39;</span><span class="p">]</span>
<span class="p">}</span>

<span class="c1"># Initialize GridSearchCV with cross-validation</span>
<span class="n">clf</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">),</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="c1"># Fit the model and find the best parameters</span>
<span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>

<span class="c1"># Print the best parameters and score</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best Parameters:&quot;</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best Score:&quot;</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="randomizedsearchcv">
<h2>5. RandomizedSearchCV<a class="headerlink" href="#randomizedsearchcv" title="Link to this heading">#</a></h2>
<p>While <strong>GridSearchCV</strong> tries all combinations of hyperparameters, <strong>RandomizedSearchCV</strong> selects a random combination of hyperparameters, which can help reduce computation time when dealing with large search spaces.</p>
<section id="when-to-use-randomizedsearchcv">
<h3>When to Use RandomizedSearchCV?<a class="headerlink" href="#when-to-use-randomizedsearchcv" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>When there are many hyperparameters and possible values to test.</p></li>
<li><p>When training time is long and computational resources are limited.</p></li>
</ul>
<p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">RandomizedSearchCV</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.svm</span><span class="w"> </span><span class="kn">import</span> <span class="n">SVC</span>

<span class="c1"># Define the hyperparameter distribution</span>
<span class="n">param_dist</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">],</span>
    <span class="s1">&#39;kernel&#39;</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="s1">&#39;linear&#39;</span><span class="p">]</span>
<span class="p">}</span>

<span class="c1"># Initialize RandomizedSearchCV</span>
<span class="n">rs</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">SVC</span><span class="p">(</span><span class="n">gamma</span><span class="o">=</span><span class="s1">&#39;auto&#39;</span><span class="p">),</span> <span class="n">param_dist</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">return_train_score</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># Fit the model and find the best parameters</span>
<span class="n">rs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>

<span class="c1"># Print the best parameters</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best Parameters:&quot;</span><span class="p">,</span> <span class="n">rs</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="key-benefits-of-randomizedsearchcv">
<h3>Key Benefits of RandomizedSearchCV:<a class="headerlink" href="#key-benefits-of-randomizedsearchcv" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Faster</strong>: It does not evaluate all combinations, which reduces computation time.</p></li>
<li><p><strong>Flexible</strong>: You can specify the number of iterations (<code class="docutils literal notranslate"><span class="pre">n_iter</span></code>), allowing for trade-offs between computation and exploration.</p></li>
</ul>
</section>
</section>
<section id="id67">
<h2>6. Conclusion<a class="headerlink" href="#id67" title="Link to this heading">#</a></h2>
<p>Both <strong>GridSearchCV</strong> and <strong>RandomizedSearchCV</strong> are powerful techniques for hyperparameter tuning, but each has its use cases:</p>
<ul class="simple">
<li><p><strong>GridSearchCV</strong> is exhaustive and provides the most thorough search, making it ideal when you have a smaller search space.</p></li>
<li><p><strong>RandomizedSearchCV</strong> is faster and useful when you have a large search space or limited computational resources.</p></li>
</ul>
<p>By properly tuning hyperparameters, you can significantly improve the performance of your machine learning models.</p>
</section>
</section>
<hr class="docutils" />
<section id="machine-learning-xgboost">
<h1>30. Machine Learning: XGBoost<a class="headerlink" href="#machine-learning-xgboost" title="Link to this heading">#</a></h1>
<section id="introduction-to-xgboost">
<h2>1. Introduction to XGBoost<a class="headerlink" href="#introduction-to-xgboost" title="Link to this heading">#</a></h2>
<p><strong>XGBoost</strong> (eXtreme Gradient Boosting) is an <strong>ensemble machine learning algorithm</strong> that builds decision trees in a gradient boosting framework. It combines the predictions from multiple models to improve performance, especially for structured/tabular data.</p>
<section id="key-features">
<h3>Key Features:<a class="headerlink" href="#key-features" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Execution Speed</strong>: XGBoost is designed to be highly efficient and fast.</p></li>
<li><p><strong>Model Performance</strong>: It generally provides superior predictive performance, making it a top choice for many machine learning tasks.</p></li>
</ul>
</section>
</section>
<section id="why-use-xgboost">
<h2>2. Why Use XGBoost?<a class="headerlink" href="#why-use-xgboost" title="Link to this heading">#</a></h2>
<p>XGBoost is widely used because of its:</p>
<ol class="arabic simple">
<li><p><strong>Execution Speed</strong>: It is optimized for speed and can handle large datasets efficiently.</p></li>
<li><p><strong>Model Performance</strong>: XGBoost often delivers high accuracy and better generalization compared to other models.</p></li>
</ol>
</section>
<section id="installing-xgboost">
<h2>3. Installing XGBoost<a class="headerlink" href="#installing-xgboost" title="Link to this heading">#</a></h2>
<p>You can install XGBoost using pip:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>xgboost
</pre></div>
</div>
</section>
<section id="id68">
<h2>4. Dataset Overview<a class="headerlink" href="#id68" title="Link to this heading">#</a></h2>
<p>We will work with the <strong>Pima Indians Diabetes Dataset</strong>.</p>
<section id="dataset-description">
<h3>Dataset Description:<a class="headerlink" href="#dataset-description" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>This dataset is related to healthcare and describes medical records of Pima Indians, including whether they developed diabetes within five years.</p></li>
<li><p>The dataset includes Pima Indian females aged 21 and older.</p></li>
<li><p>It is a <strong>binary classification problem</strong>: The output variable is <code class="docutils literal notranslate"><span class="pre">1</span></code> if the person has diabetes, and <code class="docutils literal notranslate"><span class="pre">0</span></code> if they do not.</p></li>
</ul>
</section>
<section id="id69">
<h3>Features:<a class="headerlink" href="#id69" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>All input variables are numerical.</p></li>
</ul>
</section>
</section>
<section id="id70">
<h2>5. Input and Output Variables<a class="headerlink" href="#id70" title="Link to this heading">#</a></h2>
<section id="input-variables-x">
<h3>Input Variables (X):<a class="headerlink" href="#input-variables-x" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p>Number of times pregnant</p></li>
<li><p>Plasma glucose concentration at 2 hours in an oral glucose tolerance test</p></li>
<li><p>Diastolic blood pressure (mm Hg)</p></li>
<li><p>Triceps skin fold thickness (mm)</p></li>
<li><p>2-hour serum insulin (ŒºIU/ml)</p></li>
<li><p>Body mass index (weight in kg / height in m)</p></li>
<li><p>Diabetes pedigree function</p></li>
<li><p>Age (years)</p></li>
</ol>
</section>
<section id="output-variable-y">
<h3>Output Variable (y):<a class="headerlink" href="#output-variable-y" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Class variable</strong> (0 or 1): 1 if the person has diabetes, 0 if not.</p></li>
</ul>
</section>
</section>
<section id="data-preprocessing-and-model-training">
<h2>6. Data Preprocessing and Model Training<a class="headerlink" href="#data-preprocessing-and-model-training" title="Link to this heading">#</a></h2>
<section id="loading-the-dataset">
<h3>Loading the Dataset:<a class="headerlink" href="#loading-the-dataset" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="kn">import</span> <span class="n">loadtxt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">xgboost</span><span class="w"> </span><span class="kn">import</span> <span class="n">XGBClassifier</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="c1"># Load the dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">loadtxt</span><span class="p">(</span><span class="s1">&#39;pima-indians-diabetes.csv&#39;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>

<span class="c1"># Split the dataset into input (X) and output (y)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">8</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[:,</span> <span class="mi">8</span><span class="p">]</span>

<span class="c1"># Split the data into training and testing sets</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">7</span>
<span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.33</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">test_size</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

<span class="c1"># Train the XGBoost model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Make predictions and evaluate accuracy</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">%.2f%%</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">accuracy</span> <span class="o">*</span> <span class="mf">100.0</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="explanation">
<h3>Explanation:<a class="headerlink" href="#explanation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>train_test_split</strong>: Splits the dataset into training and testing sets.</p></li>
<li><p><strong>XGBClassifier</strong>: XGBoost classifier for classification tasks.</p></li>
<li><p><strong>accuracy_score</strong>: Evaluates the accuracy of the model on the test set.</p></li>
</ul>
</section>
</section>
<section id="label-encoding-for-categorical-data">
<h2>7. Label Encoding for Categorical Data<a class="headerlink" href="#label-encoding-for-categorical-data" title="Link to this heading">#</a></h2>
<p>In some cases, the output variable might be a string. For example, the <strong>Iris flowers classification</strong> problem uses string class labels. XGBoost requires numerical labels, so we use <strong>Label Encoding</strong> to convert string values to integers.</p>
<section id="example-of-label-encoding">
<h3>Example of Label Encoding:<a class="headerlink" href="#example-of-label-encoding" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="kn">import</span> <span class="n">read_csv</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">LabelEncoder</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">xgboost</span><span class="w"> </span><span class="kn">import</span> <span class="n">XGBClassifier</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span>

<span class="c1"># Load the dataset</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;iris.csv&#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">values</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">4</span><span class="p">]</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[:,</span> <span class="mi">4</span><span class="p">]</span>

<span class="c1"># Encoding flower names into integers</span>
<span class="n">label_encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span>
<span class="n">label_encoded_y</span> <span class="o">=</span> <span class="n">label_encoder</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">Y</span><span class="p">)</span>

<span class="c1"># Split the dataset into training and testing sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">label_encoded_y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.33</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>

<span class="c1"># Train the XGBoost model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Make predictions and evaluate accuracy</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">%.2f%%</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">accuracy</span> <span class="o">*</span> <span class="mf">100.0</span><span class="p">))</span>
</pre></div>
</div>
</section>
<section id="id71">
<h3>Explanation:<a class="headerlink" href="#id71" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>LabelEncoder</strong>: Converts categorical string labels into numeric values (e.g., ‚ÄúIris-setosa‚Äù to 0, ‚ÄúIris-versicolor‚Äù to 1, etc.).</p></li>
</ul>
</section>
</section>
<section id="feature-importance-with-xgboost">
<h2>8. Feature Importance with XGBoost<a class="headerlink" href="#feature-importance-with-xgboost" title="Link to this heading">#</a></h2>
<p>XGBoost helps to identify the most important features in the dataset. By examining feature importances, you can understand which features have the most influence on the model‚Äôs predictions.</p>
<section id="example-of-feature-importance">
<h3>Example of Feature Importance:<a class="headerlink" href="#example-of-feature-importance" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="kn">import</span> <span class="n">loadtxt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">xgboost</span><span class="w"> </span><span class="kn">import</span> <span class="n">XGBClassifier</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">matplotlib</span><span class="w"> </span><span class="kn">import</span> <span class="n">pyplot</span>

<span class="c1"># Load the dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">loadtxt</span><span class="p">(</span><span class="s1">&#39;pima-indians-diabetes.csv&#39;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s1">&#39;,&#39;</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">8</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">[:,</span> <span class="mi">8</span><span class="p">]</span>

<span class="c1"># Train the XGBoost model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">XGBClassifier</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Print feature importance</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">)</span>

<span class="c1"># Plot the feature importance</span>
<span class="n">r</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">))</span>
<span class="n">f_imp</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">feature_importances_</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">f_imp</span><span class="p">)</span>
<span class="n">pyplot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="id72">
<h3>Explanation:<a class="headerlink" href="#id72" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>model.feature_importances_</strong>: Returns the importance score of each feature.</p></li>
<li><p><strong>pyplot.bar</strong>: Visualizes the feature importance using a bar chart.</p></li>
</ul>
</section>
</section>
<section id="id73">
<h2>9. Conclusion<a class="headerlink" href="#id73" title="Link to this heading">#</a></h2>
<p>XGBoost is a powerful and efficient machine learning algorithm used for classification and regression tasks. Key advantages include:</p>
<ul class="simple">
<li><p><strong>High Execution Speed</strong>: Optimized for fast performance.</p></li>
<li><p><strong>Superior Model Performance</strong>: Tends to perform well across a wide range of problems, particularly for structured/tabular data.</p></li>
<li><p><strong>Feature Importance</strong>: XGBoost provides useful insights into the importance of each feature in the dataset.</p></li>
</ul>
<p>By leveraging XGBoost, you can achieve great results in many machine learning applications, especially in classification problems like predicting diabetes or classifying flowers.</p>
</section>
</section>
<hr class="docutils" />
<section id="pickling-and-unpickling-in-python">
<h1>31. Pickling and Unpickling in Python<a class="headerlink" href="#pickling-and-unpickling-in-python" title="Link to this heading">#</a></h1>
<section id="pickling">
<h2>1. Pickling<a class="headerlink" href="#pickling" title="Link to this heading">#</a></h2>
<p>Pickling is the process of converting an object‚Äôs state into a format that can be stored in a file. This allows you to save objects and later retrieve them.</p>
<section id="id74">
<h3>Key Points:<a class="headerlink" href="#id74" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Pickling</strong> refers to writing the state of an object to a file.</p></li>
<li><p>Python‚Äôs <code class="docutils literal notranslate"><span class="pre">pickle</span></code> module is used for pickling and unpickling.</p></li>
<li><p>The function to pickle an object is <code class="docutils literal notranslate"><span class="pre">pickle.dump(object,</span> <span class="pre">file)</span></code>.</p></li>
</ul>
</section>
<section id="example-pickling-an-object">
<h3>Example: Pickling an Object<a class="headerlink" href="#example-pickling-an-object" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pickle</span>

<span class="k">class</span><span class="w"> </span><span class="nc">Employee</span><span class="p">:</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">emp_id</span><span class="p">,</span> <span class="n">name</span><span class="p">,</span> <span class="n">salary</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emp_id</span> <span class="o">=</span> <span class="n">emp_id</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">salary</span> <span class="o">=</span> <span class="n">salary</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">display</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Employee ID: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">emp_id</span><span class="si">}</span><span class="s2">, Name: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">, Salary: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">salary</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Pickling the object</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;emp.dat&quot;</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">Employee</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="s2">&quot;Daniel&quot;</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
    <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">e</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Pickling of Employee Object completed...</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="unpickling">
<h2>2. Unpickling<a class="headerlink" href="#unpickling" title="Link to this heading">#</a></h2>
<p>Unpickling is the reverse process of pickling, where you read the object‚Äôs state from a file and reconstruct it in memory.</p>
<section id="id75">
<h3>Key Points:<a class="headerlink" href="#id75" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Unpickling</strong> refers to reading the state of an object from a file.</p></li>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">pickle.load(file)</span></code> to unpickle an object.</p></li>
</ul>
</section>
<section id="example-unpickling-an-object">
<h3>Example: Unpickling an Object<a class="headerlink" href="#example-unpickling-an-object" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Unpickling the object</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;emp.dat&quot;</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">obj</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Printing Employee Information after unpickling:&quot;</span><span class="p">)</span>
    <span class="n">obj</span><span class="o">.</span><span class="n">display</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
</section>
<hr class="docutils" />
<section id="saving-models-in-machine-learning">
<h1>Saving Models in Machine Learning<a class="headerlink" href="#saving-models-in-machine-learning" title="Link to this heading">#</a></h1>
<p>In machine learning, saving trained models is important to avoid retraining them every time. There are two common ways to save models in Python:</p>
<ol class="arabic simple">
<li><p><strong>Pickling</strong></p></li>
<li><p><strong>Joblib</strong></p></li>
</ol>
<section id="saving-models-with-pickling">
<h2>3. Saving Models with Pickling<a class="headerlink" href="#saving-models-with-pickling" title="Link to this heading">#</a></h2>
<p>Pickling allows you to save a trained model to a file. This process converts the model into a byte stream and stores it.</p>
<section id="example-saving-and-loading-a-model-using-pickling">
<h3>Example: Saving and Loading a Model using Pickling<a class="headerlink" href="#example-saving-and-loading-a-model-using-pickling" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">pickle</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="c1"># Train the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">new_df</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">price</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="c1"># Save the model using pickling</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;model_pickle&#39;</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">file</span><span class="p">)</span>

<span class="c1"># Load the model using pickling</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;model_pickle&#39;</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
    <span class="n">model1</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">model1</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">5000</span><span class="p">]]))</span>
</pre></div>
</div>
</section>
</section>
<section id="saving-models-with-joblib">
<h2>4. Saving Models with Joblib<a class="headerlink" href="#saving-models-with-joblib" title="Link to this heading">#</a></h2>
<p><strong>Joblib</strong> is another efficient way of saving large machine learning models, particularly when the model involves NumPy arrays (which are common in ML models).</p>
<section id="example-saving-and-loading-a-model-using-joblib">
<h3>Example: Saving and Loading a Model using Joblib<a class="headerlink" href="#example-saving-and-loading-a-model-using-joblib" title="Link to this heading">#</a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">joblib</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="c1"># Train the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">new_df</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">price</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>

<span class="c1"># Save the model using joblib</span>
<span class="n">joblib</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;model_joblib&#39;</span><span class="p">)</span>

<span class="c1"># Load the model using joblib</span>
<span class="n">mj</span> <span class="o">=</span> <span class="n">joblib</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;model_joblib&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">mj</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">5000</span><span class="p">]]))</span>
</pre></div>
</div>
</section>
</section>
</section>
<hr class="docutils" />
<section id="id76">
<h1>Summary<a class="headerlink" href="#id76" title="Link to this heading">#</a></h1>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Operation</p></th>
<th class="head"><p>Method</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Pickling</strong></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">pickle.dump()</span></code></p></td>
<td><p>Serialize an object and save it to a file.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Unpickling</strong></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">pickle.load()</span></code></p></td>
<td><p>Load and deserialize the object from a file.</p></td>
</tr>
<tr class="row-even"><td><p><strong>Save with Joblib</strong></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">joblib.dump()</span></code></p></td>
<td><p>Save models efficiently, especially for large models.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Load with Joblib</strong></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">joblib.load()</span></code></p></td>
<td><p>Load models saved with Joblib.</p></td>
</tr>
</tbody>
</table>
</div>
<p>Pickling and Joblib provide useful ways of saving and loading machine learning models, reducing the need for retraining and improving efficiency.</p>
<hr class="docutils" />
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./contents\7_revision"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">1. Introduction to Machine Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-machine-learning">What is Machine Learning?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-program">What is a Program?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-an-algorithm">What is an Algorithm?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-machine-learning-algorithm">What is a Machine Learning Algorithm?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#differences-between-machine-learning-and-normal-algorithms">Differences Between Machine Learning and Normal Algorithms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-machine-learning">Why Machine Learning?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-use-cases">Key Use Cases:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#definitions-of-machine-learning">Definitions of Machine Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#arthur-samuel-s-definition-1959">Arthur Samuel‚Äôs Definition (1959)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#artificial-intelligence-vs-machine-learning-vs-deep-learning">Artificial Intelligence vs Machine Learning vs Deep Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning">Machine Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning">Deep Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#artificial-intelligence-ai">Artificial Intelligence (AI)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#human-vs-computer-decision-making">Human vs Computer Decision-Making</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-machines-think">How Do Machines ‚ÄúThink‚Äù?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#real-time-examples-of-machine-learning">Real-Time Examples of Machine Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gmail-spam-filter">Gmail Spam Filter</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#banking-loan-application-credit-card-approval">Banking Loan Application / Credit Card Approval</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-terminology">2. Machine Learning Terminology</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Machine Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model">Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#training-a-model">Training a Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#testing-the-model">Testing the Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-deployment">Model Deployment</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normal-vs-smart-applications">Normal vs Smart Applications</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#normal-application">Normal Application</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#smart-application">Smart Application</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-use-case">Example Use Case:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy-testing">Accuracy Testing</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#data-machine-learning-algorithm-terminology">3. Data &amp; Machine Learning Algorithm Terminology</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data">Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-in-tables">Data in Tables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-components-of-a-table">Key Components of a Table</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#statistical-learning-perspective">Statistical Learning Perspective</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#input-and-output-variables">Input and Output Variables</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#terminology-in-statistics">Terminology in Statistics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#computer-science-perspective">Computer Science Perspective</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#models-and-algorithms">Models and Algorithms</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-learning-a-function">4. Machine Learning: Learning a Function</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-a-function">Learning a Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#purpose-of-the-learning-function">Purpose of the Learning Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-algorithms">Machine Learning Algorithms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-machine-learning-models">5. Types of Machine Learning Models</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#features-and-labels">Features and Labels</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#features">Features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#label">Label</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#label-example">Label Example</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#labelled-and-unlabelled-data">Labelled and Unlabelled Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#labelled-data">Labelled Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unlabelled-data">Unlabelled Data</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supervised-vs-unsupervised-learning">Supervised vs Unsupervised Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#supervised-learning">Supervised Learning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unsupervised-learning">Unsupervised Learning</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-supervised-learning-models">Types of Supervised Learning Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-models">1. Regression Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-models">2. Classification Models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-unsupervised-learning-models">Types of Unsupervised Learning Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering">1. Clustering</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dimensionality-reduction">2. Dimensionality Reduction</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-life-cycle">6. Machine Learning Life Cycle</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-collection">1. Data Collection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preparation">2. Data Preparation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-wrangling">3. Data Wrangling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#train-the-model">4. Train the Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#test-the-model">5. Test the Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">6. Model Deployment</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Summary</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#train-test-datasets-in-machine-learning">7. Train &amp; Test Datasets in Machine Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-datasets">Types of Datasets</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-dataset">1. Train Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#test-dataset">2. Test Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#validation-dataset-optional">3. Validation Dataset (Optional)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deciding-the-size-of-datasets">Deciding the Size of Datasets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-train-test-split-p-function">Using <code class="docutils literal notranslate"><span class="pre">train_test_split(p)</span></code> Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-train-test-split-p-random-state-0-function">Using <code class="docutils literal notranslate"><span class="pre">train_test_split(p,</span> <span class="pre">random_state=0)</span></code> Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Summary</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#r-value-and-regression-analysis">8. R-Value and Regression Analysis</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-analysis">Regression Analysis</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-a-line-in-regression">Understanding a Line in Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#goal-of-linear-regression">Goal of Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#can-we-use-regression-everywhere">Can We Use Regression Everywhere?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#r-value-understanding-the-strength-of-relationship">R-Value: Understanding the Strength of Relationship</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#r-value-range">R-Value Range</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#calculating-the-r-value">Calculating the R-Value</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#interpreting-r-value-results">Interpreting R-Value Results</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Summary</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#simple-linear-regression">Simple Linear Regression</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-linear-regression">What is Linear Regression?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-linear-regression">Types of Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">1. Simple Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multiple-linear-regression">2. Multiple Linear Regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-formula">Linear Regression Formula</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-simple-linear-regression-in-python">Example: Simple Linear Regression in Python</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-calculation">Example Calculation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-best-fitted-line">Visualizing the Best Fitted Line</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Summary</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-example">Linear Regression Example</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scenario">Scenario</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-mathematics-behind-it">Understanding the Mathematics Behind It</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-linear-regression-works">How Linear Regression Works:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementing-linear-regression-in-python">Implementing Linear Regression in Python</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#important-information">Important Information</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#making-predictions">Making Predictions</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-y-pred">What is <code class="docutils literal notranslate"><span class="pre">y_pred</span></code>?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-actual-vs-predicted-values">Comparing Actual vs. Predicted Values</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#evaluating-the-model">Evaluating the Model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-evaluation-metrics-for-regression">Common Evaluation Metrics for Regression:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Summary</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-example-salary-prediction">9 Linear Regression Example - Salary Prediction</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-importing-required-libraries">Step 1: Importing Required Libraries</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-loading-the-dataset">Step 2: Loading the Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-splitting-the-dataset">Step 3: Splitting the Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-training-the-model">Step 4: Training the Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-making-predictions">Step 5: Making Predictions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-6-visualizing-the-results">Step 6: Visualizing the Results</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-dataset-visualization"><strong>Training Dataset Visualization</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#test-dataset-visualization"><strong>Test Dataset Visualization</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Summary</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">10. Multiple Linear Regression</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-statement">Problem Statement</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#given-the-following-home-details-we-need-to-predict-their-prices">Given the following home details, we need to predict their prices:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset">Dataset</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#formula-for-multiple-linear-regression">Formula for Multiple Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#step-by-step-implementation">Step-by-Step Implementation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-import-required-libraries"><strong>Step 1: Import Required Libraries</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-load-the-dataset"><strong>Step 2: Load the Dataset</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-handling-missing-values"><strong>Step 3: Handling Missing Values</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-preparing-data-for-model-training"><strong>Step 4: Preparing Data for Model Training</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-train-the-model"><strong>Step 5: Train the Model</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-6-retrieve-model-coefficients"><strong>Step 6: Retrieve Model Coefficients</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Summary</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#polynomial-features-in-machine-learning">11. Polynomial Features in Machine Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">Introduction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-polynomial-features"><strong>What Are Polynomial Features?</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-polynomial-features"><strong>Why Do We Need Polynomial Features?</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-models-vs-non-linear-data"><strong>Linear Models vs. Non-Linear Data</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-equations"><strong>Mathematical Equations</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13"><strong>Simple Linear Regression</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14"><strong>Multiple Linear Regression</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id15"><strong>Step-by-Step Implementation</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id16"><strong>Step 1: Import Required Libraries</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id17"><strong>Step 2: Load the Dataset</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-data-preparation"><strong>Step 3: Data Preparation</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-visualizing-the-data"><strong>Step 4: Visualizing the Data</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-training-a-linear-regression-model"><strong>Step 5: Training a Linear Regression Model</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-6-applying-polynomial-features"><strong>Step 6: Applying Polynomial Features</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-7-plotting-polynomial-regression"><strong>Step 7: Plotting Polynomial Regression</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#predictions"><strong>Predictions</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id18"><strong>Conclusion</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-functions-in-machine-learning">12. Cost Functions in Machine Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id19"><strong>Introduction</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-cost-functions"><strong>Why Do We Need Cost Functions?</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-a-cost-function-work"><strong>How Does a Cost Function Work?</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#goal-of-training"><strong>Goal of Training</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-functions-for-regression">13. <strong>Cost Functions for Regression</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id20"><strong>Introduction</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-regression-metrics"><strong>Types of Regression Metrics</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distance-based-error"><strong>Distance-Based Error</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error-mse"><strong>1. Mean Squared Error (MSE)</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation"><strong>Implementation:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#root-mean-squared-error-rmse"><strong>2. Root Mean Squared Error (RMSE)</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#formula"><strong>Formula:</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id21"><strong>Implementation:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-absolute-error-mae"><strong>3. Mean Absolute Error (MAE)</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id22"><strong>Implementation:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#choosing-the-right-regression-metric"><strong>Choosing the Right Regression Metric</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#handling-categorical-data-dummy-variables-one-hot-encoding">14. <strong>Handling Categorical Data: Dummy Variables &amp; One-Hot Encoding</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id23"><strong>Introduction</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-can-t-we-directly-convert-text-to-numbers"><strong>Why Can‚Äôt We Directly Convert Text to Numbers?</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-dummy-variables"><strong>Using Dummy Variables</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id24"><strong>Implementation:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id25"><strong>Making Predictions</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-house-price-in-vijayawada"><strong>Predicting House Price in Vijayawada</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-house-price-in-guntur"><strong>Predicting House Price in Guntur</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#predicting-house-price-in-gudiwada"><strong>Predicting House Price in Gudiwada</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id26"><strong>Conclusion</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-in-machine-learning">15. <strong>Gradient Descent in Machine Learning</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id27"><strong>Introduction</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-gradient-descent-works"><strong>How Gradient Descent Works</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-convergence"><strong>Understanding Convergence</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#process-behind-gradient-descent"><strong>Process Behind Gradient Descent</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-gradient-descent-process"><strong>Visualizing the Gradient Descent Process</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#steps-in-gradient-descent"><strong>Steps in Gradient Descent</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-gradient-descent-approaches"><strong>Types of Gradient Descent Approaches</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fixed-step-size-approach"><strong>Fixed Step Size Approach</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-reaching-minimum-error"><strong>Learning Rate: Reaching Minimum Error</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#effect-of-small-large-learning-rates"><strong>Effect of Small &amp; Large Learning Rates</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id28"><strong>Conclusion</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-in-machine-learning">16. <strong>Logistic Regression in Machine Learning</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id29"><strong>Introduction</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-logistic-regression"><strong>Types of Logistic Regression</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-classification"><strong>1. Binary Classification</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multiclass-classification"><strong>2. Multiclass Classification</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-problem"><strong>Understanding the Problem</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-overview"><strong>Dataset Overview</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id30"><strong>Problem Statement</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-function-sigmoid"><strong>Logistic Function (Sigmoid)</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-in-python"><strong>Implementation in Python</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#load-the-dataset"><strong>1. Load the Dataset</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#splitting-the-data"><strong>2. Splitting the Data</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-the-model"><strong>3. Training the Model</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id31"><strong>4. Making Predictions</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-performance"><strong>5. Model Performance</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prediction-insights"><strong>Prediction Insights</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id32"><strong>Conclusion</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-multiclass-classification"><strong>Logistic Regression - Multiclass Classification</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id33"><strong>Introduction</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-example-handwritten-digit-recognition"><strong>Practical Example: Handwritten Digit Recognition</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id34"><strong>Problem Statement</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id35"><strong>Implementation in Python</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id36"><strong>1. Load the Dataset</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#visualizing-the-digits"><strong>2. Visualizing the Digits</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#check-target-labels"><strong>3. Check Target Labels</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id37"><strong>Training the Model</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#splitting-the-dataset"><strong>1. Splitting the Dataset</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-the-logistic-regression-model"><strong>2. Training the Logistic Regression Model</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-evaluation-predictions"><strong>Model Evaluation &amp; Predictions</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#checking-accuracy"><strong>1. Checking Accuracy</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id38"><strong>2. Making Predictions</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id39"><strong>Conclusion</strong></a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-decision-tree">18. Machine Learning: Decision Tree</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-decision-trees">1. Introduction to Decision Trees</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-decision-tree">2. What is a Decision Tree?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#components-of-a-decision-tree">Components of a Decision Tree:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cart-algorithm">3. CART Algorithm</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-decision-trees">4. Why Use Decision Trees?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-decision-tree-terminologies">5. Key Decision Tree Terminologies</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-the-decision-tree-algorithm-work">6. How Does the Decision Tree Algorithm Work?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-a-decision-tree">7. Example of a Decision Tree</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-implementation-decision-tree-in-python">8. Practical Implementation: Decision Tree in Python</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id40">9. Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-confusion-matrix">19. Machine Learning: Confusion Matrix</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-confusion-matrix">1. Introduction to Confusion Matrix</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-confusion-matrix">2. What is a Confusion Matrix?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-terms">3. Understanding the Terms</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#type-i-and-type-ii-errors">4. Type I and Type II Errors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#representing-the-confusion-matrix">5. Representing the Confusion Matrix</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#performance-metrics">6. Performance Metrics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy">Accuracy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recall-sensitivity-true-positive-rate">Recall (Sensitivity / True Positive Rate)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#specificity-true-negative-rate">Specificity (True Negative Rate)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#precision">Precision</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#f1-score">F1 Score</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id41">7. Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-bias-variance-trade-off">20. Machine Learning: Bias-Variance Trade-Off</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id42">1. Introduction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-definitions">Key Definitions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-goal-of-supervised-learning">2. The Goal of Supervised Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-prediction-error">3. Understanding Prediction Error</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-in-machine-learning">4. Bias in Machine Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-low-and-high-bias-models">Examples of Low and High Bias Models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#variance-in-machine-learning">5. Variance in Machine Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-low-and-high-variance-models">Examples of Low and High Variance Models</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bias-variance-trade-off">6. Bias-Variance Trade-Off</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#configuring-the-bias-variance-trade-off">7. Configuring the Bias-Variance Trade-Off</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1-k-nearest-neighbors-k-nn">Example 1: k-Nearest Neighbors (K-NN)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-2-support-vector-machines-svm">Example 2: Support Vector Machines (SVM)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-relationship-between-bias-and-variance">8. The Relationship Between Bias and Variance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-error">9. Types of Error</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scenarios-of-bias-and-variance">Scenarios of Bias and Variance:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-bias-variance-decomposition-with-python">10. Example: Bias-Variance Decomposition with Python</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-example">Code Example:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id43">11. Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-random-forest-algorithm">21. Machine Learning: Random Forest Algorithm</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-random-forest">1. Introduction to Random Forest</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-points">Key Points:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-ensemble-learning">2. What is Ensemble Learning?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-random-forest-work">3. How Does Random Forest Work?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-steps-in-random-forest">Key Steps in Random Forest:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-random-forest">Why Use Random Forest?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#steps-in-random-forest-algorithm">4. Steps in Random Forest Algorithm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-by-step-process">Step-by-Step Process:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-use-case-iris-flower-classification">5. Example Use Case: Iris Flower Classification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#goal">Goal:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-code">Example Code:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-parameters">Key Parameters:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id44">6. Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-support-vector-machine-svm">22. Machine Learning: Support Vector Machine (SVM)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-support-vector-machine-svm">1. Introduction to Support Vector Machine (SVM)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id45">Key Points:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-boundary-in-svm">2. Decision Boundary in SVM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-support-vector-machine">3. Why ‚ÄúSupport Vector Machine‚Äù?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-uses-of-svm">4. Common Uses of SVM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-svm">5. Types of SVM</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-svm">Linear SVM</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#non-linear-svm">Non-linear SVM</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperplane-and-dimensions">6. Hyperplane and Dimensions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-svm-work">7. How Does SVM Work?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-svm-example">Linear SVM Example:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperplane-and-support-vectors">Hyperplane and Support Vectors</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#use-case-iris-flower-classification">8. Use Case: Iris Flower Classification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id46">Example Code:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id47">Key Parameters:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id48">9. Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-overfitting-and-underfitting">23. Machine Learning: Overfitting and Underfitting</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id49">1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts">2. Key Concepts</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#noise">Noise</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#bias">Bias</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variance">Variance</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting">3. Overfitting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#underfitting">4. Underfitting</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#good-fit-model">5. Good Fit Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#good-fit-example">6. Good Fit Example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparison">Comparison:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization">7. Regularization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-regularization">Types of Regularization:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#pros-and-cons-of-regularization">Pros and Cons of Regularization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id50">8. Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-lasso-ridge-regression">24. Machine Learning: Lasso &amp; Ridge Regression</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-linear-regression">1. Introduction to Linear Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-regression">2. Lasso Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id51">Key Points:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-avoid-overfitting-with-lasso">How to Avoid Overfitting with Lasso:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-regression-in-practice">Lasso Regression in Practice:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regression">3. Ridge Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id52">Key Points:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regression-in-practice">Ridge Regression in Practice:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-lasso-and-ridge-regression">4. Comparing Lasso and Ridge Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-melbourne-housing-market">5. Dataset: Melbourne Housing Market</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-details">Dataset Details:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preprocessing">Data Preprocessing:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id53">Splitting the Data:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression-model">Linear Regression Model:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-interpretation">Model Interpretation:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id54">6. Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-k-means-clustering">25. Machine Learning: K-Means Clustering</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-clustering">1. What is Clustering?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#k-means-clustering-algorithm">2. K-Means Clustering Algorithm</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id55">Key Concepts:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#steps-in-k-means-algorithm">Steps in K-Means Algorithm:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scenario-k-means-in-action">3. Scenario: K-Means in Action</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-to-determine-the-correct-number-of-clusters">4. How to Determine the Correct Number of Clusters?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#elbow-method">Elbow Method:</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#example-in-python">Example in Python:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#cluster-scaling">5. Cluster Scaling</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering-performance-metrics">6. Clustering Performance Metrics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id56">7. Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-k-nearest-neighbor-k-nn">Machine Learning: K-Nearest Neighbor (K-NN)</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-k-nearest-neighbor">1. What is K-Nearest Neighbor?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-characteristics">Key Characteristics:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-k-nn-works">2. How K-NN Works</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-distance-metrics">Types of Distance Metrics:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-scenario">Example Scenario</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id57">3. Use Case: Iris Flower Classification</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id58">Goal:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id59">Dataset:</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id60">Example in Python:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#results">Results:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id61">4. Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-naive-bayes-classifier">27. Machine Learning: Na√Øve Bayes Classifier</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-naive-bayes-classifier">1. What is Na√Øve Bayes Classifier?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id62">Key Use Cases:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-it-called-naive-bayes">2. Why is it called Na√Øve Bayes?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bayes-theorem">3. Bayes‚Äô Theorem</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-probability">Conditional Probability:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#scenario-coin-flip-example">4. Scenario: Coin Flip Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#use-case-titanic-survival-prediction">5. Use Case: Titanic Survival Prediction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#problem">Problem:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#steps">Steps:</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id63">Example in Python:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id64">Key Points:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id65">6. Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-gridsearchcv-randomizedsearchcv">29. Machine Learning: GridSearchCV &amp; RandomizedSearchCV</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id66">1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameter-tuning">2. Hyperparameter Tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#common-hyperparameter-tuning-approaches">Common Hyperparameter Tuning Approaches:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-tune-hyperparameters">Why Tune Hyperparameters?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-functions-for-svm">3. Kernel Functions (for SVM)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ways-to-tune-hyperparameters">4. Ways to Tune Hyperparameters</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approach-1-manual-tuning-with-train-test-split">Approach 1: Manual Tuning with <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approach-2-k-fold-cross-validation">Approach 2: K-Fold Cross Validation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approach-3-gridsearchcv">Approach 3: GridSearchCV</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#randomizedsearchcv">5. RandomizedSearchCV</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-use-randomizedsearchcv">When to Use RandomizedSearchCV?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-benefits-of-randomizedsearchcv">Key Benefits of RandomizedSearchCV:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id67">6. Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-xgboost">30. Machine Learning: XGBoost</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-xgboost">1. Introduction to XGBoost</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-features">Key Features:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-xgboost">2. Why Use XGBoost?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#installing-xgboost">3. Installing XGBoost</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id68">4. Dataset Overview</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dataset-description">Dataset Description:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id69">Features:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id70">5. Input and Output Variables</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-variables-x">Input Variables (X):</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#output-variable-y">Output Variable (y):</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#data-preprocessing-and-model-training">6. Data Preprocessing and Model Training</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-the-dataset">Loading the Dataset:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation">Explanation:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#label-encoding-for-categorical-data">7. Label Encoding for Categorical Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-label-encoding">Example of Label Encoding:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id71">Explanation:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-importance-with-xgboost">8. Feature Importance with XGBoost</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-feature-importance">Example of Feature Importance:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id72">Explanation:</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id73">9. Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#pickling-and-unpickling-in-python">31. Pickling and Unpickling in Python</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pickling">1. Pickling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id74">Key Points:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-pickling-an-object">Example: Pickling an Object</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#unpickling">2. Unpickling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id75">Key Points:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-unpickling-an-object">Example: Unpickling an Object</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#saving-models-in-machine-learning">Saving Models in Machine Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#saving-models-with-pickling">3. Saving Models with Pickling</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-saving-and-loading-a-model-using-pickling">Example: Saving and Loading a Model using Pickling</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#saving-models-with-joblib">4. Saving Models with Joblib</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-saving-and-loading-a-model-using-joblib">Example: Saving and Loading a Model using Joblib</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id76">Summary</a></li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gajanesh
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright ¬© 2025 Gajanesh. All rights reserved..
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>