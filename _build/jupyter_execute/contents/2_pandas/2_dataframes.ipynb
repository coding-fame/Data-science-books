{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d38aec7c",
   "metadata": {},
   "source": [
    "# Pandas DataFrame\n",
    "\n",
    "---\n",
    "\n",
    "# 1. DataFrame Basics\n",
    "\n",
    "## **What is a Pandas DataFrame?**\n",
    "A **DataFrame** is a 2-dimensional (rows and columns) labeled data structure in the `pandas` library like a spreadsheet or SQL table. \n",
    "\n",
    "It consists of:\n",
    "- **Rows** (observations): Indexed (default: 0-based integers, or custom labels).\n",
    "- **Columns** (variables): Named (each column is a `Series` object).\n",
    "- **Data**: Heterogeneous (can hold mixed types like integers, floats, strings).\n",
    "\n",
    "It‚Äôs highly flexible, efficient, and equipped with powerful methods for data manipulation, cleaning, aggregation, and visualization.\n",
    "\n",
    "---\n",
    "\n",
    "# **2. Creating a DataFrame**\n",
    "You can create a DataFrame from various sources: dictionaries, lists, NumPy arrays, CSV files, etc.\n",
    "\n",
    "| Method               | Syntax                                    | Description                                |\n",
    "|----------------------|------------------------------------------|--------------------------------------------|\n",
    "| **Empty DF**        | `pd.DataFrame()`                        | Creates blank DataFrame                    |\n",
    "| **From Single List** | `pd.DataFrame(list, columns=[], index=[])` | Creates single-column DF with custom labels |\n",
    "| **From Nested List** | `pd.DataFrame([[1,2],[3,4]])`           | Creates DF from 2D list (rows ‚Üí observations) |\n",
    "| **From Dictionary**  | `pd.DataFrame({\"A\": [1,2], \"B\": [3,4]})` | Keys become column names                   |\n",
    "| **From Existing DF** | `new_df = pd.DataFrame(original_df)`    | Creates independent copy                   |\n",
    "| **From Files**       | `pd.read_csv(\"data.csv\"`              | Real-world data loading                    |\n",
    "\n",
    "Loading Data: \n",
    "`pd.read_.` \n",
    "\n",
    "‚ùó **File Handling Tip**: Always verify file paths to avoid `FileNotFoundError`\n",
    "\n",
    "**From a NumPy Array**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e3e7c92",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m], [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m], [\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m]])\n\u001b[1;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(array, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(df)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Output:\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#    A  B\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# 0  1  2\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 1  3  4\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# 2  5  6\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "array = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "df = pd.DataFrame(array, columns=[\"A\", \"B\"])\n",
    "print(df)\n",
    "# Output:\n",
    "#    A  B\n",
    "# 0  1  2\n",
    "# 1  3  4\n",
    "# 2  5  6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52fb1ed1",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "# 3. DataFrame Attributes\n",
    "\n",
    "A **DataFrame** is a predefined class in the pandas library that has several attributes providing useful metadata about the DataFrame object.\n",
    "\n",
    "## Key Attributes\n",
    "These tell you about a DataFrame‚Äôs structure:\n",
    "\n",
    "| Attribute | Description | Example Output |\n",
    "|-----------|-------------|----------------|\n",
    "| `.columns` | Returns column names as an **Index object** | `Index(['Name', 'Age'], dtype='object')` |\n",
    "| `.shape`   | Returns a tuple of **(rows, columns)** | `(1000, 5)` |\n",
    "| `.size`    | Returns the total number of elements (rows √ó columns) | `5000` |\n",
    "| `.dtypes`  | Displays data types of each column | `Name: object, Age: int64` |\n",
    "| `.empty`   | Returns `True` if the DataFrame is empty, else `False` | `False` |\n",
    "| `.index`   | Shows index details | `RangeIndex(start=0, stop=1000, step=1)` |\n",
    "| `.values`  | Returns a NumPy array of all values | `[[1, 'Alice'], [2, 'Bob']]` |\n",
    "| `.T`       | Transposes the DataFrame (swaps rows and columns) | Columns become row indices |\n",
    "\n",
    "### **Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b163012",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)      # Output: (3, 2)\n",
    "print(df.index)      # Output: RangeIndex(start=0, stop=3, step=1)\n",
    "print(df.columns)    # Output: Index(['A', 'B'], dtype='object')\n",
    "print(df.dtypes)     # Output: A    int64 \\n B    int64\n",
    "print(df.values)     # Output: [[1 2] [3 4] [5 6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e499e19d",
   "metadata": {},
   "source": [
    "‚ùó **Key Differences**:  \n",
    "- `len(df)` ‚â† `df.size`  \n",
    "  - `len(df)` ‚Üí Returns the number of rows (same as `shape[0]`)\n",
    "  - `len(df.columns)` ‚Üí Returns the number of columns (same as `shape[1]`)\n",
    "  - `df.size` ‚Üí Returns the total number of elements (rows √ó columns)\n",
    "\n",
    "---\n",
    "\n",
    "# 4. DataFrame Methods\n",
    "A **DataFrame** is a predefined class in the pandas library, offering several methods that perform operations on the DataFrame and return results.\n",
    "\n",
    "## Useful Methods\n",
    "These help you explore and change DataFrames:\n",
    "\n",
    "| Method       | Description | Key Parameters |\n",
    "|-------------|--------------|-----------------|\n",
    "| `.head()`    | Returns first **5 rows** | `n` - Custom row count (e.g., `head(3)`) |\n",
    "| `.tail()`    | Returns last **5 rows** | `n` - Custom row count (e.g., `tail(3)`) |\n",
    "| `.info()`    | Displays:<br>- Column count & names<br>- Non-null counts<br>- Data types<br>- Memory usage | `verbose` - Detailed output<br>`memory_usage` - Memory analysis |\n",
    "| `.count()`   | Shows non-null values **per column** | `axis` - 0 for columns (default), 1 for rows |\n",
    "| `.describe()` | Generates statistics:<br>- Count, mean, std<br>- Min/Max<br>- 25/50/75% quartiles | `include`/`exclude` - Control data types analyzed |\n",
    "| `.nunique()` | Counts unique values **per column** | `axis` - 0 for columns (default), 1 for rows |\n",
    "\n",
    "### Basic Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5840154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Sample DataFrame\n",
    "data = {'Temperature': [22.1, 23.5, None, 19.8],\n",
    "        'City': ['Paris', 'London', 'Berlin', None]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "print(\"First 2 rows:\")\n",
    "print(df.head(2))\n",
    "\n",
    "print(\"\\nData Summary:\")\n",
    "print(df.describe(include='all'))\n",
    "\n",
    "print(\"\\nStructure Info:\")\n",
    "df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89740cb6",
   "metadata": {},
   "source": [
    "### Smart `describe()` Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a5fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical columns only\n",
    "df.describe(include=[np.number])\n",
    "\n",
    "# Categorical columns only\n",
    "df.describe(include=['object'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071cb12b",
   "metadata": {},
   "source": [
    "## Changing Data Types\n",
    "\n",
    "### Method 1: Change Data Type After Reading CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805da75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "drinks['beer_servings'] = drinks.beer_servings.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed61e25",
   "metadata": {},
   "source": [
    "### Method 2: Change Data Type While Reading CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a42ef50",
   "metadata": {},
   "outputs": [],
   "source": [
    "drinks = pd.read_csv('drinks.csv', dtype={'beer_servings': float})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a53813",
   "metadata": {},
   "source": [
    "### Bulk Type Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab768f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_map = {'Temperature': 'float32',\n",
    "            'City': 'category'}\n",
    "df = df.astype(type_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb78881",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "üìå **Pro Tip**: Create method chains for efficient analysis  \n",
    "`df.head(3).T.astype('str').to_dict()` ‚Üí Quick JSON preview\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Renaming Columns and Indexes\n",
    "In pandas, we can rename or modify column names and indexes based on requirements. There are multiple ways to achieve this efficiently.\n",
    "\n",
    "## Renaming Columns\n",
    "- **Selective Rename**: for targeted changes.\n",
    "    The `rename()` method allows you to rename specific columns using a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fe2ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.rename(columns={\"Name\": \"FullName\"}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fd904e",
   "metadata": {},
   "source": [
    "*‚ùó **Key Note:** Dictionary keys **must exactly match** existing column names. Non-matching keys are ignored silently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928f9d9b",
   "metadata": {},
   "source": [
    "- **Full Rename**: for complete renaming.\n",
    "    You can replace all column names at once by assigning a list to the `columns` attribute:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2cfda4",
   "metadata": {},
   "source": [
    "```python\n",
    "    # Assign new column names\n",
    "    df.columns = ['new_col1', 'new_col2', 'new_col3']\n",
    "```\n",
    "**Important:** The number of new names must match the number of existing columns; otherwise, a `ValueError` occurs.\n",
    "\n",
    "```python\n",
    "# Incorrect: Mismatch in column count\n",
    "# df.columns = ['A', 'B']  # Raises ValueError\n",
    "```\n",
    "- **Case Conversion & Bulk Operations:**\n",
    "  - `df.columns = df.columns.str.upper()` (Uppercase)\n",
    "  - `df.columns = [col.upper() for col in df.columns]`\n",
    "  - `df.columns = df.columns.str.replace(' ', '_')` (Underscore format)\n",
    "  - `df.columns = [col + '_2023' for col in df.columns]`\n",
    "  - ``"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08e1ad5",
   "metadata": {},
   "source": [
    "**Method Chaining:**\n",
    "   ```python\n",
    "   df = (pd.read_csv(\"data.csv\")\n",
    "           .rename(columns={\"temp\": \"temperature\"})\n",
    "           .set_index(\"timestamp\"))\n",
    "   ```\n",
    "\n",
    "**Validation Check Before Renaming:**\n",
    "   ```python\n",
    "   assert {'old_name', 'another_col'}.issubset(df.columns), \"Missing columns!\"\n",
    "   ```\n",
    "\n",
    "üìå **Pro Tip:** Use `df.filter()` to verify columns before renaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14550eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_cols = df.filter(items=[\"temp\", \"hum\"]).columns\n",
    "df = df[valid_cols].rename(columns={\"temp\": \"temperature\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694d2434",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Renaming Indexes\n",
    "- **Selective**: for specific row labels.\n",
    "  ```python\n",
    "  # Rename specific index values\n",
    "    df.rename(index={0: 'first', 1: 'second'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e80b6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function-based renaming\n",
    "df.rename(index=lambda x: f'row_{x+1}', inplace=True)\n",
    "\n",
    "# **Full**: for full index replacement.\n",
    "\n",
    "df.index = [\"Person1\", \"Person2\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551a19b5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Understanding `axis`\n",
    "\n",
    "- `axis=0`: Row-wise operations\n",
    "- `axis=1`: Column-wise operations\n",
    "\n",
    "- Dropping a Column\n",
    "    `drinks.drop('continent', axis=1).head()`\n",
    "- Calculating Mean for Each Column\n",
    "    `drinks.mean(axis=0)  # Same as drinks.mean()`\n",
    "- Calculating Mean for Each Row\n",
    "    `drinks.mean(axis=1).head()`\n",
    "\n",
    "---\n",
    "## Adding and Dropping Columns\n",
    "\n",
    "We can add new or drop columns to an existing DataFrame based on our requirements.\n",
    "\n",
    "- Remove columns using the `drop()` method.\n",
    "\n",
    "### 1. Inserting a Column at a Specific Position\n",
    "\n",
    "You can insert a column at a specific index position using `insert()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0155af91",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column = df['Product Cost'] * df['Quantity']\n",
    "df.insert(5, \"Total Cost\", new_column)  # Position 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5465721",
   "metadata": {},
   "source": [
    "üîß **Pro Tip**: Use `assign()` for temporary column additions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5da28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df.assign(Temp=lambda x: x.Price * 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babce822",
   "metadata": {},
   "source": [
    "### 2. Dropping a Column at a Specific Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35115248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single column\n",
    "df.drop(columns='Customer Name', inplace=True)\n",
    "\n",
    "# Multiple columns\n",
    "df.drop(['Customer Name', 'Product Name'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617bcec1",
   "metadata": {},
   "source": [
    "## Dropping Rows\n",
    "\n",
    "Use the `drop()` method to remove rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8674b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single row by index\n",
    "df.drop(3, axis=0, inplace=True)\n",
    "\n",
    "# Multiple rows\n",
    "df.drop([1, 2], axis=0, inplace=True)\n",
    "\n",
    "# Conditional dropping\n",
    "df = df[df['Sales'] > 1000]  # Keep rows where sales > 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc03c384",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 6. `inplace` Parameter\n",
    "\n",
    "Understand when and how to use `inplace` to modify DataFrames without creating new objects.\n",
    "\n",
    "## Common Methods with `inplace`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37f9127",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns=..., inplace=True)\n",
    "df.drop(labels=..., inplace=True)\n",
    "df.sort_values(by=..., inplace=True)\n",
    "df.set_index(keys=..., inplace=True)\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce148b33",
   "metadata": {},
   "source": [
    "## Memory Management Tradeoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71aa3bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good for large datasets (avoids duplication)\n",
    "big_data.rename(columns={...}, inplace=True)  # Saves memory\n",
    "\n",
    "# Bad for small data (unnecessary mutation)\n",
    "small_data.dropna(inplace=True)  # Prefer: small_data = small_data.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51295030",
   "metadata": {},
   "source": [
    "‚ö†Ô∏è **Deprecation Warning**  \n",
    "`inplace` parameter is being phased out in future pandas versions.  \n",
    "**Recommended Alternative**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1568d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of:\n",
    "df.method(inplace=True)\n",
    "\n",
    "# Use:\n",
    "df = df.method()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02d500a",
   "metadata": {},
   "source": [
    "---\n",
    "# 7. Handling Missing Values\n",
    "\n",
    "## 1. Understanding Missing Data\n",
    "\n",
    "What is NaN (Not a Number)?\n",
    "- **Purpose**: Represents missing or undefined numerical values.\n",
    "- **Data Type**: Stored as `float64` in Pandas, even in non-float columns.\n",
    "- **How NaNs are Created**:\n",
    "  - Loading CSV/Excel files with empty cells.\n",
    "  - Performing undefined mathematical operations.\n",
    "  - Explicitly inserting `np.nan`.\n",
    "\n",
    "## 2. Detecting Missing Values\n",
    "- `.isna()`: Identifies `NaN` values\n",
    "- `.isnull()`: Identifies `NaN` values\n",
    "- `.isna().sum()`: Count missing values per column\n",
    "- `.isna()..mean()*100`: Percentage of missing values\n",
    "\n",
    "**Pro Tip**: Visualize missing values using the `missingno` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f748c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno\n",
    "msno.matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4410b8b4",
   "metadata": {},
   "source": [
    "## 3. Handling Missing Values (NaNs)\n",
    "There are four common approaches to handle missing values:\n",
    "\n",
    "1. **Drop rows** containing NaNs.\n",
    "2. **Drop columns** containing NaNs.\n",
    "3. **Fill NaNs** with imputed values.\n",
    "4. **Use models** that natively handle NaNs.\n",
    "\n",
    "## **Data Cleaning**\n",
    "- `.dropna()`: Remove rows/columns with NaN.\n",
    "- `.fillna(value)`: Replace NaN with a value.\n",
    "- `.duplicated()`: Check for duplicate rows.\n",
    "- `.drop_duplicates()`: Remove duplicates.\n",
    "\n",
    "### 1. Removing Missing Values (`dropna()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d907f692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with any missing values\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Custom removal parameters\n",
    "df.dropna(\n",
    "    axis=0,         # 0=rows, 1=columns\n",
    "    how='any',      # 'any' or 'all'\n",
    "    thresh=2,       # Keep rows with ‚â•2 non-NA values\n",
    "    subset=['Age']  # Only check specific columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caafc542",
   "metadata": {},
   "source": [
    "### 2. Imputing Missing Values (`fillna()`)\n",
    "- **Numerical Data**: Mean/median for normally distributed data.\n",
    "- **Categorical Data**: Replace with mode or \"Unknown\" category.\n",
    "- **Time-Series Data**: Use forward/backward fill (`ffill`, `bfill`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123455b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imputation\n",
    "df_filled = df.fillna({\n",
    "    'Age': df['Age'].mean(),  # Replace NaN with column mean\n",
    "    'Income': 0               # Replace NaN with 0\n",
    "})\n",
    "\n",
    "# Forward fill (propagate last valid value)\n",
    "df.fillna(method='ffill', limit=1)\n",
    "\n",
    "# Backward fill (propagate next valid value)\n",
    "df.fillna(method='bfill')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a229bf",
   "metadata": {},
   "source": [
    "### 3. Replacing Missing Values (`replace()`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fda8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN with a specific value\n",
    "df_replaced = df.replace(np.nan, -999)\n",
    "\n",
    "# Multi-value replacement\n",
    "df.replace({\n",
    "    np.nan: 'Missing',\n",
    "    0: 'Zero'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5749de1",
   "metadata": {},
   "source": [
    "### 4. Advanced Imputation Methods\n",
    "If `SimpleImputer` is too basic, consider **KNNImputer** or **IterativeImputer**:\n",
    "- `KNNImputer`: Uses K-nearest neighbors to estimate missing values.\n",
    "- `IterativeImputer`: Uses a regression model to predict missing values based on other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924325d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, KNNImputer\n",
    "\n",
    "df = pd.read_csv('http://bit.ly/kaggletrain', nrows=6)\n",
    "cols = ['SibSp', 'Fare', 'Age']\n",
    "X = df[cols]\n",
    "\n",
    "# Iterative Imputation\n",
    "impute_it = IterativeImputer()\n",
    "impute_it.fit_transform(X)\n",
    "\n",
    "# KNN Imputation\n",
    "impute_knn = KNNImputer(n_neighbors=2)\n",
    "impute_knn.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd75685",
   "metadata": {},
   "source": [
    "---\n",
    "## Imputing Missing Categorical Values\n",
    "\n",
    "Two common methods:\n",
    "1. Impute the most frequent value.\n",
    "2. Impute the value `\"missing\"`, treating it as a separate category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c8e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Sample data with missing categorical values\n",
    "X = pd.DataFrame({'Shape': ['square', 'square', 'oval', 'circle', np.nan]})\n",
    "\n",
    "# Impute with most frequent value\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "print(imputer.fit_transform(X))\n",
    "\n",
    "# Impute with a constant value \"missing\"\n",
    "imputer = SimpleImputer(strategy='constant', fill_value='missing')\n",
    "print(imputer.fit_transform(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cc1ba1",
   "metadata": {},
   "source": [
    "---\n",
    "## Adding a Missing Indicator\n",
    "When imputing missing values, you can preserve information about which values were missing by adding a **missing indicator**.\n",
    "Including a missing indicator can improve model performance when missing values are meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9a90f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "X = pd.DataFrame({'Age': [20, 30, 10, np.nan, 10]})\n",
    "\n",
    "# Impute missing values and add an indicator matrix\n",
    "imputer = SimpleImputer(add_indicator=True)\n",
    "imputer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df46f48",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **8. Accessing and Modifying Columns**\n",
    "\n",
    "## Basic Column Selection\n",
    "\n",
    "| Operation           | Syntax                    | Returns     |\n",
    "|--------------------|-------------------------|------------|\n",
    "| Single Column      | `df[\"col\"]` or `df.col`  | `Series`   |\n",
    "| Multiple Columns   | `df[[\"col1\", \"col2\"]]`  | `DataFrame` |\n",
    "| Conditional Rows   | `df[df[\"col\"] > value]` | `DataFrame` |\n",
    "\n",
    "---\n",
    "\n",
    "## `iloc` Indexer (Position-Based Selection)\n",
    "\n",
    "### Key Features\n",
    "- Purely integer-based (row/column positions)\n",
    "- Excludes the end index in ranges\n",
    "- Cannot use column/row labels\n",
    "- Optimized for positional access\n",
    "\n",
    "### Syntax Patterns\n",
    "- `df.iloc[row_index]`: return Single row\n",
    "- `df.iloc[:, column_index]`: return Single column\n",
    "- `df.iloc[row_range, column_range]`: Multiple rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f42ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row and column selection\n",
    "df.iloc[0, 2]       # Row 0, Column 2\n",
    "df.iloc[:, 1:3]     # All rows, Columns 1-2\n",
    "df.iloc[2:5, :]     # Rows 2-4, All columns\n",
    "\n",
    "# List of positions\n",
    "df.iloc[[0, 2, 4]]  # Rows 0, 2, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6573ffd",
   "metadata": {},
   "source": [
    "## `loc` Indexer (Label-Based Selection)\n",
    "\n",
    "### Key Features\n",
    "- Uses actual row/column labels\n",
    "- Includes the end index in ranges\n",
    "- Allows boolean indexing\n",
    "- Requires a set index for meaningful row labels\n",
    "\n",
    "### Syntax Patterns\n",
    "- `df.loc[\"row_label\"]`: return Specific row\n",
    "- `df.loc[[\"row1\", \"row3\"]]`: return Multiple rows\n",
    "- `df.loc[\"row1\":\"row4\"]`: return row1 to \"row4\" (Includes)\n",
    "\n",
    "- `df.loc[:, \"col_label\"]`: return Specific column\n",
    "- `df.loc[:, [\"colA\", \"colC\"]]`: return Multiple columns\n",
    "- `df.loc[:, \"colB\":\"colD\"]`: return colB to \"colD\" (Includes)\n",
    "- `df.loc[df[\"price\"] > 100]`: return Boolean indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36470db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set custom index\n",
    "df = df.set_index(\"product_id\")\n",
    "\n",
    "# Combined selection\n",
    "df.loc[[\"P100\", \"P102\"], \"price\":\"stock\"]\n",
    "\n",
    "# Conditional + column slice\n",
    "df.loc[df[\"category\"] == \"Electronics\", \"price\":]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6cddeb",
   "metadata": {},
   "source": [
    "## Key Differences: `iloc` vs `loc`\n",
    "\n",
    "| Feature        | `iloc`               | `loc`                 |\n",
    "|---------------|---------------------|-----------------------|\n",
    "| **Index Type** | Integer positions    | Labels/Booleans       |\n",
    "| **Range End**  | Exclusive            | Inclusive             |\n",
    "| **Column Ref** | Position only        | Name/Position         |\n",
    "| **Performance**| Faster               | Slower (depends on index) |\n",
    "| **Use Case**   | Positional access    | Label-based queries   |\n",
    "\n",
    "### Boolean Indexing Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cded3ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex conditions\n",
    "mask = (df[\"price\"] > 100) & (df[\"stock\"] < 50)\n",
    "df.loc[mask, [\"product\", \"price\"]]\n",
    "\n",
    "# Lambda functions\n",
    "df.loc[lambda x: x[\"sales\"] > x[\"target\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cbce0f",
   "metadata": {},
   "source": [
    "### Performance Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a302cbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use iloc for large datasets\n",
    "large_df.iloc[10000:20000]  # Faster than loc\n",
    "\n",
    "# Pre-calculate indexes\n",
    "fast_access = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3b4fb6",
   "metadata": {},
   "source": [
    "## Common Pitfalls\n",
    "\n",
    "### 1. Off-by-One Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645ab5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0:5]  # Rows 0-4 (5 rows)\n",
    "df.loc[\"A\":\"E\"]  # Includes E (5 rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a293b73",
   "metadata": {},
   "source": [
    "### 2. Mixed Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd869af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If index contains both strings and numbers\n",
    "df.loc[1]  # Could return label 1 or position 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4337a19b",
   "metadata": {},
   "source": [
    "### 3. Chaining Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4994d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avoid:\n",
    "df.iloc[5:10][\"Price\"]\n",
    "\n",
    "# Use:\n",
    "df.iloc[5:10, df.columns.get_loc(\"Price\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d74a3847",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 9. Filtering\n",
    "\n",
    "Filtering data is essential for extracting meaningful insights from large datasets.\n",
    "\n",
    "Use boolean indexing or `.query()`.\n",
    "\n",
    "## 1. Relational Operators\n",
    "Filtering using relational operators such as `>`, `<`, `>=`, `<=`, `==`, and `!=`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5749157b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single condition\n",
    "high_cost = df['Product_Cost'] > 65000\n",
    "filtered_df = df[high_cost]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aba46e8",
   "metadata": {},
   "source": [
    "## 2. Logical Operators\n",
    "Filtering using logical operators such as AND (`&`), OR (`|`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008699ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies[(movies.duration >= 200) & (movies.genre == 'Drama')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7f61b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies[(movies.genre == 'Crime') | (movies.genre == 'Drama') | (movies.genre == 'Action')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3e2b34",
   "metadata": {},
   "source": [
    "## 3. Label-Based Filtering (`loc`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf608f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering specific products and customers\n",
    "df_filtered = df.loc[(df.Product_Name == \"iPhone 11\") & (df.Customer_Name == \"Shahid\")]\n",
    "\n",
    "# Selecting columns based on a condition\n",
    "df_subset = df.loc[df['Sales'] > 500, ['Product', 'Region', 'Revenue']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21be84b9",
   "metadata": {},
   "source": [
    "## 4. Position-Based Filtering (`iloc`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d4ae6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting first 5 rows\n",
    "df_head = df.iloc[:5, :]\n",
    "\n",
    "# Selecting specific rows and columns\n",
    "critical_data = df.iloc[10:20, [0, 2, 4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc7d5ed",
   "metadata": {},
   "source": [
    "## 5. Chained Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58e37be",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = (\n",
    "    df.query('Price > 1000')\n",
    "      .loc[:, ['Product', 'Category']]\n",
    "      .iloc[::2]  # Select every other row\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17a3370",
   "metadata": {},
   "source": [
    "## Advanced Filtering Methods\n",
    "\n",
    "### 1. Multi-Value Selection with `isin()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523ed68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single product filter\n",
    "macbooks = df[df.Product_Name.isin([\"Macbook Pro Laptop\"])]\n",
    "\n",
    "# Multiple products filter\n",
    "df_premium = df[df.Product_Name.isin([\"34in Ultrawide Monitor\", \"Macbook Pro Laptop\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fb3696",
   "metadata": {},
   "source": [
    "### 2. Unique Value Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5295954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique products\n",
    "unique_products = pd.unique(df.Product_Name)\n",
    "\n",
    "# Identify rare customers\n",
    "rare_customers = pd.unique(df.Customer_Name)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33253001",
   "metadata": {},
   "source": [
    "### 3. Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2512c05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering non-null values\n",
    "valid_emails = df[df.Email.notnull()]\n",
    "\n",
    "# Combining null checks\n",
    "complete_records = df[df['Address'].notnull() & df['Phone'].notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5170b26",
   "metadata": {},
   "source": [
    "## Practical Example: E-Commerce Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f3e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    'OrderID': range(1001, 1021),\n",
    "    'Product': np.random.choice(['Laptop', 'Phone', 'Tablet', 'Monitor'], 20),\n",
    "    'Price': np.random.randint(200, 2000, 20),\n",
    "    'Customer': np.random.choice(['Alice', 'Bob', 'Charlie', 'David'], 20),\n",
    "    'Rating': np.round(np.random.uniform(3.5, 5, 20), 1)\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Complex filter\n",
    "premium_tech = df[\n",
    "    (df['Product'].isin(['Laptop', 'Monitor'])) &\n",
    "    (df['Price'] > 1000) &\n",
    "    (df['Rating'] >= 4.0) &\n",
    "    (df['Customer'].notnull())\n",
    "]\n",
    "\n",
    "print(\"Premium Tech Products:\")\n",
    "print(premium_tech)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87fffee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 10. Sorting\n",
    "\n",
    "Pandas provides powerful methods to sort DataFrames based on column values or index labels.\n",
    "\n",
    "The `sort_values()` method sorts a DataFrame based on column values. By default, it sorts in ascending order for numerical values and alphabetically for strings.\n",
    "\n",
    "The `sort_index()` method sorts a DataFrame based on index labels, often useful after reindexing or for time-series data.\n",
    "\n",
    "## Syntax:\n",
    "- `.sort_values(by=\"column_name\", ascending=True)`: Sort by column(s).\n",
    "- `.sort_index(ascending=True)`: Sort by index.\n",
    "\n",
    "## Key Parameters\n",
    "| Parameter  | Description                 | Default | Common Values  |\n",
    "|------------|-----------------------------|---------|----------------|\n",
    "| `by`       | Column(s) to sort by        | Required | String or list |\n",
    "| `ascending`| Sort order                  | `True`  | `True`/`False` or list |\n",
    "| `inplace`  | Modify original DataFrame   | `False` | `True`/`False` |\n",
    "| `na_position` | NaN values placement   | 'last'  | 'first'/'last' |\n",
    "\n",
    "## Usage Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5e9ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic single-column sort\n",
    "df_sorted = df.sort_values(by='Customer_Id')\n",
    "\n",
    "# Descending order with NaN handling\n",
    "df_sorted = df.sort_values(by='Revenue', ascending=False, na_position='first')\n",
    "\n",
    "# Multi-column sorting\n",
    "df_sorted = df.sort_values(by=['Department', 'Salary'], ascending=[True, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a73335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic index sort\n",
    "df_sorted = df.sort_index()\n",
    "\n",
    "# Descending index order\n",
    "df_sorted = df.sort_index(ascending=False)\n",
    "\n",
    "# Sort columns alphabetically\n",
    "df_sorted = df.sort_index(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7838af",
   "metadata": {},
   "source": [
    "## `sort_values()` vs `sort_index()`\n",
    "\n",
    "| Feature         | `sort_values()`             | `sort_index()`           |\n",
    "|----------------|-----------------------------|---------------------------|\n",
    "| **Primary Use** | Sorts by data values       | Sorts by index labels    |\n",
    "| **Multi-level** | Supports multi-column sorting | Supports multi-index sorting |\n",
    "| **Axis Control** | Default: rows (`axis=0`)  | Can sort columns (`axis=1`) |\n",
    "| **NaN Handling** | Placement control         | Follows label order       |\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Reset Index After Sorting**\n",
    "   ```python\n",
    "   df = df.sort_values('date').reset_index(drop=True)\n",
    "   ```\n",
    "2. **Verify Sort Stability**\n",
    "   ```python\n",
    "   assert df.index.is_monotonic_increasing, \"Data not sorted!\"\n",
    "   ```\n",
    "3. **Optimize Memory Usage for Large Datasets**\n",
    "   ```python\n",
    "   df.sort_values(by='timestamp', inplace=True)\n",
    "   ```\n",
    "4. **Multi-Level Sorting**\n",
    "   ```python\n",
    "   df.sort_values(by=['department', 'salary'], ascending=[True, False], inplace=True)\n",
    "   ```\n",
    "\n",
    "## Common Pitfalls & Solutions\n",
    "\n",
    "### 1. Mixed Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d7d220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to consistent types before sorting\n",
    "df['Price'] = pd.to_numeric(df['Price'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d974b046",
   "metadata": {},
   "source": [
    "### 2. Case-Sensitive Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88150ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='Name', key=lambda x: x.str.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a21cb4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **11. Aggregation and Grouping**\n",
    "\n",
    "GroupBy is a powerful operation in data analysis that allows you to split, apply, and combine data based on certain criteria. It is commonly used for summarizing data or performing aggregations.\n",
    "\n",
    "## Steps in GroupBy Operation\n",
    "\n",
    "1. **Splitting**: The data is split into groups based on a specified criterion.\n",
    "2. **Applying**: A function is applied to each group independently.\n",
    "3. **Combining**: The results from all groups are combined to form a new DataFrame.\n",
    "\n",
    "## **GroupBy**\n",
    "- The `groupby()` method is a predefined method in the DataFrame class. \n",
    "- We should access this method by using DataFrame object.\n",
    "- It returns a `GroupBy` object, which can be used to perform various operations on the grouped data.\n",
    "\n",
    "- `.groupby(column)`: Group data for aggregation.\n",
    "\n",
    "### Key Parameters\n",
    "| Parameter | Description | Default | Example |\n",
    "|-----------|-------------|---------|---------|\n",
    "| `by` | Grouping criteria | Required | `by='Department'` |\n",
    "| `axis` | Group rows(0) or columns(1) | 0 | `axis=1` |\n",
    "| `as_index` | Use group labels as index | True | `as_index=False` |\n",
    "| `sort` | Sort group keys | True | `sort=False` |\n",
    "\n",
    "### 1. Grouping by a Single Column\n",
    "Group the DataFrame by the `Product` column and calculate the sum for each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde05262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by product and sum sales\n",
    "product_group = df.groupby('Product')\n",
    "total_sales = product_group['Sales'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628dd843",
   "metadata": {},
   "source": [
    "### 2. Multi-Column Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f158fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by product and region\n",
    "regional_sales = df.groupby(['Product', 'Region'])['Sales'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5ab8ed",
   "metadata": {},
   "source": [
    "## **Aggregation**\n",
    "- `.sum()`, `.mean()`, `.median()`, `count()`,  `.min()`, `.max()`, etc.\n",
    "\n",
    "### 1. Custom Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac6327d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple statistics at once\n",
    "product_stats = df.groupby('Product')['Sales'].agg(['sum', 'mean', 'count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a6bfdd",
   "metadata": {},
   "source": [
    "### 2. Named Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04c390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Product').agg(\n",
    "    total_sales=('Sales', 'sum'),\n",
    "    avg_sales=('Sales', 'mean'),\n",
    "    orders=('Sales', 'count')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9857a527",
   "metadata": {},
   "source": [
    "üîó **Pro Tip**: Combine with `sort_values` for ordered analysis  \n",
    "`df.groupby('Category')['Sales'].sum().sort_values(ascending=False).head(5)`\n",
    "\n",
    "---\n",
    "\n",
    "# **12. Merging and Joining**\n",
    "\n",
    "**Methods**\n",
    "- `.merge()`: SQL-like joins.\n",
    "- `.concat()`: Stack DataFrames.\n",
    "- `.join()`: Join on index.\n",
    "\n",
    "## Merges\n",
    "\n",
    "Merging or joining is the process of combining two DataFrames based on common attributes in columns. This operation is similar to the `JOIN` operation in databases.\n",
    "\n",
    "The `merge()` function in pandas enables various types of join operations between DataFrames.\n",
    "\n",
    "### Syntax:\n",
    "- `pd.merge(df1, df2, on=\"column\", how=\"join_type\")`: \n",
    "- **Parameters**:\n",
    "  - `df1`: The first DataFrame.\n",
    "  - `df2`: The second DataFrame.\n",
    "  - `on`: The column(s) to join on.\n",
    "  - `how`: The type of join (default is 'inner').\n",
    "\n",
    "Practical Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1c68d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "customers = pd.DataFrame({\n",
    "    'cust_id': [1, 2, 3],\n",
    "    'name': ['Alice', 'Bob', 'Charlie']\n",
    "})\n",
    "\n",
    "orders = pd.DataFrame({\n",
    "    'order_id': [101, 102, 103],\n",
    "    'cust_id': [1, 2, 4],\n",
    "    'total': [150.0, 99.99, 200.0]\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d894109",
   "metadata": {},
   "source": [
    "### **Types of Joins**\n",
    "#### 1. Inner Join (Default)\n",
    "- Common data only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263e1667",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(customers, orders, on='cust_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e49286a",
   "metadata": {},
   "source": [
    "#### 2. Left Join\n",
    "- A **left join** keeps all rows from the left DataFrame and fills missing data from the right DataFrame with `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d64ac46",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(customers, orders, on='cust_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17bdc18",
   "metadata": {},
   "source": [
    "#### 3. Right Join\n",
    "- A **right join** keeps all rows from the right DataFrame and fills missing data from the left DataFrame with `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70393d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(customers, orders, on='cust_id', how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9c1357",
   "metadata": {},
   "source": [
    "#### 3. Outer Join\n",
    "- All rows from both DataFrames, missing values filled with `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121b33dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(customers, orders, on='cust_id', how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc3a3ab",
   "metadata": {},
   "source": [
    "**Join Types**:\n",
    "- `pd.merge(customers, orders, on='cust_id', how='outer', validate='one_to_one')`\n",
    "  - **One-to-One**: One row in the left and right DataFrames match.\n",
    "  - **Many-to-One**: Duplicate values in the left DataFrame.\n",
    "  - **Many-to-Many**: Duplicate values in both DataFrames.\n",
    "\n",
    "---\n",
    "### Advanced Techniques\n",
    "\n",
    "1. Multi-Key Merges\n",
    "    ```python\n",
    "    pd.merge(df1, df2, on=['country', 'city'])\n",
    "    ```\n",
    "2. Indicator Flag\n",
    "    ```python\n",
    "    pd.merge(df1, df2, how='outer', indicator=True)\n",
    "    ```\n",
    "3. Merging on Indexes\n",
    "    ```python\n",
    "    pd.merge(df1, df2, left_index=True, right_index=True)\n",
    "    ```\n",
    "\n",
    "---\n",
    "\n",
    "## Concatenation with `pd.concat()`\n",
    "\n",
    "Concatenation in pandas allows us to combine or stack multiple DataFrames either vertically or horizontally based on specific requirements.\n",
    "\n",
    "## The `concat()` Function\n",
    "\n",
    "The `concat()` function in pandas is used to concatenate DataFrames along a particular axis (either rows or columns).\n",
    "\n",
    "- `.concat()`: Stack DataFrames.\n",
    "\n",
    "### Core Syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daff91fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat(\n",
    "    objs,             # List/sequence of DataFrames\n",
    "    axis=0,           # 0=vertical (default), 1=horizontal\n",
    "    join='outer',     # 'outer' (default) or 'inner'\n",
    "    ignore_index=False # Reset index after concatenation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa79fa6",
   "metadata": {},
   "source": [
    "### Concatenation Modes\n",
    "\n",
    "#### 1. Vertical Concatenation (Axis=0)\n",
    "- Stacks DataFrames one below the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02a0a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
    "df2 = pd.DataFrame({'A': [5, 6], 'B': [7, 8]})\n",
    "\n",
    "result = pd.concat([df1, df2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64c4333",
   "metadata": {},
   "source": [
    "#### 2. Horizontal Concatenation (Axis=1)\n",
    "- Aligns DataFrames side by side.\n",
    "\n",
    "**Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb61a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({'C': [9, 10], 'D': [11, 12]})\n",
    "result = pd.concat([df1, df3], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056cf9c5",
   "metadata": {},
   "source": [
    "### 3. Mixed Axis Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82eafc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine vertical and horizontal\n",
    "combined = pd.concat([\n",
    "    pd.concat([df1, df2], axis=0),\n",
    "    pd.concat([df3], axis=1)\n",
    "], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547d232e",
   "metadata": {},
   "source": [
    "üîó **Pro Tip**: Combine with `groupby` for complex data assembly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad49ded2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([group for _, group in df.groupby('category')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bd44a4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 13. Concatenating Multiple CSV Files\n",
    "\n",
    "In real-world scenarios, data is often spread across multiple CSV files. To analyze or process the data efficiently, we need to **concatenate these files** into a single dataset.\n",
    "\n",
    "## ‚úÖ Steps to Concatenate CSV Files\n",
    "\n",
    "### 1. Using the `glob` Module\n",
    "We can use the `glob` module to find CSV files in a more structured way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1586b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "path = \"./data\"\n",
    "csv_files = glob.glob(os.path.join(path, \"*.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39252dc1",
   "metadata": {},
   "source": [
    "This fetches **all CSV files** from the directory in a list format.\n",
    "\n",
    "### 2. **Concatenating All CSV Files**\n",
    "\n",
    "Once we have the list of CSV files, we can use the `pandas` library to read and merge them into a single DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de760bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and concatenate all CSV files\n",
    "result = (pd.read_csv(file) for file in csv_files)\n",
    "df = pd.concat(result, ignore_index=True)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153885d5",
   "metadata": {},
   "source": [
    "- `pd.read_csv(file)`: Reads each file into a DataFrame.\n",
    "- `pd.concat()`: Combines all DataFrames into one.\n",
    "- `ignore_index=True`: Resets the index after concatenation.\n",
    "\n",
    "### Method 2: `os` module\n",
    "\n",
    "1Ô∏è. **Accessing All Files in a Directory**\n",
    "\n",
    "The `os` module is a built-in Python library that allows interaction with the operating system, such as listing files in a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2165be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"./data\"\n",
    "all_files = os.listdir(path)  # Retrieves all file names in the folder\n",
    "print(all_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f10159",
   "metadata": {},
   "source": [
    "This will return a list of all files inside the `./data` directory.\n",
    "\n",
    "2Ô∏è. **Filter Only CSV Files**\n",
    "Since the directory may contain various file types, we must filter only CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c4b240",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [file for file in all_files if file.endswith(\".csv\")]\n",
    "print(csv_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b255095f",
   "metadata": {},
   "source": [
    "This ensures that only `.csv` files are selected for concatenation.\n",
    "\n",
    "---\n",
    "\n",
    "# **14. Applying Functions**\n",
    "- `.apply()`: Apply a function along an axis.\n",
    "- `.map()`: Apply to a Series (column).\n",
    "- `.applymap()`: Element-wise for entire DataFrame.\n",
    "\n",
    "## `.apply()`\n",
    "### 1. `apply()` for Series-Wide Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d512b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating string length for each row in 'Name' column\n",
    "train['Name_length'] = train.Name.apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413ed511",
   "metadata": {},
   "source": [
    "### 2. Adding a Column Using `apply()`\n",
    "\n",
    "The `apply()` method allows row-wise operations to calculate a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe750a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_total(row):\n",
    "    return row['Product Cost'] * row['Quantity']\n",
    "\n",
    "df['Total Cost'] = df.apply(calculate_total, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5229c0",
   "metadata": {},
   "source": [
    "### 3. `apply()` for DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6a2a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding max value per column\n",
    "drinks.loc[:, 'beer_servings':'wine_servings'].apply(max, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99094076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding max value per row\n",
    "drinks.loc[:, 'beer_servings':'wine_servings'].apply(max, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cd4637",
   "metadata": {},
   "source": [
    "---\n",
    "## `.map()`: Apply to a Series (column).\n",
    "\n",
    "### 1. `map()` for Simple Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dc30d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping categorical values to numerical\n",
    "train['Sex_num'] = train.Sex.map({'female': 0, 'male': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826e071f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## `.applymap()`: Element-wise for entire DataFrame.\n",
    "\n",
    "### 4. `applymap()` for Element-Wise Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8028746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying function to every element\n",
    "drinks.loc[:, 'beer_servings': 'wine_servings'] = drinks.loc[:, 'beer_servings': 'wine_servings'].applymap(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33df88e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 15. üìÖ Working with Date and Time\n",
    "\n",
    "Date and time manipulations are crucial when working with datasets that contain time-based information.\n",
    "\n",
    "## Converting Data Type\n",
    "### 1. Loading a CSV with Date Parsing\n",
    "When loading a CSV file, if a column contains date values, Pandas treats it as an **object** by default. To explicitly parse date columns, use `parse_dates` while reading the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a1434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sales7_dates.csv', parse_dates=['Pur_Date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd557b3",
   "metadata": {},
   "source": [
    "### 2. Converting Object Data Type to Date\n",
    "Sometimes, date values are stored as objects (strings). We can convert them explicitly using:\n",
    "\n",
    "- `pd.to_datetime(df['date_col'], format='%d%m%Y')`\n",
    "- `.astype('datetime64[ns]')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de9214d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Pur_Date'] = pd.to_datetime(df['Pur_Date'])\n",
    "\n",
    "df['Pur_Date'] = df['Pur_Date'].astype('datetime64[ns]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bce20b7",
   "metadata": {},
   "source": [
    "**Formatting Date Strings**\n",
    "Date formats can vary, such as **\"03-23-15\"** or **\"3|23|2015\"**. We can use the `format` parameter to specify the exact format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e69ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PurDate'] = pd.to_datetime(df['PurDate'], format='%d%m%Y')\n",
    "df['PurDate'] = pd.to_datetime(df['PurDate'], format='%d%b%Y')  # Example: 23Mar2015"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a69a79",
   "metadata": {},
   "source": [
    "**Handling Missing Dates (NaT Values)**\n",
    "If your date column contains missing values (`NaN`), converting them to datetime will raise an error. Use `errors=\"coerce\"` to convert invalid values to **NaT (Not a Time).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fe3a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PurDate'] = pd.to_datetime(df['PurDate'], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da039ecb",
   "metadata": {},
   "source": [
    "## Selecting Date Ranges\n",
    "We can filter data based on a **start and end date.**\n",
    "\n",
    "### Selecting Data Between Two Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f796048",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = df['Pur_Date'] > '2019-1-1 01:00:00'\n",
    "end = df['Pur_Date'] < '2019-1-1 05:00:00'\n",
    "result = df[start & end]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ad01fa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Accessing Last N Days, Months, or Years\n",
    "Pandas allows retrieving records for the last **N days, months, or years** by setting the date column as the index and using the `.last()` method.\n",
    "\n",
    "### Selecting Recent Date Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95cead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index(\"Pur_Date\")\n",
    "\n",
    "# Last 10 days\n",
    "days_10 = df.last(\"10D\")\n",
    "\n",
    "# Last 40 days\n",
    "days_40 = df.last(\"40D\")\n",
    "\n",
    "# Last 1 month\n",
    "month_1 = df.last(\"1M\")\n",
    "\n",
    "# Last 1 year\n",
    "year_1 = df.last(\"1Y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7d2546",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Extracting Date Components\n",
    "We can extract individual components of the date (year, month, day, hour, minute) using `.dt`.\n",
    "\n",
    "### Extracting Year, Month, Day, Hour, and Minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4351d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df['Pur_Date'].dt.year\n",
    "df['month'] = df['Pur_Date'].dt.month\n",
    "df['day'] = df['Pur_Date'].dt.day\n",
    "df['hour'] = df['Pur_Date'].dt.hour\n",
    "df['minute'] = df['Pur_Date'].dt.minute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b93de5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Encoding Days of the Week\n",
    "Extracting the **day of the week** helps in analyzing trends, such as comparing sales on different days.\n",
    "Use `.dt.day_name()` or `.dt.weekday` to get the day of the week.\n",
    "\n",
    "### Getting the Day of the Week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb86a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PurDate'] = pd.to_datetime(df['PurDate'])\n",
    "print(df[\"PurDate\"].dt.day_name())  # Monday, Tuesday, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6ef92c",
   "metadata": {},
   "source": [
    "### Getting the Day as a Number (Monday = 0, Sunday = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31933ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['PurDate'].dt.weekday)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff642ff",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **16. Tools and Utilities**\n",
    "- **`pd.read_*`**: Read from CSV, Excel, JSON, SQL, etc.\n",
    "  ```python\n",
    "  # df = pd.read_excel(\"file.xlsx\")\n",
    "  ```\n",
    "- **`.to_*`**: Export to formats.\n",
    "  ```python\n",
    "  df.to_csv(\"output.csv\", index=False)\n",
    "  ```\n",
    "- **`.value_counts()`**: Count unique values in a column.\n",
    "  ```python\n",
    "  print(df[\"Department\"].value_counts())  # Output: HR    2 \\n IT    2\n",
    "  ```\n",
    "- **`.crosstab()`**: Cross-tabulation.\n",
    "  ```python\n",
    "  print(pd.crosstab(df[\"Department\"], df[\"Salary\"]))\n",
    "  ```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}