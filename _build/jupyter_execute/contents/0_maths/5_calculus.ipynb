{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8238470",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Calculus \n",
    "\n",
    "Calculus is a branch of mathematics that studies how things change. In Machine Learning (ML) and Deep Learning (DL), it helps us improve models by finding the best settings for their parameters. Two main ideas from calculus **derivatives** and **gradients** are especially important for this process.\n",
    "\n",
    "## Why Calculus Matters in ML and DL\n",
    "\n",
    "Calculus is the mathematics of change, and in ML and DL, it’s essential for:\n",
    "- **Optimization**: Finding the best model parameters by minimizing loss functions (e.g., gradient descent).\n",
    "- **Understanding Models**: Analyzing how inputs affect outputs (e.g., backpropagation).\n",
    "- **Regularization**: Balancing model complexity and fit.\n",
    "- **Probability**: Linking to probability density functions and expectations.\n",
    "\n",
    "**Key Roles**\n",
    "1. **Derivatives**: Measure how functions change, critical for optimization.\n",
    "2. **Gradients**: Multi-dimensional derivatives, guiding parameter updates.\n",
    "3. **Integrals**: Compute areas under curves, used in probability and normalization.\n",
    "\n",
    "---\n",
    "\n",
    "# Derivatives: Measuring Change\n",
    "\n",
    "## What Are Derivatives?\n",
    "\n",
    "A **derivative** measures how a function changes when its input changes. In simple terms, it’s the \"slope\" or steepness of the function at a specific point.\n",
    "\n",
    "## Why They Help in ML\n",
    "\n",
    "In ML, we use a **loss function** to measure how far off our predictions are from the actual results. The derivative of this loss function tells us how to adjust the model’s parameters to reduce the error.\n",
    "\n",
    "## Example\n",
    "\n",
    "Consider a simple function:\n",
    "\\[\n",
    "f(x) = x^2\n",
    "\\]\n",
    "Its derivative is:\n",
    "\\[\n",
    "f'(x) = 2x\n",
    "\\]\n",
    "- At \\( x = 1 \\), the derivative is \\( f'(1) = 2 \\) (a positive slope, meaning the function is increasing).\n",
    "- At \\( x = -1 \\), it’s \\( f'(-1) = -2 \\) (a negative slope, meaning it’s decreasing).\n",
    "\n",
    "This slope helps us decide how to change \\( x \\) to make \\( f(x) \\) smaller, which is key in training ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a279386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import symbols, diff\n",
    "\n",
    "# Define symbolic variable and function\n",
    "x = symbols('x')\n",
    "f = x**2 + 3*x + 2\n",
    "\n",
    "# Derivative\n",
    "f_prime = diff(f, x)\n",
    "print(\"Derivative of f(x) = x^2 + 3x + 2:\", f_prime)  # Output: 2*x + 3\n",
    "\n",
    "# Evaluate at x = 2\n",
    "value = f_prime.subs(x, 2)\n",
    "print(\"Slope at x = 2:\", value)  # Output: 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4f9b71",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Partial Derivatives: Handling Multiple Variables\n",
    "\n",
    "## What Are Partial Derivatives?\n",
    "\n",
    "When a function has more than one variable—like \\( f(x, y) \\) a **partial derivative** measures how the function changes when only one variable changes, while the others stay fixed. In ML, models have many parameters (e.g., weights in a neural network), and partial derivatives help us adjust each one separately.\n",
    "\n",
    "## Why They’re Important\n",
    "\n",
    "ML models often predict based on multiple inputs, like size and location for house prices. Partial derivatives tell us how each parameter affects the error, letting us fine-tune them to improve predictions.\n",
    "\n",
    "## Example: Linear Regression\n",
    "\n",
    "In **linear regression**, we fit a line to data:\n",
    "\\[\n",
    "y = w_1 x + w_0\n",
    "\\]\n",
    "- \\( y \\): Predicted value\n",
    "- \\( x \\): Input (e.g., house size)\n",
    "- \\( w_1 \\): Slope (weight)\n",
    "- \\( w_0 \\): Intercept (bias)\n",
    "\n",
    "We measure error with the **Mean Squared Error (MSE)**:\n",
    "\\[\n",
    "L(w_1, w_0) = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - (w_1 x_i + w_0))^2\n",
    "\\]\n",
    "- \\( n \\): Number of data points\n",
    "- \\( y_i \\): Actual value\n",
    "- \\( x_i \\): Input value\n",
    "\n",
    "To minimize this error, we calculate partial derivatives:\n",
    "1. For \\( w_1 \\):\n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial w_1} = \\frac{2}{n} \\sum_{i=1}^{n} (y_i - (w_1 x_i + w_0)) (-x_i)\n",
    "\\]\n",
    "2. For \\( w_0 \\):\n",
    "\\[\n",
    "\\frac{\\partial L}{\\partial w_0} = \\frac{2}{n} \\sum_{i=1}^{n} (y_i - (w_1 x_i + w_0)) (-1)\n",
    "\\]\n",
    "\n",
    "These tell us how to adjust \\( w_1 \\) and \\( w_0 \\) to lower the error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d644d8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function with two variables\n",
    "y = symbols('y')\n",
    "g = x**2 + y**2 + x*y\n",
    "\n",
    "# Partial derivatives\n",
    "dg_dx = diff(g, x)\n",
    "dg_dy = diff(g, y)\n",
    "print(\"∂g/∂x:\", dg_dx)  # Output: 2*x + y\n",
    "print(\"∂g/∂y:\", dg_dy)  # Output: 2*y + x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf8b1ba",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Gradients: Directions for Improvement\n",
    "\n",
    "## What Are Gradients?\n",
    "\n",
    "A **gradient** is a collection of partial derivatives for a function with multiple variables. It’s a vector that points in the direction where the function increases the most.\n",
    "\n",
    "## How Gradients Help\n",
    "\n",
    "In ML, we use gradients to find the minimum of the loss function. By moving in the **opposite direction** of the gradient (called the negative gradient), we reduce the error step by step.\n",
    "\n",
    "---\n",
    "\n",
    "# Gradient Descent: Step-by-Step Optimization\n",
    "\n",
    "## What Is Gradient Descent?\n",
    "\n",
    "**Gradient descent** is a method to minimize a loss function by repeatedly adjusting its parameters in the opposite direction of the gradient.\n",
    "\n",
    "## How It Works\n",
    "\n",
    "Imagine you’re on a hill and want to reach the bottom:\n",
    "- The gradient shows the steepest way up.\n",
    "- You step downhill (negative gradient) a little at a time until you reach the lowest point.\n",
    "\n",
    "In ML, we update parameters like this:\n",
    "\\[\n",
    "w_1 = w_1 - \\eta \\cdot \\frac{\\partial L}{\\partial w_1}\n",
    "\\]\n",
    "\\[\n",
    "w_0 = w_0 - \\eta \\cdot \\frac{\\partial L}{\\partial w_0}\n",
    "\\]\n",
    "- \\( \\eta \\): Learning rate (how big each step is)\n",
    "\n",
    "## Example: Linear Regression in Action\n",
    "\n",
    "Here’s a small dataset:\n",
    "| \\( x \\) (Size) | \\( y \\) (Price) |\n",
    "|----------------|-----------------|\n",
    "| 1              | 2               |\n",
    "| 2              | 4               |\n",
    "| 3              | 6               |\n",
    "\n",
    "Using gradient descent, we adjust \\( w_1 \\) and \\( w_0 \\) until the line \\( y = w_1 x + w_0 \\) fits the data. After many steps, \\( w_1 \\) becomes 2 and \\( w_0 \\) becomes 0, giving \\( y = 2x \\)—the best fit.\n",
    "\n",
    "## Python Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee225ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sample data\n",
    "X = np.array([[1], [2], [3]])\n",
    "y = np.array([[2], [4], [6]])\n",
    "\n",
    "# Starting values\n",
    "m = 0.0  # Slope\n",
    "b = 0.0  # Intercept\n",
    "learning_rate = 0.01\n",
    "steps = 1000\n",
    "n = len(X)\n",
    "\n",
    "# Gradient descent\n",
    "for _ in range(steps):\n",
    "    y_pred = m * X + b\n",
    "    gradient_m = (-2/n) * np.sum(X * (y - y_pred))\n",
    "    gradient_b = (-2/n) * np.sum(y - y_pred)\n",
    "    m -= learning_rate * gradient_m\n",
    "    b -= learning_rate * gradient_b\n",
    "\n",
    "# Plot results\n",
    "plt.scatter(X, y, color='blue', label='Data')\n",
    "plt.plot(X, m * X + b, color='red', label='Best Fit Line')\n",
    "plt.xlabel('Size')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Linear Regression with Gradient Descent')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Slope (m): {m:.2f}, Intercept (b): {b:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb32916",
   "metadata": {},
   "source": [
    "This code finds the best-fit line by minimizing the error.\n",
    "\n",
    "---\n",
    "\n",
    "# Convolutional Neural Networks (CNNs): Calculus in Deep Learning\n",
    "\n",
    "### What Are CNNs?\n",
    "\n",
    "**Convolutional Neural Networks (CNNs)** are DL models designed for tasks like image recognition. They use calculus to learn patterns (e.g., edges, shapes) in images.\n",
    "\n",
    "### How Calculus Helps CNNs\n",
    "\n",
    "CNNs minimize a loss function using gradient descent, just like linear regression. However, they have many more parameters (weights in layers), and partial derivatives guide updates across all of them.\n",
    "\n",
    "### Key Parts of a CNN\n",
    "\n",
    "1. **Convolutional Layer**: Detects patterns using filters.\n",
    "2. **Pooling Layer**: Shrinks data to focus on key features.\n",
    "3. **Fully Connected Layer**: Combines features for predictions.\n",
    "\n",
    "### Example: Image Classification\n",
    "\n",
    "Here’s a simple CNN for classifying images from the CIFAR-10 dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54528a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize\n",
    "\n",
    "# Build CNN\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10)  # 10 classes\n",
    "])\n",
    "\n",
    "# Compile and train\n",
    "model.compile(optimizer='adam', loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))\n",
    "\n",
    "# Test accuracy\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_acc:.2f}\")\n",
    "\n",
    "# Show a prediction\n",
    "predictions = model.predict(x_test)\n",
    "plt.imshow(x_test[0])\n",
    "plt.title(f\"Predicted: {predictions[0].argmax()}, True: {y_test[0][0]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f5d032",
   "metadata": {},
   "source": [
    "This CNN uses gradient descent to adjust weights and classify images.\n",
    "\n",
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "- **Derivatives** show how a function changes, helping adjust model parameters.\n",
    "- **Partial Derivatives** handle multiple variables, key for complex models.\n",
    "- **Gradients** guide us to the minimum error using **gradient descent**.\n",
    "- In ML (e.g., linear regression) and DL (e.g., CNNs), calculus optimizes models for better predictions.\n",
    "\n",
    "---\n",
    "\n",
    "# **What are Integrals in ML?**\n",
    "Integrals represent the accumulation of quantities, often visualized as the area under a curve. In ML, they’re used to:\n",
    "- **Compute Probabilities**: Integrate probability density functions (PDFs) over intervals.\n",
    "- **Normalization**: Ensure distributions sum to 1.\n",
    "- **Expectations**: Calculate averages or moments of random variables.\n",
    "- **Loss Functions**: Define continuous objectives in some models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5daff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import integrate\n",
    "\n",
    "# Integral of f(x) = 2x\n",
    "h = 2*x\n",
    "integral = integrate(h, x)\n",
    "print(\"Integral of 2x:\", integral)  # Output: x^2\n",
    "\n",
    "# Definite integral from 0 to 2\n",
    "def_integral = integrate(h, (x, 0, 2))\n",
    "print(\"Area from 0 to 2:\", def_integral)  # Output: 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121e0ff4",
   "metadata": {},
   "source": [
    "## **a. Probability and PDFs**\n",
    "- **Concept**: For a continuous random variable \\(X\\) with PDF \\(f(x)\\), the probability over an interval \\([a, b]\\) is:\n",
    "  \\[\n",
    "  P(a \\leq X \\leq b) = \\int_a^b f(x) \\, dx\n",
    "  \\]\n",
    "- **Use**: Compute likelihoods in generative models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0131efd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Normal distribution (mu=0, sigma=1)\n",
    "norm = stats.norm(loc=0, scale=1)\n",
    "x = np.linspace(-3, 3, 100)\n",
    "pdf = norm.pdf(x)\n",
    "\n",
    "# Probability P(-1 < X < 1)\n",
    "prob = norm.cdf(1) - norm.cdf(-1)  # Cumulative Distribution Function (integral)\n",
    "print(\"P(-1 < X < 1):\", prob)  # Output: ~0.6827\n",
    "\n",
    "# Visualize\n",
    "plt.plot(x, pdf, label=\"PDF\")\n",
    "plt.fill_between(x, pdf, where=(x >= -1) & (x <= 1), alpha=0.3, color=\"skyblue\", label=\"P(-1 < X < 1)\")\n",
    "plt.title(\"Normal Distribution\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b6e458",
   "metadata": {},
   "source": [
    "## **b. Normalization**\n",
    "- **Concept**: Ensure a PDF integrates to 1 over its domain: \\( \\int_{-\\infty}^{\\infty} f(x) \\, dx = 1 \\).\n",
    "- **Use**: Validate or adjust custom distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea498c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy import symbols, integrate, exp\n",
    "\n",
    "# Symbolic example: Exponential PDF\n",
    "x = symbols('x')\n",
    "lam = 2  # Rate parameter\n",
    "f = lam * exp(-lam * x)  # f(x) = λe^(-λx), x >= 0\n",
    "\n",
    "# Check normalization\n",
    "integral = integrate(f, (x, 0, np.inf))\n",
    "print(\"Integral (Normalization):\", integral)  # Output: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886f2932",
   "metadata": {},
   "source": [
    "## **c. Expectation (Mean)**\n",
    "- **Concept**: \\( E[X] = \\int_{-\\infty}^{\\infty} x f(x) \\, dx \\) for continuous variables.\n",
    "- **Use**: Predict average outcomes in probabilistic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5b4982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical integration for E[X] (normal distribution)\n",
    "from scipy.integrate import quad\n",
    "\n",
    "def integrand(x):\n",
    "    return x * norm.pdf(x)\n",
    "\n",
    "expectation, _ = quad(integrand, -np.inf, np.inf)\n",
    "print(\"Expected Value (Mean):\", expectation)  # Output: ~0 (matches mu=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6cf973",
   "metadata": {},
   "source": [
    "## **d. Variance**\n",
    "- **Concept**: \\( \\text{Var}(X) = E[(X - \\mu)^2] = \\int_{-\\infty}^{\\infty} (x - \\mu)^2 f(x) \\, dx \\).\n",
    "- **Use**: Measure spread in uncertainty quantification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ec496e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def var_integrand(x):\n",
    "    return (x - norm.mean())**2 * norm.pdf(x)\n",
    "\n",
    "variance, _ = quad(var_integrand, -np.inf, np.inf)\n",
    "print(\"Variance:\", variance)  # Output: ~1 (matches sigma^2=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a9d28c",
   "metadata": {},
   "source": [
    "## **e. Bayesian Marginalization**\n",
    "- **Concept**: Marginal probability \\( P(X) = \\int P(X, Y) \\, dY \\).\n",
    "- **Use**: Integrate out nuisance variables in Bayesian ML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acd1563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint PDF: P(X, Y) = N(X; 0, 1) * N(Y; 0, 1)\n",
    "def joint_pdf(x, y):\n",
    "    return norm.pdf(x) * norm.pdf(y)\n",
    "\n",
    "# Marginal P(X) at x=0\n",
    "marginal, _ = quad(lambda y: joint_pdf(0, y), -np.inf, np.inf)\n",
    "print(\"Marginal P(X=0):\", marginal)  # Output: ~0.3989 (matches N(0,1) PDF at 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09cf949",
   "metadata": {},
   "source": [
    "---\n",
    "## **5. Integrals in ML Algorithms**\n",
    "\n",
    "### **a. Kernel Density Estimation (KDE)**\n",
    "- Integrates kernel functions to estimate PDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d252404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "kde = KernelDensity(kernel=\"gaussian\", bandwidth=1).fit(tenure.reshape(-1, 1))\n",
    "x = np.linspace(0, 30, 100)\n",
    "log_dens = kde.score_samples(x.reshape(-1, 1))\n",
    "plt.plot(x, np.exp(log_dens), label=\"KDE\")\n",
    "plt.title(\"KDE of Tenure\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a8bcd4",
   "metadata": {},
   "source": [
    "### **b. Bayesian Neural Networks**\n",
    "- Integrate over weight distributions (often approximated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7226e7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "\n",
    "# Simple BNN prior\n",
    "dist = tfp.distributions.Normal(loc=0., scale=1.)\n",
    "x = np.linspace(-3, 3, 100)\n",
    "pdf = dist.prob(x)\n",
    "plt.plot(x, pdf)\n",
    "plt.title(\"Prior Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe6387b",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}