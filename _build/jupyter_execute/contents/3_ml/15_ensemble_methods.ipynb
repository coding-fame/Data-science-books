{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "939cc5f7",
   "metadata": {},
   "source": [
    "# ðŸŒ³ Random Forest Algorithm\n",
    "\n",
    "The Random Forest algorithm is a powerful and versatile supervised machine learning technique widely used for both classification and regression tasks. It builds an ensemble of decision trees, combining their predictions to improve accuracy, reduce overfitting, and enhance robustness. \n",
    "\n",
    "---\n",
    "\n",
    "## **What is the Random Forest Algorithm?**\n",
    "Random Forest is an **ensemble learning** technique that combines the predictions of multiple decision trees to produce a more accurate and stable outcome. \n",
    "Each tree is trained on a random subset of the data and features, and the final prediction is determined by aggregating the individual tree predictions typically by majority voting for classification or averaging for regression.\n",
    "\n",
    "### **Why Use Random Forest?**\n",
    "- **Accuracy**: Often outperforms single decision trees by reducing variance.\n",
    "- **Robustness**: Handles noisy data and overfitting well.\n",
    "- **Versatility**: Works for both classification (e.g., spam detection) and regression (e.g., house price prediction).\n",
    "- **Feature Importance**: Provides insights into which features drive predictions.\n",
    "\n",
    "### **How It Works**\n",
    "1. **Bootstrap (Bagging) Sampling**: Create multiple subsets of the data with replacement (bagging). Each subset trains a separate decision tree.\n",
    "2. **Feature Randomness**: At each split in a decision tree, Random Forest considers only a random subset of features (e.g., `sqrt(n_features)` by default in `scikit-learn`). The best feature from this subset is chosen for the split.\n",
    "3. **Tree Construction**: Build independent decision trees on each subset.\n",
    "4. **Aggregation**: Combining predictions of all decision trees and takes the **majority vote** (for classification) or **average** (for regression) as the final prediction.\n",
    "\n",
    "### The Random Forest Algorithm Steps\n",
    "1. **Draw Random Samples**: Create multiple bootstrap samples from the training dataset.\n",
    "2. **Build Decision Trees**: For each sample, grow a tree, selecting a random subset of features at each node to determine the best split.\n",
    "3. **Repeat**: Generate a forest of diverse trees (e.g., 100 or 500 trees).\n",
    "4. **Aggregate Predictions**: For a new data point, collect predictions from all trees and compute the final output via voting (classification) or averaging (regression).\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages and Drawbacks\n",
    "\n",
    "### Advantages\n",
    "- **Improved Accuracy**: Outperforms single decision trees and many other algorithms.\n",
    "- **Reduced Overfitting**: Randomness and averaging mitigate overfitting risks.\n",
    "- **Handles Missing Values**: Can work with incomplete datasets.\n",
    "- **Feature Importance**: Quantifies the contribution of each feature.\n",
    "- **Parallelizable**: Trees can be trained independently, speeding up computation.\n",
    "\n",
    "### Drawbacks\n",
    "- **Complexity**: A large forest can be memory-intensive and hard to interpret.\n",
    "- **Computationally Intensive**: Training and prediction take longer than simpler models.\n",
    "- **Less Interpretable**: The ensemble obscures the simplicity of individual trees.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Concepts and Methods**\n",
    "\n",
    "### **a. Core Mechanics**\n",
    "- **Bagging (Bootstrap Aggregating)**: Reduces variance by averaging predictions from diverse trees.\n",
    "- **Random Feature Selection**: Ensures trees are decorrelated by limiting feature choices at splits.\n",
    "- **Out-of-Bag (OOB) Error**: Estimates generalization error using samples not included in each treeâ€™s bootstrap sample.\n",
    "\n",
    "### **b. Hyperparameters**\n",
    "- `n_estimators`: Number of trees in the forest.\n",
    "- `max_depth`: Maximum depth of each tree.\n",
    "- `min_samples_split`: Minimum samples required to split a node.\n",
    "- `max_features`: Number of features considered at each split (e.g., \"sqrt\", \"log2\").\n",
    "- `oob_score`: Use out-of-bag samples for validation.\n",
    "\n",
    "---\n",
    "\n",
    "## **Practical Examples**\n",
    "\n",
    "### **Example 1: Classification (Iris Dataset)**\n",
    "\n",
    "Weâ€™ll implement Random Forest using `scikit-learn` and the Iris dataset, a classic dataset with 150 samples, 4 features (sepal length, sepal width, petal length, petal width), and 3 classes (setosa, versicolor, virginica).\n",
    "\n",
    "### Step 1: Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86867b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features (4 columns)\n",
    "y = iris.target  # Target labels (0, 1, 2)\n",
    "\n",
    "# Optional: Convert to DataFrame for exploration\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "df['target'] = y\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4da306",
   "metadata": {},
   "source": [
    "### Step 2: Split Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb5c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training samples: {X_train.shape[0]}, Testing samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985635cf",
   "metadata": {},
   "source": [
    "### Step 3: Train the Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd3ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Random Forest classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2dbda2",
   "metadata": {},
   "source": [
    "- `n_estimators=100`: Number of trees in the forest.\n",
    "- `random_state=42`: Ensures reproducibility.\n",
    "\n",
    "### Step 4: Make Predictions and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4c0bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Detailed evaluation\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38157b6b",
   "metadata": {},
   "source": [
    "### Step 5: Analyze Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93665214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature importances\n",
    "importances = rf_clf.feature_importances_\n",
    "feature_names = iris.feature_names\n",
    "\n",
    "# Display feature importances\n",
    "print(\"\\nFeature Importances:\")\n",
    "for feature, importance in zip(feature_names, importances):\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c44c38",
   "metadata": {},
   "source": [
    "### Example Output\n",
    "Assuming the code runs on the Iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c872b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Training samples: 120, Testing samples: 30\n",
    "Accuracy: 1.00\n",
    "\n",
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "    setosa      1.00      1.00      1.00        10\n",
    "versicolor      1.00      1.00      1.00         9\n",
    " virginica      1.00      1.00      1.00        11\n",
    "\n",
    "Feature Importances:\n",
    "sepal length (cm): 0.1023\n",
    "sepal width (cm): 0.0234\n",
    "petal length (cm): 0.4412\n",
    "petal width (cm): 0.4331"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2764b201",
   "metadata": {},
   "source": [
    "(Note: Exact values may vary slightly due to randomness, but the Iris dataset is small and well-separated, often yielding near-perfect accuracy.)\n",
    "\n",
    "---\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "To optimize Random Forest, we can tune hyperparameters like `n_estimators`, `max_depth`, and `min_samples_split` using `GridSearchCV` or `RandomizedSearchCV`.\n",
    "\n",
    "### Example: Grid Search for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b83a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5, n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.2f}\")\n",
    "\n",
    "# Use the best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "y_pred_best = best_rf.predict(X_test)\n",
    "print(f\"Test accuracy with best model: {accuracy_score(y_test, y_pred_best):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8d3796",
   "metadata": {},
   "source": [
    "- `cv=5`: 5-fold cross-validation.\n",
    "- `n_jobs=-1`: Use all available CPU cores.\n",
    "\n",
    "### Example Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc35726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Best parameters: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
    "Best cross-validation score: 0.97\n",
    "Test accuracy with best model: 1.00"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853c193d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Interpreting the Model\n",
    "\n",
    "While Random Forest is less interpretable than a single tree, several tools help:\n",
    "- **Feature Importance**: Already shown above, highlights key predictors (e.g., petal length and width dominate in Iris).\n",
    "- **Partial Dependence Plots**: Visualize the effect of a feature on predictions.\n",
    "- **Tree Visualization**: Inspect individual trees (though less practical for large forests).\n",
    "\n",
    "### Example: Visualize a Single Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e9921",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the first tree in the forest\n",
    "plt.figure(figsize=(20, 10))\n",
    "plot_tree(rf_clf.estimators_[0], feature_names=iris.feature_names, class_names=iris.target_names, filled=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287b58ea",
   "metadata": {},
   "source": [
    "### Example: Partial Dependence Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf9f083",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "# Plot partial dependence for petal length (feature 2)\n",
    "PartialDependenceDisplay.from_estimator(rf_clf, X_train, features=[2], target=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69452a07",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Example 2: Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aeb336",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Synthetic regression data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = np.sin(X) + np.random.normal(0, 0.1, X.shape)\n",
    "\n",
    "# Fit Random Forest\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, max_depth=3, random_state=42)\n",
    "rf_reg.fit(X, y.ravel())\n",
    "\n",
    "# Predict\n",
    "y_pred = rf_reg.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(\"MSE:\", mse)\n",
    "\n",
    "# Visualize\n",
    "plt.scatter(X, y, label=\"Data\", alpha=0.5)\n",
    "plt.plot(X, y_pred, color=\"red\", label=\"RF Fit\")\n",
    "plt.title(\"Random Forest Regression\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc9bcc2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **4. Tools and Methods Summary**\n",
    "- **Modeling**: `sklearn.ensemble.RandomForestClassifier`, `RandomForestRegressor`.\n",
    "- **Evaluation**: `sklearn.metrics.accuracy_score`, `mean_squared_error`.\n",
    "- **Tuning**: `sklearn.model_selection.GridSearchCV`.\n",
    "- **Visualization**: `seaborn.barplot()` for feature importance.\n",
    "- **Feature Importance**: `.feature_importances_`.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "Random Forest is a powerful ensemble algorithm that combines the simplicity of decision trees with the strength of bagging and feature randomness, delivering high accuracy and robustness.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}