{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20280832",
   "metadata": {},
   "source": [
    "# Support Vector Machine (SVM) in Detail\n",
    "\n",
    "Support Vector Machine (SVM) is a powerful supervised machine learning algorithm widely used for classification and regression tasks. It excels in high-dimensional spaces and is applied in domains like image recognition, text classification, and bioinformatics.\n",
    "\n",
    "---\n",
    "\n",
    "SVM aims to find the best decision boundary that separates different classes in an **n-dimensional space**.\n",
    "\n",
    "## **What is a Support Vector Machine (SVM)?**\n",
    "- SVM is a machine learning algorithm that finds the optimal **hyperplane** to separate data points of different classes with the maximum margin. \n",
    "- The hyperplane is chosen to maximize the **margin**, which is the distance between the hyperplane and the nearest data points from each class, known as **support vectors**. \n",
    "- When data is not linearly separable, SVM uses the **kernel trick** to transform it into a higher-dimensional space where a linear boundary can be established.\n",
    "\n",
    "## Why is it called Support Vector Machine?\n",
    "- SVM selects **extreme data points (vectors)** that define the decision boundary.\n",
    "- These data points are known as **Support Vectors**.\n",
    "- The model is named **Support Vector Machine** because these vectors help construct the hyperplane.\n",
    "\n",
    "### **Why Use SVM?**\n",
    "- **Maximizes Margin**: Robust to noise and outliers due to its focus on the widest separation.\n",
    "- **Versatility**: Handles linear and non-linear problems via kernels.\n",
    "- **High-Dimensional Efficiency**: Excels in datasets with many features (e.g., text classification).\n",
    "- **Strong Theoretical Foundation**: Rooted in optimization and geometry.\n",
    "\n",
    "### **How It Works**\n",
    "1. **Hyperplane**: A decision boundary separating classes (e.g., a line in 2D, a plane in 3D).\n",
    "2. **Support Vectors**: Data points closest to the hyperplane that define its position and orientation.\n",
    "3. The **optimal hyperplane** is chosen by maximizing the margin (distance between support vectors and the hyperplane).\n",
    "4. **Margin**: The distance between the hyperplane and the support vectors. SVM maximizes this margin for better generalization.\n",
    "5. **Kernel Trick**: A technique to handle non-linear data by mapping it to a higher-dimensional space.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Key Concepts and Methods**\n",
    "\n",
    "### Types of SVM\n",
    "1. Linear SVM: Used when data is linearly separable, meaning a straight line (or hyperplane) can separate the classes.\n",
    "\n",
    "2. Non-Linear SVM: Used when data is not linearly separable. It employs kernel functions to transform the data into a space where it becomes linearly separable.\n",
    "\n",
    "3. Support Vector Regression (SVR): An extension of SVM for regression tasks, aiming to fit a function within a margin of tolerance around the target values.\n",
    "\n",
    "### Kernel Functions\n",
    "Kernel functions transform data into higher dimensions. Common options include:\n",
    "\n",
    "1. **Linear Kernel**: For linearly separable data (`kernel='linear'`). \\( K(x, x') = x^T x' \\) (simple dot product).\n",
    "2. **Polynomial Kernel**: Captures polynomial relationships (`kernel='poly'`). \\( K(x, x') = (x^T x' + c)^d \\) (captures interactions).\n",
    "3. **Radial Basis Function (RBF) Kernel**: Handles complex, non-linear patterns (`kernel='rbf'`). \\( K(x, x') = \\exp(-\\gamma \\|x - x'\\|^2) \\) (non-linear, default).\n",
    "4. **Sigmoid Kernel**: Mimics neural network behavior (`kernel='sigmoid'`). \\( K(x, x') = \\tanh(\\gamma x^T x' + c) \\).\n",
    "\n",
    "### **c. Hyperparameters**\n",
    "- `C`: Regularization parameter (small \\(C\\) = larger margin, more errors; large \\(C\\) = smaller margin, fewer errors).\n",
    "- `kernel`: Type of kernel (e.g., \"linear\", \"rbf\").\n",
    "- `gamma`: Kernel coefficient for RBF/polynomial (small = broader influence, large = tighter fit).\n",
    "\n",
    "---\n",
    "\n",
    "## SVM for Classification\n",
    "\n",
    "### Binary Classification\n",
    "In binary classification, SVM finds the hyperplane that best separates two classes by maximizing the margin.\n",
    "\n",
    "### Multi-Class Classification\n",
    "For problems with more than two classes, SVM uses strategies like:\n",
    "\n",
    "1. **One-vs-One (OvO)**: Trains a classifier for each pair of classes.\n",
    "2. **One-vs-All (OvA)**: Trains a classifier for each class against all others.\n",
    "\n",
    "---\n",
    "\n",
    "## SVM for Regression\n",
    "\n",
    "In **Support Vector Regression (SVR)**, the goal is to predict continuous values by finding a function that keeps most data points within a specified margin (controlled by the parameter `epsilon`).\n",
    "\n",
    "---\n",
    "## Advantages of SVM\n",
    "\n",
    "- **High-Dimensional Effectiveness**: Performs well with many features.\n",
    "- **Robustness**: Less prone to overfitting with proper regularization.\n",
    "- **Versatility**: Handles both linear and non-linear data via kernels.\n",
    "\n",
    "## Disadvantages of SVM\n",
    "\n",
    "- **Computationally Intensive**: Slow training on large datasets.\n",
    "- **Kernel Selection**: Choosing the right kernel and parameters is tricky.\n",
    "- **Interpretability**: Less intuitive than models like decision trees.\n",
    "\n",
    "---\n",
    "\n",
    "# Example 1: SVM for Classification (Iris Dataset)\n",
    "\n",
    "Let’s classify the Iris dataset using SVM with a linear kernel.\n",
    "\n",
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66edb2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca3674c",
   "metadata": {},
   "source": [
    "## Step 2: Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a13f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Use first two features (sepal length, sepal width) for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3976c7",
   "metadata": {},
   "source": [
    "## Step 3: Train the SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df4fd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train an SVM classifier with a linear kernel\n",
    "svm_clf = SVC(kernel='linear', C=1.0)\n",
    "svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2694cd7a",
   "metadata": {},
   "source": [
    "- `C`: Regularization parameter (smaller values increase the margin, larger values reduce it).\n",
    "\n",
    "## Step 4: Make Predictions and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969a600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8ca01a",
   "metadata": {},
   "source": [
    "## Step 5: Visualize the Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2aae8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(X, y, model):\n",
    "    h = .02  # Step size for mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', marker='o')\n",
    "    plt.xlabel('Sepal Length')\n",
    "    plt.ylabel('Sepal Width')\n",
    "    plt.title('SVM Decision Boundary')\n",
    "    plt.show()\n",
    "\n",
    "# Plot\n",
    "plot_decision_boundary(X_train, y_train, svm_clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207eb679",
   "metadata": {},
   "source": [
    "This code visualizes how SVM separates the three Iris classes based on sepal length and width.\n",
    "\n",
    "---\n",
    "\n",
    "# Example 2: SVM for Regression (Boston Housing Dataset)\n",
    "\n",
    "Let’s predict house prices using SVR with an RBF kernel.\n",
    "\n",
    "## Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d2c89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aabc955",
   "metadata": {},
   "source": [
    "## Step 2: Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53d8d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Boston Housing dataset\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee290430",
   "metadata": {},
   "source": [
    "## Step 3: Train the SVR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c31371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train an SVR model with RBF kernel\n",
    "svr = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "svr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b237741",
   "metadata": {},
   "source": [
    "- `epsilon`: Defines the margin of tolerance where no penalty is given to errors.\n",
    "\n",
    "## Step 4: Make Predictions and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65865692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_pred = svr.predict(X_test)\n",
    "\n",
    "# Evaluate with Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c75e8e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hyperparameter Tuning\n",
    "\n",
    "SVM performance depends on parameters like:\n",
    "- `C`: Controls the trade-off between margin maximization and classification error.\n",
    "- `gamma`: Defines the influence of a single training example (RBF kernel).\n",
    "- `kernel`: Determines the type of transformation.\n",
    "\n",
    "### Example: Grid Search for Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac016620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': [0.001, 0.01, 0.1],\n",
    "    'kernel': ['linear', 'rbf']\n",
    "}\n",
    "\n",
    "# Create SVM classifier\n",
    "svm = SVC()\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Results\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "# Test the best model\n",
    "best_svm = grid_search.best_estimator_\n",
    "y_pred_best = best_svm.predict(X_test)\n",
    "print(\"Test Accuracy with Best Model:\", accuracy_score(y_test, y_pred_best))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba95ba6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **4. Tools and Methods Summary**\n",
    "- **Modeling**: `sklearn.svm.SVC`, `SVR`.\n",
    "- **Evaluation**: `sklearn.metrics.accuracy_score`, `mean_squared_error`.\n",
    "- **Tuning**: `sklearn.model_selection.GridSearchCV`.\n",
    "- **Visualization**: `matplotlib.pyplot.contourf()`, `seaborn.scatterplot()`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efa135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Detailed evaluation\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9fbeb5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Key Considerations**\n",
    "- **Kernel Choice**: Linear for separable data, RBF/polynomial for non-linear.\n",
    "- **Parameter Tuning**: \\(C\\) (trade-off between margin and errors), `gamma` (kernel shape).\n",
    "- **Feature Scaling**: Critical—SVM is sensitive to feature magnitude.\n",
    "- **Computational Cost**: Scales poorly with large datasets (\\(O(n^2)\\) or worse).\n",
    "\n",
    "#### **Tuning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7c4c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Parameter grid\n",
    "param_grid = {\"C\": [0.1, 1, 10], \"gamma\": [\"scale\", \"auto\", 0.1], \"kernel\": [\"linear\", \"rbf\"]}\n",
    "grid = GridSearchCV(SVC(), param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best Params:\", grid.best_params_)\n",
    "print(\"Best Score:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2e5576",
   "metadata": {},
   "source": [
    "#### **Scaling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4722ff41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "svm_rbf.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c768abe",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}