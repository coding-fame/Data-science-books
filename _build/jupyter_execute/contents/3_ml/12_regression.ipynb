{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49aecdca",
   "metadata": {},
   "source": [
    "# Regression Algorithms\n",
    "\n",
    "---\n",
    "\n",
    "# ML: R-Value and Regression Analysis\n",
    "\n",
    "## üîç Introduction to Regression\n",
    "Regression analysis is a fundamental concept in Machine Learning used to explain the relationship between a **dependent variable** and one or more **independent variables**.\n",
    "  \n",
    "---\n",
    "\n",
    "## üìà Understanding the Regression Line\n",
    "- If two variables are related, we can visualize their relationship in a **two-dimensional space**.\n",
    "- The result is often a **straight line** that represents their correlation.\n",
    "- Think of it as plotting scattered points and finding the best possible line to pass through them.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ The Goal of Regression\n",
    "- The primary objective of **Linear Regression** is to draw the **best-fit line**.\n",
    "- A **best-fit line** is the one that passes as **close as possible** to all data points.\n",
    "\n",
    "---\n",
    "\n",
    "## ü§î When to Use Regression?\n",
    "\n",
    "**Linear Regression** can only be applied when there is a clear relationship between the variables.\n",
    "\n",
    "| Scenario | Can We Use Regression? |\n",
    "|----------|----------------------|\n",
    "| There is a **strong relationship** between variables | ‚úÖ Yes |\n",
    "| There is **no significant relationship** between variables | ‚ùå No |\n",
    "\n",
    "---\n",
    "\n",
    "## üìå What is R Value?\n",
    "- **R value (correlation coefficient)** helps measure how strongly two variables are related.\n",
    "- It is a critical step in determining whether **Linear Regression** can be applied to a dataset.\n",
    "\n",
    "### üìè R Value Range Interpretation\n",
    "| R Value | Interpretation |\n",
    "|---------|---------------|\n",
    "| **1.0 or -1.0** | Strong relationship between variables (‚úÖ Regression is applicable) |\n",
    "| **Close to 0** | Weak or no relationship (‚ùå Regression is not applicable) |\n",
    "\n",
    "---\n",
    "\n",
    "## üßë‚Äçüíª Calculating R Value in Python\n",
    "We can use the **scipy.stats** module to compute the R value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3184b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Example dataset\n",
    "X = [1, 2, 3, 4, 5]\n",
    "y = [2, 4, 6, 8, 10]\n",
    "\n",
    "# Calculate regression statistics\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(X, y)\n",
    "\n",
    "print(\"R Value:\", r_value)  # Output: 1.0 (Strong Relationship)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4963cf20",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Example Interpretations\n",
    "### ‚úÖ Strong Relationship (Regression is useful)\n",
    "- **Result**: `R Value = 1.0`\n",
    "- **Conclusion**: A strong relationship exists ‚Üí **Linear Regression can be applied** for future predictions.\n",
    "\n",
    "### ‚ùå Weak or No Relationship (Regression is not useful)\n",
    "- **Result**: `R Value = -0.065`\n",
    "- **Conclusion**: The variables are **not related** ‚Üí **Linear Regression will give inaccurate predictions**.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "- **Linear regression** is a powerful tool, but it‚Äôs essential to first check if there‚Äôs a **relationship** between the variables.\n",
    "- The **r value** helps in assessing the strength of this relationship, guiding whether regression is the right approach.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# Linear Regression\n",
    "\n",
    "Linear regression is a fundamental algorithm used in **supervised learning** for predicting continuous values.\n",
    "\n",
    "## Introduction to Regression\n",
    "**Regression analysis** is used to understand the relationship between two variables. It is widely used in data science and machine learning for **predictive modeling**.\n",
    "\n",
    "# What Is Linear Regression?\n",
    "\n",
    "**Linear Regression** is a fundamental machine learning technique used to model the relationship between **dependent** and **independent** variables. It helps in predicting outcomes based on input data.\n",
    "\n",
    "Linear regression assumes that the relationship between variables can be represented by a straight line (or a flat surface in higher dimensions). It‚Äôs widely used because it‚Äôs simple, effective, and provides a strong foundation for more advanced ML techniques.\n",
    "\n",
    "### Types of Linear Regression\n",
    "There are two main types:\n",
    "\n",
    "### Simple Linear Regression\n",
    "- Uses **one independent variable** (X) to predict **one dependent variable** (Y).\n",
    "- The goal is to draw a **straight line** that best fits the data, showing how changes in the independent variable affect the dependent variable.\n",
    "- Example: Predicting house price based on area.\n",
    "- Formula:\n",
    "  ```\n",
    "  y = m * X + b\n",
    "  ```\n",
    "  Where:  \n",
    "  - **y** = Dependent variable (Predicted output)  \n",
    "  - **X** = Independent variable (Input feature)  \n",
    "  - **m** = Slope (Coefficient)  \n",
    "  - **b** = Intercept (Constant)\n",
    "\n",
    "This formula creates a straight line, where **m** controls the tilt and **b** shifts the line up or down.\n",
    "\n",
    "### Multiple Linear Regression\n",
    "- Uses **two or more independent variables** (X‚ÇÅ, X‚ÇÇ, ‚Ä¶ X‚Çô) to predict **one dependent variable** (Y).\n",
    "- The goal is to find the coefficients that define the best-fitting **hyperplane**, minimizing the sum of squared differences between actual and predicted values.\n",
    "- Example: Predicting house price based on **area, location, and number of bedrooms**.\n",
    "- Formula:  \n",
    "  ```\n",
    "  y = m‚ÇÅ*X‚ÇÅ + m‚ÇÇ*X‚ÇÇ + .... + m‚Çô*X‚Çô + b\n",
    "  ```  \n",
    "  Where:\n",
    "  - **Y** = Predicted price\n",
    "  - **X‚ÇÅ, X‚ÇÇ, X‚ÇÉ** = Independent variables (Area, Bedrooms, Age)\n",
    "  - **m‚ÇÅ, m‚ÇÇ, m‚ÇÉ** = Coefficients (Slope values) (the effect of each x on y)\n",
    "  - **b** = Intercept (Bias)\n",
    "\n",
    "This formula creates a straight line, where **m** controls the tilt and **b** shifts the line up or down.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### 1. Coefficients\n",
    "- **Slope (m)**: Measures the impact of the independent variable. For example, if m = 135.79, the price increases by $135.79 for each square foot.\n",
    "- **Intercept (b)**: The starting point of the line when x = 0.\n",
    "- The **intercept (b)** and **coefficients (M‚ÇÅ, M‚ÇÇ, M‚ÇÉ)** define the linear equation.\n",
    "\n",
    "### 2. Assumptions\n",
    "Simple linear regression works best when:\n",
    "- The relationship between x and y is **linear** (a straight line fits).\n",
    "- Data points are **independent** (one doesn‚Äôt affect another) that is uncorrelated (No Multicollinearity).\n",
    "- Errors (differences between actual and predicted values) have **constant variance** (Homoscedasticity) and are **normally distributed**.\n",
    "\n",
    "### 3. Evaluation Metrics\n",
    "To check how good the model is:\n",
    "- **Mean Squared Error (MSE)**: Average of squared differences between actual and predicted values. Lower is better.\n",
    "- **R-squared (R¬≤)**: Shows how much of the data‚Äôs variation the model explains (0 to 1, closer to 1 is better).\n",
    "\n",
    "### 4. Best Fit line\n",
    "- The \"best-fitting\" line is the one that gets as close as possible to all data points. \n",
    "- In ML, this is done by minimizing the **sum of squared errors (SSE)** the total difference between actual and predicted values.\n",
    "- The model finds the best-fit line that **minimizes the prediction error**.\n",
    "\n",
    "## Limitations\n",
    "- Assumes linearity; struggles with complex non-linear relationships.\n",
    "- Sensitive to outliers and multicollinearity.\n",
    "- Requires careful validation of assumptions.\n",
    "\n",
    "## Handling Violations of Assumptions\n",
    "- **Non-linearity**: Add polynomial/interaction terms.\n",
    "- **Heteroscedasticity**: Use weighted least squares or transform Y.\n",
    "- **Non-normality**: Transform Y (e.g., log transformation).\n",
    "- **Multicollinearity**: Remove correlated variables or use regularization (Ridge/Lasso).\n",
    "\n",
    "### Regularization Techniques\n",
    "- Ridge Regression (L2): Shrinks coefficients but does not eliminate them.\n",
    "- Lasso Regression (L1): Performs feature selection by driving some coefficients to zero.\n",
    "\n",
    "---\n",
    "\n",
    "# Simple Linear: Predicting House Price\n",
    "\n",
    "Let‚Äôs use simple linear regression to predict home prices based on their area. Here, **area** is the independent variable (x), and **price** is the dependent variable (y).\n",
    "\n",
    "## Step 1: Set Up the Tools\n",
    "We‚Äôll use Python and the **Scikit-Learn** library, a popular tool in ML, to build our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952eebda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ebd5f3",
   "metadata": {},
   "source": [
    "## Step 2: Prepare the Data\n",
    "Imagine we have this data:\n",
    "\n",
    "| Area (sq. ft) | Price ($) |\n",
    "|---------------|-----------|\n",
    "| 1500          | 300,000   |\n",
    "| 1800          | 350,000   |\n",
    "| 2000          | 400,000   |\n",
    "| 2200          | 420,000   |\n",
    "| 2500          | 480,000   |\n",
    "\n",
    "We‚Äôll store it in a table (DataFrame) using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b3b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'area': [1500, 1800, 2000, 2200, 2500],\n",
    "        'price': [300000, 350000, 400000, 420000, 480000]}\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a020f3",
   "metadata": {},
   "source": [
    "## Step 3: Train the Model\n",
    "We create a linear regression model and train it with our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf517d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(df[['area']], df['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c7c8cb",
   "metadata": {},
   "source": [
    "- `fit()` finds the best **m** and **b** values to match the data.\n",
    "\n",
    "## Step 4: Make a Prediction\n",
    "Let‚Äôs predict the price for a house with 3300 sq. ft:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45509f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = reg.predict([[3300]])\n",
    "print(\"Predicted price:\", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d04d7c1",
   "metadata": {},
   "source": [
    "This gives us the estimated price based on the trained model.\n",
    "\n",
    "## Step 5: Check the Coefficients\n",
    "The model calculates:\n",
    "- **Slope (m)**: `reg.coef_` (e.g., 135.79)\n",
    "- **Intercept (b)**: `reg.intercept_` (e.g., 180616.44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda89cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Slope (m):\", reg.coef_)\n",
    "print(\"Intercept (b):\", reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca2b228",
   "metadata": {},
   "source": [
    "## Step 6: Manual Calculation\n",
    "Using the formula `y = m * x + b`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be0e808",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = reg.coef_[0]\n",
    "b = reg.intercept_\n",
    "x = 3300\n",
    "y_manual = m * x + b\n",
    "print(\"Manual prediction:\", y_manual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea82bf52",
   "metadata": {},
   "source": [
    "This should match the result from `reg.predict()`.\n",
    "\n",
    "---\n",
    "\n",
    "## Visualize the Fit\n",
    "We can plot the data and the line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1648072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel('Area (sq. ft)')\n",
    "plt.ylabel('Price ($)')\n",
    "plt.scatter(df['area'], df['price'], color='red', marker='*')\n",
    "plt.plot(df['area'], reg.predict(df[['area']]), color='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba892c11",
   "metadata": {},
   "source": [
    "- **Red stars**: Actual data points.\n",
    "- **Blue line**: Predicted values (the best-fitting line).\n",
    "\n",
    "This graph shows how well the line fits the data.\n",
    "\n",
    "---\n",
    "\n",
    "# Example: Multiple Linear Regression\n",
    "\n",
    "## Problem Statement\n",
    "Imagine you're planning to buy a new house and need to predict the price based on multiple factors:\n",
    "- **Area (square feet)**\n",
    "- **Number of bedrooms**\n",
    "- **Age of the house (in years)**\n",
    "\n",
    "Given the home price dataset, we aim to predict the price of homes with:\n",
    "- **3000 sq. ft area, 3 bedrooms, 40 years old**\n",
    "- **2500 sq. ft area, 4 bedrooms, 5 years old**\n",
    "\n",
    "## Dataset Overview\n",
    "We will use **homeprices1.csv** as our dataset, which contains the following columns:\n",
    "- **Area** (Square Feet)\n",
    "- **Bedrooms** (Number of Bedrooms)\n",
    "- **Age** (Years)\n",
    "- **Price** (Target Variable)\n",
    "\n",
    "## Implementing Multiple Linear Regression in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d243d40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"homeprices1.csv\")\n",
    "\n",
    "# Handle missing values by filling with median of 'bedrooms'\n",
    "df['bedrooms'].fillna(df['bedrooms'].median(), inplace=True)\n",
    "\n",
    "# Separate features and target variable\n",
    "X = df[['area', 'bedrooms', 'age']]\n",
    "y = df['price']\n",
    "\n",
    "# Train the model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X, y)\n",
    "\n",
    "# Display model parameters\n",
    "print(\"Intercept:\", reg.intercept_)\n",
    "print(\"Coefficients:\", reg.coef_)\n",
    "\n",
    "# Predict prices for new homes\n",
    "new_homes = pd.DataFrame({\n",
    "    'area': [3000, 2500],\n",
    "    'bedrooms': [3, 4],\n",
    "    'age': [40, 5]\n",
    "})\n",
    "\n",
    "predicted_prices = reg.predict(X)\n",
    "print(\"Predicted Prices:\", predicted_prices)\n",
    "\n",
    "print(f\"R¬≤: {r2_score(y, predicted_prices)}, RMSE: {np.sqrt(mean_squared_error(y, predicted_prices))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cc4ce8",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "To predict house prices for new homes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7222cc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_price = reg.predict([[3000, 3, 40]])\n",
    "print(f\"Predicted price for 3000 sq.ft, 3 bedrooms, 40 years old: ${predicted_price[0]:,.2f}\")\n",
    "\n",
    "predicted_price = reg.predict([[2500, 4, 5]])\n",
    "print(f\"Predicted price for 2500 sq.ft, 4 bedrooms, 5 years old: ${predicted_price[0]:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210f6554",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **Example: Predicting California House Prices**\n",
    "\n",
    "We‚Äôll use the **California housing dataset** to predict median house values based on features like median income, house age, and average rooms.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Data Collection** üóÇÔ∏è\n",
    "\n",
    "**Objective**: Gather relevant data for the machine learning task.  \n",
    "**Description**: For this example, we‚Äôll use the California housing dataset, which is available through Scikit-Learn. This dataset includes features such as median income (`MedInc`), house age (`HouseAge`), average number of rooms (`AveRooms`), and others, with the target variable being the median house value (`MedHouseVal`).  \n",
    "**Code**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469648df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = fetch_california_housing()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df['MedHouseVal'] = data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dea0d0",
   "metadata": {},
   "source": [
    "**Explanation**: The `fetch_california_housing()` function retrieves the dataset, and we convert it into a Pandas DataFrame for easier manipulation. The features are stored in `data.data`, and the target variable is in `data.target`. This step simulates collecting data from a reliable source.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Data Preparation** üîç\n",
    "\n",
    "**Objective**: Clean and preprocess the data to make it suitable for modeling.  \n",
    "**Actions**:\n",
    "- Check for and handle missing values.\n",
    "- Select relevant features for the model.\n",
    "- Scale the features to standardize them for better model performance.  \n",
    "\n",
    "**Code**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe5ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(df.isnull().sum())  # No missing values in this dataset\n",
    "\n",
    "# Select features and target\n",
    "X = df[['MedInc', 'HouseAge', 'AveRooms']]\n",
    "y = df['MedHouseVal']\n",
    "\n",
    "# Scale the features\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914cba67",
   "metadata": {},
   "source": [
    "**Explanation**: \n",
    "- **Missing Values**: The California housing dataset is clean and has no missing values, as confirmed by `isnull().sum()`.\n",
    "- **Feature Selection**: We choose three features‚Äî`MedInc` (median income), `HouseAge` (median house age), and `AveRooms` (average rooms)‚Äîto predict the target `MedHouseVal`.\n",
    "- **Scaling**: `StandardScaler` standardizes the features by removing the mean and scaling to unit variance, which helps the model converge faster and perform better.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Data Wrangling** üõ†Ô∏è\n",
    "\n",
    "**Objective**: Structure and split the data for training and testing.  \n",
    "**Action**: Divide the dataset into training and testing sets to evaluate the model on unseen data.  \n",
    "**Code**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d17cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6191732",
   "metadata": {},
   "source": [
    "**Explanation**: We use `train_test_split` to allocate 80% of the data for training (`X_train`, `y_train`) and 20% for testing (`X_test`, `y_test`). The `random_state=42` ensures reproducibility. This step prepares the data for model training and evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Train the Model** üéØ\n",
    "\n",
    "**Objective**: Build a predictive model using the training data.  \n",
    "**Action**: Train a multiple linear regression model on the selected features.  \n",
    "**Code**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a121c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Initialize and train the model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e06f2fb",
   "metadata": {},
   "source": [
    "**Explanation**: The `LinearRegression` model from Scikit-Learn is used to fit a linear equation to the training data. The `fit` method computes the optimal coefficients for the features (`MedInc`, `HouseAge`, `AveRooms`) to predict `MedHouseVal`.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Test the Model** üß™\n",
    "\n",
    "**Objective**: Evaluate the model‚Äôs performance on the test data.  \n",
    "**Actions**:\n",
    "- Generate predictions for the test set.\n",
    "- Calculate evaluation metrics such as R-squared and Mean Squared Error (MSE).  \n",
    "\n",
    "**Code**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b302f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = reg.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"R-squared:\", r2)\n",
    "print(\"MSE:\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265c6739",
   "metadata": {},
   "source": [
    "**Explanation**: \n",
    "- **Predictions**: The `predict` method uses the trained model to estimate house values for the test set.\n",
    "- **Metrics**: \n",
    "  - **R-squared**: Measures the proportion of variance in the target variable explained by the model (closer to 1 is better).\n",
    "  - **MSE**: Calculates the average squared difference between predicted and actual values (lower is better).  \n",
    "These metrics indicate how well the model generalizes to unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Model Deployment** üöÄ\n",
    "\n",
    "**Objective**: Make the trained model available for real-world use.  \n",
    "**Action**: Save the model to a file for later use or deployment.  \n",
    "**Code**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37d56ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(reg, 'california_housing_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc46c7d",
   "metadata": {},
   "source": [
    "**Explanation**: The `joblib.dump` function serializes the trained model to a file named `california_housing_model.pkl`. This file can be loaded later to make predictions without retraining. In a real-world scenario, you might deploy this model via a web service (e.g., using Flask or FastAPI), but saving it is a key first step.\n",
    "\n",
    "---\n",
    "\n",
    "# ML: Polynomial Features\n",
    "\n",
    "Polynomial features extend linear regression by adding terms that are powers or interactions of the original features, allowing the model to fit non-linear patterns. While basic linear regression assumes a straight-line relationship (\\(y = w_0 + w_1x\\)), polynomial regression introduces higher-degree terms (e.g., \\(x^2, x^3\\)) and interactions (e.g., \\(x_1x_2\\)) to model curves and complex dependencies.\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "Polynomial Features are a type of **feature engineering** where we create new input features based on existing ones by applying polynomial transformations.\n",
    "\n",
    "### Example\n",
    "If we have a dataset with one input feature \\( X \\), we can create a new feature by squaring \\( X \\), i.e., \\( X^2 \\). This process can be extended for higher-degree polynomials:\n",
    "- **Degree 1:** \\( X \\)\n",
    "- **Degree 2:** \\( X, X^2 \\)\n",
    "- **Degree 3:** \\( X, X^2, X^3 \\), etc.\n",
    "\n",
    "The **degree** of the polynomial controls the number of new features added.\n",
    "\n",
    "---\n",
    "\n",
    "## Why Do We Need Polynomial Features?\n",
    "\n",
    "### Case 1: Linear Data\n",
    "- If we apply a **linear model** to **linear data**, it works well (as seen in **Simple Linear Regression**).\n",
    "- However, if the dataset is **non-linear**, using a linear model **without modification** will produce **poor results**.\n",
    "- This leads to **high error rates** and inaccurate predictions.\n",
    "\n",
    "### Case 2: Non-Linear Data\n",
    "- If we use a linear model on **non-linear data**, the predictions will be incorrect.\n",
    "- This leads to **high error rates** and poor model performance.\n",
    "- **Solution:** Use **Polynomial Regression**, which extends linear regression by adding polynomial terms.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Use Polynomial Features?**\n",
    "- **Non-Linearity**: Real-world data often exhibits non-linear relationships (e.g., quadratic growth).\n",
    "- **Flexibility**: Adds expressive power to linear models without changing the algorithm.\n",
    "- **Feature Engineering**: Enhances model performance by capturing more patterns.\n",
    "\n",
    "## **Mathematical Representation**\n",
    "\n",
    "Simple Linear Regression Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5246b2",
   "metadata": {
    "attributes": {
     "classes": [
      "math"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "y = b_0 + b_1x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a5895b",
   "metadata": {},
   "source": [
    "For a single feature \\(x\\), polynomial regression of degree \\(n\\) might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6b90ce",
   "metadata": {
    "attributes": {
     "classes": [
      "math"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "y = b_0 + b_1x + b_2x^2 + b_3x^3 + ... + b_nx^n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b49371",
   "metadata": {},
   "source": [
    "For multiple features (e.g., \\(x_1, x_2\\)), it includes interaction terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8a0f13",
   "metadata": {
    "attributes": {
     "classes": [
      "math"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "y = b_0 + b_1x_1 + b_2x_2 + b_3x_1^2 + b_4x_1x_2 + b_5x_2^2 + \\cdots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94678eb8",
   "metadata": {},
   "source": [
    "## Step : Simple Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500770dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"poly_dataset.csv\")\n",
    "\n",
    "# Extracting independent and dependent variables\n",
    "X = df.iloc[:, 1:2].values  # Selecting feature column\n",
    "y = df.iloc[:, 2].values    # Target variable\n",
    "\n",
    "# Scatter plot to visualize data distribution\n",
    "plt.scatter(X, y, color=\"blue\")\n",
    "plt.xlabel(\"Feature X\")\n",
    "plt.ylabel(\"Target y\")\n",
    "plt.title(\"Data Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Train a simple linear regression model\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "\n",
    "# Plot the Linear Regression model\n",
    "plt.scatter(X, y, color=\"blue\")\n",
    "plt.plot(X, lin_reg.predict(X), color=\"red\")\n",
    "plt.title(\"Linear Regression Fit\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af906ab",
   "metadata": {},
   "source": [
    "## **Key Concepts and Methods**\n",
    "\n",
    "### **a. Generating Polynomial Features**\n",
    "- **Concept**: Transform original features into a higher-dimensional space with polynomial terms.\n",
    "- **Parameters**:\n",
    "  - `degree`: Maximum power of features.\n",
    "  - `interaction_only`: Include only interaction terms (e.g., \\(x_1x_2\\)).\n",
    "  - `include_bias`: Add a constant term (intercept)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23077718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial features (degree 2)\n",
    "poly = PolynomialFeatures(degree=2, include_bias=True)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "print(\"Original X:\\n\", X)\n",
    "print(\"Polynomial X (degree 2):\\n\", X_poly)\n",
    "print(\"Feature Names:\", poly.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1610c5ca",
   "metadata": {},
   "source": [
    "### **b. Polynomial Regression**\n",
    "- **Concept**: Fit a linear model to the polynomial features.\n",
    "- **Steps**: Transform data, then apply linear regression.\n",
    "- **`y.ravel()` vs. `y`**  \n",
    "  If `y` is already a 1D array, `.ravel()` has no effect. If `y` is a column vector (e.g., from a Pandas DataFrame), `.ravel()` reshapes it to avoid warnings/errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36242ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Target: Quadratic relationship y = 2x^2 + x + 1\n",
    "X = np.array([[0], [1], [2], [3]])\n",
    "y = 2 * X**2 + X + 1\n",
    "\n",
    "# Transform\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# Fit model\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly, y.ravel())\n",
    "print(\"Coefficients:\", model.coef_)  # Output: [0, 1, 2] (bias adjusted in intercept)\n",
    "print(\"Intercept:\", model.intercept_)  # Output: ~1\n",
    "\n",
    "plt.scatter(X, y, color=\"blue\")\n",
    "plt.plot(X, poly_model.predict(X_poly), color=\"red\")\n",
    "plt.xlabel(\"Input Feature\")\n",
    "plt.ylabel(\"Target Variable\")\n",
    "plt.title(\"Polynomial Regression Fit\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f72421",
   "metadata": {},
   "source": [
    "### **c. Interaction Terms**\n",
    "- **Concept**: Capture relationships between features (e.g., \\(x_1x_2\\))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b508fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two features\n",
    "X = np.array([[1, 2], [2, 3], [3, 4]])\n",
    "\n",
    "# Polynomial features with interactions\n",
    "poly_inter = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "X_poly_inter = poly_inter.fit_transform(X)\n",
    "print(\"Polynomial with Interactions:\\n\", X_poly_inter)\n",
    "print(\"Feature Names:\", poly_inter.get_feature_names_out())\n",
    "# Output:\n",
    "# [[ 1.  2.  1.  2.  4.]\n",
    "#  [ 2.  3.  4.  6.  9.]\n",
    "#  [ 3.  4.  9. 12. 16.]]\n",
    "# Feature Names: ['x0' 'x1' 'x0^2' 'x0 x1' 'x1^2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb0f731",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a07e87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict output using Linear Regression\n",
    "linear_pred = lin_reg.predict([[330]])\n",
    "print(\"Linear Regression Predicted Output:\", linear_pred)\n",
    "\n",
    "# Predict output using Polynomial Regression\n",
    "poly_pred = model.predict(poly_reg.fit_transform([[330]]))\n",
    "print(\"Polynomial Regression Predicted Output:\", poly_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e816c4",
   "metadata": {},
   "source": [
    "- **Linear Regression predicted output:** `[330378.78787879]`\n",
    "- **Polynomial Regression predicted output:** `[158862.45265155]`\n",
    "- **Polynomial Regression provides a more accurate prediction** in cases where the dataset exhibits **non-linearity**.\n",
    "- Choosing the right polynomial degree is crucial to avoid **overfitting** or **underfitting**.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Tools and Methods Summary**\n",
    "- **Feature Generation**: `sklearn.preprocessing.PolynomialFeatures`.\n",
    "- **Modeling**: `sklearn.linear_model.LinearRegression`.\n",
    "- **Visualization**: `matplotlib.pyplot.plot()`, `scatter()`.\n",
    "- **Evaluation**: `sklearn.metrics.r2_score` (below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fce4980",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y, y_pred)\n",
    "print(\"R¬≤ Score:\", r2)  # Measures fit quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b21c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Ridge regression with polynomial features\n",
    "model_ridge = Ridge(alpha=1.0)\n",
    "model_ridge.fit(X_poly, y)\n",
    "y_pred_ridge = model_ridge.predict(X_poly)\n",
    "print(\"Ridge Coefficients:\", model_ridge.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71e0627",
   "metadata": {},
   "source": [
    "---\n",
    "## **5. Key Considerations**\n",
    "- **Degree Selection**: Higher degrees increase flexibility but risk overfitting.\n",
    "- **Feature Scaling**: Polynomial terms amplify scale differences normalize first.\n",
    "- **Computational Cost**: Number of features grows as \\( \\binom{n+d}{d} \\) (where \\(n\\) = features, \\(d\\) = degree).\n",
    "- **Regularization**: Use Ridge/Lasso with high-degree polynomials to control overfitting.\n",
    "- **Optimal degree selection** is crucial for balancing bias and variance.\n",
    "\n",
    "üìå **Key Takeaway:** Polynomial Regression is a **powerful extension of Linear Regression** that allows models to fit **non-linear data patterns** more effectively.\n",
    "\n",
    "---\n",
    "\n",
    "## Feature Interactions with Polynomial Features\n",
    "Use `PolynomialFeatures` to include feature interactions in your model. However, this is impractical for large datasets and unnecessary for tree-based models.\n",
    "\n",
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb230ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 4, 4], 'C': [0, 10, 100]})\n",
    "\n",
    "# Generate interaction-only features (exclude polynomial terms)\n",
    "poly = PolynomialFeatures(include_bias=False, interaction_only=True)\n",
    "poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6670359",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **4. Tools and Methods Summary**\n",
    "- **Data**: `pandas.DataFrame()`, `sklearn.preprocessing.StandardScaler`.\n",
    "- **Models**: `sklearn.linear_model.LogisticRegression`, `tensorflow.keras.models`.\n",
    "- **Loss**: `np.mean()`, `sklearn.metrics.log_loss`.\n",
    "- **Optimization**: Manual gradient descent, `tensorflow.keras.optimizers`.\n",
    "- **Evaluation**: `sklearn.metrics.accuracy_score`, `r2_score`.\n",
    "- **Regularization**: `penalty` in `sklearn` models.\n",
    "- **Stats**: `scipy.stats`, `numpy.random`.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}