{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "468b5033",
   "metadata": {},
   "source": [
    "# 4ï¸âƒ£ Model Evaluation\n",
    "\n",
    "**Model Evaluation Metrics**\n",
    "\n",
    "---\n",
    "# **Regression Metrics**\n",
    "- **Mean Squared Error (MSE)**: Average of squared errors.\n",
    "- **Root Mean Squared Error (RMSE)**: \\(\\sqrt{MSE}\\).\n",
    "- **Mean Absolute Error (MAE)**: Average of absolute errors.\n",
    "- **RÂ² (R-Squared)**: Proportion of variance explained by the model (0 to 1).\n",
    "\n",
    "Common Cost Functions for Regression\n",
    "In regression tasks, models predict continuous numerical values (e.g., house prices, temperatures). **Cost functions** quantify prediction accuracy by measuring the \"distance\" between predicted and actual values.  \n",
    "\n",
    "âœ… **Purpose of Cost Functions:**  \n",
    "- Guide model training by providing feedback on prediction errors.  \n",
    "- Enable optimization algorithms (e.g., Gradient Descent) to adjust model weights.  \n",
    "\n",
    "## Distance-Based Error Explained  \n",
    "For each data point:  \n",
    "- **Actual Value**: $y$  \n",
    "- **Predicted Value**: $y'$  \n",
    "- **Error**: $y - y'$  \n",
    "\n",
    "Cost functions aggregate these errors across the dataset to measure overall model performance. \n",
    "\n",
    "## 1. Mean Squared Error (MSE)\n",
    "- MSE is the most widely used cost function in regression. \n",
    "- MSE is calculated as the **average of the squared differences** between predicted (y`) and actual (y) target values.\n",
    "- Squaring the errors amplifies larger discrepancies, making MSE sensitive to outliers.\n",
    "\n",
    "**Formula**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "578bf24b",
   "metadata": {
    "attributes": {
     "classes": [
      "math"
     ],
     "id": ""
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (1815677819.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    MSE = (1/n) * Î£ (yi - yi')Â²\u001b[0m\n\u001b[1;37m                               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "MSE = (1/n) * Î£ (yi - yi')Â²"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a73e0022",
   "metadata": {},
   "source": [
    "Where:  \n",
    "- **n** = Number of data points  \n",
    "- **yi** = Actual target value  \n",
    "- **yi'** = Predicted value  \n",
    "\n",
    "**When to Use**:  \n",
    "- When large errors should be penalized more heavily.\n",
    "- Ideal for regression tasks where errors are assumed to be normally distributed.\n",
    "\n",
    "**Example**:  \n",
    "Suppose weâ€™re predicting house prices. If a model predicts $300,000 for a house that actually costs $350,000, the squared error is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97963eb0",
   "metadata": {
    "attributes": {
     "classes": [
      "math"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "(350,000 - 300,000)Â² = 2,500,000,000\n",
    "``` \n",
    "Averaging such errors across all predictions yields the MSE.\n",
    "\n",
    "### Python Implementation  \n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Sample data: Actual vs. Predicted values\n",
    "expected = [1.0] * 11\n",
    "predicted = [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0]\n",
    "\n",
    "mse = mean_squared_error(expected, predicted)\n",
    "print(f\"MSE: {mse:.2f}\")  # Output: MSE: 0.35"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525b89f7",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Mean Absolute Error (MAE) \n",
    "- MAE is calculated as the **average absolute difference** between predicted and actual values.\n",
    "- Unlike MSE, it doesnâ€™t square errors, making it less sensitive to outliers.\n",
    "\n",
    "**Formula**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d676c34",
   "metadata": {
    "attributes": {
     "classes": [
      "math"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "MAE = (1/n) * Î£ |yi - yi'|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3881db6f",
   "metadata": {},
   "source": [
    "**When to Use**:  \n",
    "- When robustness to outliers is desired.\n",
    "- Suitable when all errors should be weighted equally, regardless of magnitude.\n",
    "\n",
    "**Example**:  \n",
    "Using the same house price scenario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb85c87f",
   "metadata": {
    "attributes": {
     "classes": [
      "math"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "|350,000 - 300,000| = 50,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1ce7f4",
   "metadata": {},
   "source": [
    "MAE averages these absolute differences, providing a simple measure of average error magnitude.\n",
    "\n",
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8151e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "mae = mean_absolute_error(expected, predicted)\n",
    "print(f\"MAE: {mae:.2f}\")  # Output: MAE: 0.50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e565849",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Root Mean Squared Error (RMSE) \n",
    "- RMSE is the square root of MSE, returning the error to the same units as the target variable. \n",
    "- It retains MSEâ€™s sensitivity to larger errors but is more interpretable.\n",
    "\n",
    "**Formula**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067b39bd",
   "metadata": {
    "attributes": {
     "classes": [
      "math"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "RMSE = sqrt(MSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ce7a2f",
   "metadata": {},
   "source": [
    "**When to Use**:  \n",
    "- When you need an error metric in the same units as the target variable.\n",
    "- Often used to report model performance in an intuitive way.\n",
    "\n",
    "**Example**:  \n",
    "If MSE is 2,500,000,000 (from the earlier example), then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58886ade",
   "metadata": {
    "attributes": {
     "classes": [
      "math"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "RMSE = sqrt(2,500,000,000) = 50,000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ce1087",
   "metadata": {},
   "source": [
    "This indicates an average error of $50,000 in house price predictions.\n",
    "\n",
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c2f798",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mean_squared_error(expected, predicted, squared=False)\n",
    "print(f\"RMSE: {rmse:.2f}\")  # Output: RMSE: 0.59"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92922e0",
   "metadata": {},
   "source": [
    "---\n",
    "## Tools and Methods\n",
    "\n",
    "Several Python libraries simplify the implementation of these cost functions:\n",
    "- **Scikit-learn**: Provides `mean_squared_error`, `mean_absolute_error`, and supports custom extensions.\n",
    "- **TensorFlow**: Offers `tf.keras.losses` with `MeanSquaredError`, `MeanAbsoluteError`, `Huber`, etc.\n",
    "- **PyTorch**: Includes `torch.nn` modules like `MSELoss`, `L1Loss` (MAE), and `SmoothL1Loss` (Huber-like).\n",
    "\n",
    "---\n",
    "\n",
    "## One-Stop Solution: Python Code Example\n",
    "\n",
    "This script uses the **California Housing dataset** to train a linear regression model and compute MSE, MAE, RMSE, Huber Loss, and Log-Cosh Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d743e931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the California Housing dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# 1. Mean Squared Error (MSE)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "\n",
    "# 2. Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "\n",
    "# 3. Root Mean Squared Error (RMSE)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "\n",
    "# 4. Huber Loss (using TensorFlow)\n",
    "huber = tf.keras.losses.Huber(delta=1.0)\n",
    "huber_loss = huber(y_test, y_pred).numpy()\n",
    "print(f\"Huber Loss (delta=1.0): {huber_loss:.4f}\")\n",
    "\n",
    "# 5. Log-Cosh Loss (manual implementation)\n",
    "log_cosh_loss = np.mean(np.log(np.cosh(y_test - y_pred)))\n",
    "print(f\"Log-Cosh Loss: {log_cosh_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b949a7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways  \n",
    "1ï¸âƒ£ **MSE**: Use when large errors must be flagged (e.g., fraud detection).  \n",
    "2ï¸âƒ£ **RMSE**: Default choice for model comparison and reporting.  \n",
    "3ï¸âƒ£ **MAE**: Best for noisy datasets or when outliers should not dominate error calculations.  \n",
    "\n",
    "ðŸ” **Pro Tip**: Always visualize residuals (prediction errors) to understand error distribution and metric suitability!\n",
    "\n",
    "---\n",
    "\n",
    "# **Classification Metrics**\n",
    "- **Accuracy**: (Correct Predictions) / (Total Predictions).\n",
    "- **Precision**: (True Positives) / (True Positives + False Positives).\n",
    "- **Recall**: (True Positives) / (True Positives + False Negatives).\n",
    "- **F1-Score**: Harmonic mean of precision and recall.\n",
    "- **ROC-AUC**: Area under the ROC curve (measures class separation).\n",
    "- **Confusion Matrix**: Table showing true vs. predicted classes.\n",
    "\n",
    "---\n",
    "\n",
    "Common Cost Functions for Classification\n",
    "\n",
    "## 1. Binary Cross-Entropy (Log Loss)\n",
    "- Binary cross-entropy measures the difference between predicted probabilities and true binary labels (0 or 1). \n",
    "- Itâ€™s widely used in binary classification tasks and penalizes predictions more heavily as they deviate from the true label.\n",
    "\n",
    "**Formula:**  \n",
    "For a single sample:  \n",
    "BCE = - [y * log(Å·) + (1 - y) * log(1 - Å·)]  \n",
    "\n",
    "For a dataset with *n* samples:  \n",
    "BCE = - (1/n) * Î£ [yáµ¢ * log(Å·áµ¢) + (1 - yáµ¢) * log(1 - Å·áµ¢)], for i = 1 to n  \n",
    "\n",
    "Where:  \n",
    "- *yáµ¢* = True label (0 or 1)  \n",
    "- *Å·áµ¢* = Predicted probability for the positive class (between 0 and 1)  \n",
    "\n",
    "**When to Use:**  \n",
    "- Binary classification problems (e.g., spam vs. not spam).  \n",
    "- Models that output probabilities (e.g., logistic regression).  \n",
    "\n",
    "**Example:**  \n",
    "- If *y = 1* and *Å· = 0.9*:  \n",
    "  BCE = - [1 * log(0.9) + (1 - 1) * log(1 - 0.9)]  \n",
    "       = - log(0.9) â‰ˆ 0.105  \n",
    "\n",
    "- If *y = 1* and *Å· = 0.1*:  \n",
    "  BCE = - log(0.1) â‰ˆ 2.303  \n",
    "\n",
    "The loss increases significantly for poor predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Categorical Cross-Entropy \n",
    "- Categorical cross-entropy extends binary cross-entropy to multi-class classification. \n",
    "- It compares the true label distribution (typically one-hot encoded) with the predicted probability distribution across all classes.\n",
    "\n",
    "**Formula:**  \n",
    "\n",
    "For a single sample with C classes:  \n",
    "`CCE = - Î£ (y_c * log(yÌ‚_c))`  \n",
    "\n",
    "For a dataset:  \n",
    "`CCE = - (1/n) * Î£ Î£ (y_i,c * log(yÌ‚_i,c))`  \n",
    "\n",
    "Where:  \n",
    "- y_i,c = 1 if the sample belongs to class c, otherwise 0.  \n",
    "- yÌ‚_i,c = Predicted probability for class c.  \n",
    "\n",
    "---\n",
    "\n",
    "### **When to Use**  \n",
    "- Multi-class classification (e.g., classifying images into 10 digit categories).  \n",
    "- Models that output probability distributions (e.g., softmax output).  \n",
    "\n",
    "### **Example**  \n",
    "For a 3-class problem:  \n",
    "- True label = class 2 (one-hot: [0, 1, 0])  \n",
    "- Predicted probabilities = [0.1, 0.7, 0.2]  \n",
    "\n",
    "Calculation:  \n",
    "`CCE = - (0 * log(0.1) + 1 * log(0.7) + 0 * log(0.2))`  \n",
    "`CCE = - log(0.7) â‰ˆ 0.357`  \n",
    "\n",
    "---\n",
    "\n",
    "## 3. Sparse Categorical Cross-Entropy\n",
    "- Similar to categorical cross-entropy, but designed for integer labels instead of one-hot encoded vectors. \n",
    "- Itâ€™s more memory-efficient, especially with many classes.\n",
    "\n",
    "**Formula**:  \n",
    "For a single sample:  \n",
    "Sparse CCE = - log(Å·_y)  \n",
    "\n",
    "Where:  \n",
    "- y = True class index  \n",
    "- Å·_y = Predicted probability for the true class  \n",
    "\n",
    "**When to Use**  \n",
    "- Multi-class classification with integer labels  \n",
    "- Large number of classes where one-hot encoding is impractical  \n",
    "\n",
    "**Example**  \n",
    "True label = class 2 (index 2)  \n",
    "Predicted probabilities = [0.1, 0.7, 0.2]  \n",
    "\n",
    "Calculation:  \n",
    "`Sparse CCE = - log(0.7) â‰ˆ 0.357` \n",
    "\n",
    "---\n",
    "\n",
    "## 4. Hinge Loss\n",
    "- Hinge loss is used in Support Vector Machines (SVMs) for binary classification. \n",
    "- It encourages correct classification with a margin, penalizing predictions that are correct but too close to the decision boundary.\n",
    "\n",
    "For a single sample:  \n",
    "Hinge = max(0, 1 - y * Å·)  \n",
    "\n",
    "Where:  \n",
    "- y = True label (-1 or 1)  \n",
    "- Å· = Predicted score (not a probability)  \n",
    "\n",
    "**When to Use**  \n",
    "- SVM-based classification  \n",
    "- Models outputting decision scores rather than probabilities  \n",
    "\n",
    "**Example**  \n",
    "1. If y = 1, Å· = 0.8:  \n",
    "   Hinge = max(0, 1 - (1 * 0.8)) = max(0, 0.2) = **0.2**  \n",
    "\n",
    "2. If y = 1, Å· = -0.5:  \n",
    "   Hinge = max(0, 1 - (1 * -0.5)) = max(0, 1.5) = **1.5**  \n",
    "\n",
    "---\n",
    "\n",
    "## Tools and Methods\n",
    "\n",
    "Python libraries provide built-in implementations of these cost functions:\n",
    "- **Scikit-learn**: `log_loss` (cross-entropy), `hinge_loss`.\n",
    "- **TensorFlow**: `tf.keras.losses` (e.g., `BinaryCrossentropy`, `CategoricalCrossentropy`, `SparseCategoricalCrossentropy`, `Hinge`).\n",
    "- **PyTorch**: `torch.nn` (e.g., `CrossEntropyLoss`, `BCELoss`).\n",
    "- **NumPy**: For manual implementations or custom loss functions.\n",
    "\n",
    "---\n",
    "\n",
    "## One-Stop Solution: Python Code Example\n",
    "\n",
    "This script uses the **Iris dataset** for multi-class classification and a synthetic dataset for binary classification, computing the cost functions discussed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c13331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.datasets import load_iris, make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# --- Binary Classification ---\n",
    "# Generate synthetic binary data\n",
    "X_bin, y_bin = make_classification(n_samples=100, n_features=2, n_classes=2, random_state=42)\n",
    "X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(X_bin, y_bin, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train logistic regression\n",
    "model_bin = LogisticRegression()\n",
    "model_bin.fit(X_train_bin, y_train_bin)\n",
    "y_pred_prob_bin = model_bin.predict_proba(X_test_bin)[:, 1]  # Probability of class 1\n",
    "\n",
    "# 1. Binary Cross-Entropy\n",
    "bce = log_loss(y_test_bin, y_pred_prob_bin)\n",
    "print(f\"Binary Cross-Entropy (Log Loss): {bce:.4f}\")\n",
    "\n",
    "# 2. Hinge Loss (using SVM)\n",
    "model_svm = SVC(kernel='linear')\n",
    "model_svm.fit(X_train_bin, y_train_bin)\n",
    "y_pred_decision = model_svm.decision_function(X_test_bin)  # Decision scores\n",
    "y_test_bin_svm = 2 * y_test_bin - 1  # Convert 0/1 to -1/1\n",
    "hinge_loss = np.mean(np.maximum(0, 1 - y_test_bin_svm * y_pred_decision))\n",
    "print(f\"Hinge Loss: {hinge_loss:.4f}\")\n",
    "\n",
    "# --- Multi-Class Classification ---\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X_iris, y_iris = iris.data, iris.target\n",
    "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train logistic regression\n",
    "model_multi = LogisticRegression(multi_class='ovr', max_iter=200)\n",
    "model_multi.fit(X_train_iris, y_train_iris)\n",
    "y_pred_prob_multi = model_multi.predict_proba(X_test_iris)\n",
    "\n",
    "# 3. Categorical Cross-Entropy\n",
    "cce = log_loss(y_test_iris, y_pred_prob_multi)\n",
    "print(f\"Categorical Cross-Entropy: {cce:.4f}\")\n",
    "\n",
    "# 4. Sparse Categorical Cross-Entropy (TensorFlow)\n",
    "sparse_cce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "sparse_cce_loss = sparse_cce(y_test_iris, y_pred_prob_multi).numpy()\n",
    "print(f\"Sparse Categorical Cross-Entropy: {sparse_cce_loss:.4f}\")\n",
    "\n",
    "# 5. Focal Loss (manual implementation for binary)\n",
    "def focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):\n",
    "    pt = y_pred * y_true + (1 - y_pred) * (1 - y_true)\n",
    "    return -alpha * (1 - pt) ** gamma * np.log(pt + 1e-10)  # Add small constant to avoid log(0)\n",
    "\n",
    "focal_loss_value = np.mean([focal_loss(y, p) for y, p in zip(y_test_bin, y_pred_prob_bin)])\n",
    "print(f\"Focal Loss (gamma=2, alpha=0.25): {focal_loss_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67ed22c",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "- **Binary Classification**:\n",
    "  - Uses `make_classification` to create synthetic data.\n",
    "  - Computes **Binary Cross-Entropy** with `log_loss`.\n",
    "  - Computes **Hinge Loss** manually using SVM decision scores (labels converted to -1/1).\n",
    "- **Multi-Class Classification**:\n",
    "  - Uses the Iris dataset.\n",
    "  - Computes **Categorical Cross-Entropy** with `log_loss`.\n",
    "  - Computes **Sparse Categorical Cross-Entropy** with TensorFlow.\n",
    "- **Focal Loss**:\n",
    "  - Manually implemented for the binary case, adding a small constant to prevent log(0) errors.\n",
    "\n",
    "### Sample Output (Approximate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eba72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Binary Cross-Entropy (Log Loss): 0.2456\n",
    "Hinge Loss: 0.1234\n",
    "Categorical Cross-Entropy: 0.1234\n",
    "Sparse Categorical Cross-Entropy: 0.1234\n",
    "Focal Loss (gamma=2, alpha=0.25): 0.0567"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ba51b8",
   "metadata": {},
   "source": [
    "*(Actual values vary based on data splits and model performance.)*\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Cost functions are critical for training classification models:\n",
    "- **Binary Cross-Entropy**: Ideal for binary tasks with probability outputs.\n",
    "- **Categorical Cross-Entropy**: Suited for multi-class problems with one-hot labels.\n",
    "- **Sparse Categorical Cross-Entropy**: Efficient for multi-class integer labels.\n",
    "- **Hinge Loss**: Best for SVMs and margin maximization.\n",
    "- **Focal Loss**: Effective for imbalanced datasets by focusing on hard examples.\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "Cost functions are indispensable in machine learning, serving as the foundation for evaluating and optimizing models. \n",
    "\n",
    "---\n",
    "\n",
    "# **Clustering Metrics**\n",
    "- Silhouette Score, Davies-Bouldin Index.\n",
    "\n",
    "---\n",
    "\n",
    "# Confusion Matrix\n",
    "\n",
    "A **Confusion Matrix** is a powerful tool used in machine learning to evaluate the performance of classification models.\n",
    "\n",
    "---\n",
    "\n",
    "## **What is a Confusion Matrix?**\n",
    "A confusion matrix is a tabular representation of a classification modelâ€™s performance, comparing predicted labels to actual labels. It summarizes the counts of correct and incorrect predictions across all classes, providing a detailed breakdown beyond simple accuracy.\n",
    "\n",
    "## **Why Use a Confusion Matrix?**\n",
    "- **Granular Insight**: Reveals specific errors (e.g., false positives vs. false negatives).\n",
    "- **Class Imbalance**: Highlights performance in imbalanced datasets where accuracy alone is misleading.\n",
    "- **Derived Metrics**: Basis for precision, recall, F1-score, and more.\n",
    "- **Decision Making**: Helps assess model suitability for specific tasks (e.g., minimizing false negatives in medical diagnosis).\n",
    "\n",
    "## **Structure**\n",
    "For a binary classification problem (positive vs. negative):\n",
    "- **True Positive (TP)**: The model correctly predicts the positive class (e.g., identifying a sick patient as sick).\n",
    "- **True Negative (TN)**: The model correctly predicts the negative class (e.g., identifying a healthy patient as healthy).\n",
    "- **False Positive (FP)**: The model incorrectly predicts the positive class (a Type I error). (e.g., saying a healthy patient is sick)\n",
    "- **False Negative (FN)**: The model incorrectly predicts the negative class (a Type II error). (e.g., saying a sick patient is healthy)\n",
    "\n",
    "\n",
    "|                | Predicted Positive | Predicted Negative |\n",
    "|----------------|--------------------|--------------------|\n",
    "| **Actual Positive** | TP                | FN                |\n",
    "| **Actual Negative** | FP                | TN                |\n",
    "\n",
    "For multiclass, it extends to a \\(k \\times k\\) matrix where \\(k\\) is the number of classes.\n",
    "\n",
    "---\n",
    "## Example of a Confusion Matrix\n",
    "\n",
    "Letâ€™s consider a binary classification problem: predicting whether an email is **spam** (positive class) or **not spam** (negative class). Suppose a model makes predictions on 175 emails, with the following results:\n",
    "\n",
    "- **TP**: 50 spam emails correctly classified as spam.\n",
    "- **TN**: 100 non-spam emails correctly classified as non-spam.\n",
    "- **FP**: 15 non-spam emails incorrectly classified as spam.\n",
    "- **FN**: 10 spam emails incorrectly classified as non-spam.\n",
    "\n",
    "The confusion matrix would look like this:\n",
    "\n",
    "|           | Predicted Spam | Predicted Not Spam |\n",
    "|-----------|----------------|--------------------|\n",
    "| **Actual Spam**    | 50 (TP)        | 10 (FN)            |\n",
    "| **Actual Not Spam**| 15 (FP)        | 100 (TN)           |\n",
    "\n",
    "This table shows the model correctly classified 150 emails (TP + TN = 50 + 100) and misclassified 25 (FP + FN = 15 + 10).\n",
    "\n",
    "---\n",
    "\n",
    "## Metrics Derived from the Confusion Matrix\n",
    "\n",
    "The confusion matrix enables the calculation of several key performance metrics:\n",
    "\n",
    "### **Components**\n",
    "- **Diagonal**: Correct predictions (TP for each class in multiclass).\n",
    "- **Off-Diagonal**: Errors (FP and FN for binary; misclassifications for multiclass).\n",
    "\n",
    "### **1. Accuracy**\n",
    "- **What It Means**: The percentage of correct predictions.\n",
    "- **Formula**:\n",
    "  ```python\n",
    "  accuracy = (TP + TN) / (TP + FP + FN + TN)\n",
    "  ```\n",
    "- **Example**: If TP = 50, TN = 105, FP = 10, FN = 15, then:\n",
    "  ```python\n",
    "  accuracy = (50 + 100) / (50 + 100 + 15 + 10)\n",
    "  accuracy = 150/175 # 0.857 --> 85.7%\n",
    "  ```\n",
    "- **When to Use**: Good for balanced data, but misleading if one class dominates.\n",
    "\n",
    "### **2. Precision**\n",
    "- **What It Means**: How many predicted positives are actually correct.\n",
    "- **Formula**:\n",
    "  ```python\n",
    "  recall = TP / (TP + FP)  # \"How many did we catch?\"\n",
    "  ```\n",
    "- Interpretation: The proportion of predicted positives that are actually positive (how precise the positive predictions are).\n",
    "- **Example**: \n",
    "  ```python\n",
    "  recall = 50 / (50 + 15)  # 76.9%\n",
    "  ```\n",
    "- **When to Use**: Important when false positives are costly (e.g., cancer screening).\n",
    "\n",
    "### **3. Recall (Sensitivity)**\n",
    "- **What It Means**: How many actual positives the model finds.\n",
    "- **Formula**:\n",
    "  ```python\n",
    "  recall = TP / (TP + FN)  # \"How many did we catch?\"\n",
    "  ```\n",
    "- Interpretation: The proportion of actual positives correctly identified (how well the model captures positives).\n",
    "- **Example**: \n",
    "  ```python\n",
    "  recall = 50 / (50 + 10)  # 83%\n",
    "  ```\n",
    "- **When to Use**: Critical when missing positives is risky (e.g., detecting diseases).\n",
    "\n",
    "### **4. F1 Score**\n",
    "- **What It Means**: A balance between precision and recall.\n",
    "- **Formula**:\n",
    "  ```python\n",
    "  F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "  ```\n",
    "- Interpretation: The harmonic mean of precision and recall, balancing both when theyâ€™re equally important.\n",
    "- **Example**: \n",
    "  ```python\n",
    "  F1 Score = 2 * (0.769 * 0.833) / (0.769 + 0.833) # 80%\n",
    "  ```\n",
    "- **When to Use**: When you want a single score to compare models.\n",
    "\n",
    "### **5. Specificity**\n",
    "- Formula:  \n",
    "    ```python\n",
    "    Specificity = TN / (TN + FP)\n",
    "    ```\n",
    "- Interpretation: The proportion of actual negatives correctly identified.\n",
    "- Example: \n",
    "  ```python\n",
    "  F1 Score = 100 / (100 + 15) # 87%\n",
    "  ```\n",
    "\n",
    "These metrics provide a comprehensive view of the model's performance, tailored to different priorities (e.g., minimizing false positives vs. maximizing true positives).\n",
    "\n",
    "---\n",
    "\n",
    "## One-Stop Solution: Python Code Example\n",
    "\n",
    "This example uses the **Iris dataset** (a multi-class classification problem with three classes: setosa, versicolor, virginica) to train a model, compute the confusion matrix, visualize it, and calculate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7007414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target  # Features and target labels\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)  # Make predictions on the test set\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names)\n",
    "disp.plot(cmap='Blues')\n",
    "plt.title(\"Confusion Matrix - Iris Dataset\")\n",
    "plt.show()\n",
    "\n",
    "# Print a detailed classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9640760e",
   "metadata": {},
   "source": [
    "### Code Explanation\n",
    "\n",
    "1. **Data Loading and Splitting**:\n",
    "   - The Iris dataset is loaded with 150 samples across 3 classes.\n",
    "   - Itâ€™s split into 80% training and 20% testing data (`test_size=0.2`).\n",
    "\n",
    "2. **Model Training**:\n",
    "   - A `RandomForestClassifier` is trained on the training data.\n",
    "\n",
    "3. **Confusion Matrix Computation and Visualization**:\n",
    "   - `confusion_matrix` computes the matrix comparing `y_test` (actual labels) and `y_pred` (predicted labels).\n",
    "   - `ConfusionMatrixDisplay` creates a heatmap of the matrix, labeled with class names (setosa, versicolor, virginica).\n",
    "\n",
    "4. **Classification Report**:\n",
    "   - `classification_report` outputs precision, recall, F1-score, and support (number of samples) for each class, plus overall averages.\n",
    "\n",
    "### Sample Output\n",
    "\n",
    "#### Confusion Matrix Plot\n",
    "A 3x3 heatmap might look like this (values depend on the random split):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5839b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "           Predicted\n",
    "           | Setosa | Versicolor | Virginica |\n",
    "Actual     |--------|------------|-----------|\n",
    "Setosa     |   10   |     0      |     0     |\n",
    "Versicolor |    0   |     9      |     1     |\n",
    "Virginica  |    0   |     1      |     9     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a822de2",
   "metadata": {},
   "source": [
    "- Diagonal values (10, 9, 9) are TPs for each class.\n",
    "- Off-diagonal values (0s and 1s) are FPs and FNs.\n",
    "\n",
    "#### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3debf151",
   "metadata": {},
   "outputs": [],
   "source": [
    "Classification Report:\n",
    "              precision    recall  f1-score   support\n",
    "      setosa       1.00      1.00      1.00        10\n",
    "  versicolor       0.90      0.90      0.90        10\n",
    "   virginica       0.90      0.90      0.90        10\n",
    "    accuracy                           0.93        30\n",
    "   macro avg       0.93      0.93      0.93        30\n",
    "weighted avg       0.93      0.93      0.93        30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2dfa38",
   "metadata": {},
   "source": [
    "- **Setosa**: Perfectly classified (precision, recall, F1 = 1.00).\n",
    "- **Versicolor/Virginica**: Minor errors (e.g., one misclassification each), yielding 0.90 for precision, recall, and F1.\n",
    "- **Accuracy**: 93% overall.\n",
    "\n",
    "*(Note: Exact numbers may vary due to randomness in the split and model.)*\n",
    "\n",
    "---\n",
    "## **AUC and ROC Curve**\n",
    "\n",
    "The **Area Under the Curve (AUC)** measures how well the model separates classes. It comes from the **ROC Curve**, which plots Recall (True Positive Rate) against False Positive Rate.\n",
    "\n",
    "- **AUC = 1**: Perfect model.\n",
    "- **AUC = 0.5**: No better than guessing.\n",
    "- **AUC < 0.5**: Worse than random.\n",
    "\n",
    "### **Code Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c5bcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Get prediction probabilities\n",
    "y_scores = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot it\n",
    "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve: Titanic Survival')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f40d99",
   "metadata": {},
   "source": [
    "A higher AUC means better performance.\n",
    "\n",
    "---\n",
    "\n",
    "## **Tools and Methods Summary**\n",
    "- `sklearn.metrics.confusion_matrix`: Computes the confusion matrix from true and predicted labels.\n",
    "- `sklearn.metrics.classification_report`: Provides a detailed report with precision, recall, F1-score, and support.\n",
    "- **Visualization**: `seaborn.heatmap()`, `matplotlib.pyplot` for custom plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd85517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom metrics\n",
    "accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "print(f\"Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7a7cb6",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}