{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dedae27a",
   "metadata": {},
   "source": [
    "# Naïve Bayes Classifier\n",
    "\n",
    "The Naïve Bayes Classifier is a popular *probabilistic*  supervised machine learning algorithm used for classification tasks. \n",
    "It is based on **Bayes' Theorem**. \n",
    "\n",
    "## **What is the Naïve Bayes Classifier?**\n",
    "Naïve Bayes is a family of simple, probabilistic classifiers based on applying Bayes’ theorem with a “naïve” assumption of *conditional independence* between features given the class label. \n",
    "It predicts the class of a data point by calculating the posterior probability of each class and selecting the one with the highest probability.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why Use Naïve Bayes?**\n",
    "- **Simplicity**: Easy to implement and computationally efficient.\n",
    "- **Probabilistic**: Provides probability estimates, not just predictions.\n",
    "- **Scalability**: Performs well with high-dimensional data (e.g., text).\n",
    "- **Robustness**: Works surprisingly well even when independence assumption is violated.\n",
    "\n",
    "\n",
    "## **How It Works**\n",
    "1. **Bayes’ Theorem**: `\\( P(y|X) = P(X|y).P(y) / P(X) \\)`.\n",
    "   - \\(P(y|X)\\): Posterior probability of class \\(y\\) given features \\(X\\).\n",
    "   - \\(P(X|y)\\): Likelihood of features given class.\n",
    "   - \\(P(y)\\): Prior probability of class.\n",
    "   - \\(P(X)\\): Evidence (often ignored during classification since as it’s constant across classes). \n",
    "\n",
    "2. **Naïve Assumption**: The \"naïve\" assumption is that all features \\( x_1, x_2, ...... , x_n \\) in \\( X \\) are independent given \\(y\\):\n",
    "   ```math\n",
    "    P(X|y) = P(x_1|y).P(x_2|y) ........ P(x_n|y)\n",
    "   ```\n",
    "- This assumption reduces computational complexity, making Naïve Bayes efficient even for large datasets.\n",
    "\n",
    "3. **Prediction**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f98d48",
   "metadata": {
    "attributes": {
     "classes": [
      "math"
     ],
     "id": ""
    }
   },
   "outputs": [],
   "source": [
    "\\hat{y} = \\arg\\max_y P(y) \\cdot \\prod_{i=1}^n P(x_i|y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1efb32",
   "metadata": {},
   "source": [
    "## **Key Concepts and Variants**\n",
    "\n",
    "### **a. Variants of Naïve Bayes**\n",
    "1. **Gaussian Naïve Bayes**:\n",
    "   - Assumes continuous features follow a Gaussian (normal) distribution.\n",
    "   - Commonly used for datasets with numeric features, like measurements.\n",
    "   - Likelihood: \n",
    "    ```math\n",
    "    P(x_i|C) = \\frac{1}{\\sqrt{2\\pi\\sigma_C^2}} \\exp\\left(-\\frac{(x_i - \\mu_C)^2}{2\\sigma_C^2}\\right)\n",
    "    ```\n",
    "where \\( \\mu_C \\) and \\( \\sigma_C \\) are the mean and variance of feature \\( x_i \\) for class \\( C \\).\n",
    "\n",
    "2. **Multinomial Naïve Bayes**:\n",
    "   - Suitable for **discrete features**, such as word counts in text data.\n",
    "   - Often used in text classification tasks (e.g., spam detection).\n",
    "   - Models the frequency or occurrence of events.\n",
    "3. **Bernoulli Naïve Bayes**:\n",
    "   - Designed for **binary/boolean features** (e.g., 0 or 1 indicating absence or presence).\n",
    "   - Useful in text classification when only the presence of a feature matters, not its frequency.\n",
    "\n",
    "### **b. Hyperparameters**\n",
    "- **Smoothing**: Prevents zero probabilities (e.g., Laplace smoothing with `alpha` in MultinomialNB).\n",
    "- **Prior**: Can specify class priors (`prior` in `scikit-learn`), otherwise estimated from data.\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages and Limitations\n",
    "\n",
    "### Advantages\n",
    "- **Efficiency**: Fast training and prediction, even with large datasets.\n",
    "- **Simplicity**: Easy to implement and interpret.\n",
    "- **Scalability**: Handles high-dimensional data well (e.g., text with many words).\n",
    "\n",
    "### Limitations\n",
    "- **Independence Assumption**: May fail if features are strongly correlated.\n",
    "- **Limited Expressiveness**: Less powerful than complex models (e.g., neural networks) for some tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## **Practical Examples**\n",
    "\n",
    "### **Example 1: Classification with Gaussian Naïve Bayes (Iris Dataset)**\n",
    "\n",
    "The Iris dataset contains measurements of flowers (sepal length, sepal width, petal length, petal width) labeled into three species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ebbbcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Class labels\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize and train the Gaussian Naïve Bayes classifier\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910a2233",
   "metadata": {},
   "source": [
    "**Output**: You’ll see an accuracy (e.g., 0.96) and a detailed report with precision, recall, and F1-score for each class.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example 2: Text Classification with Multinomial Naïve Bayes**\n",
    "Let’s classify short movie reviews as **positive (1)** or **negative (0)** based on word counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62923a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Synthetic text data\n",
    "texts = [\"good movie\", \"bad film\", \"great show\", \"terrible plot\"]\n",
    "labels = [1, 0, 1, 0]  # 1 = positive, 0 = negative\n",
    "\n",
    "# Vectorize text\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts).toarray()\n",
    "\n",
    "# Fit Multinomial NB\n",
    "mnb = MultinomialNB(alpha=1.0)  # Laplace smoothing\n",
    "mnb.fit(X, labels)\n",
    "\n",
    "# Predict\n",
    "test_text = [\"good show\"]\n",
    "X_test = vectorizer.transform(test_text).toarray()\n",
    "print(\"Prediction:\", mnb.predict(X_test))  # Output: [1]\n",
    "print(\"Probabilities:\\n\", mnb.predict_proba(X_test))\n",
    "\n",
    "# Evaluation\n",
    "y_pred = mnb.predict(X)\n",
    "print(\"Classification Report:\\n\", classification_report(labels, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae06f14",
   "metadata": {},
   "source": [
    "**Output**: Likely `1` (positive), as \"good show\" aligns with positive sentiment.\n",
    "\n",
    "---\n",
    "\n",
    "### **Example 3: Binary Features with Bernoulli Naïve Bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d3a9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Synthetic binary data\n",
    "X = np.array([[0, 1, 0], [1, 0, 1], [1, 1, 0], [0, 0, 1]])\n",
    "y = np.array([0, 1, 1, 0])\n",
    "\n",
    "# Fit Bernoulli NB\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "print(\"Predictions:\", bnb.predict(X))  # Output: [0 1 1 0]\n",
    "print(\"Feature Log Probabilities:\\n\", bnb.feature_log_prob_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e12c58",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Tools and Methods Summary**\n",
    "- **Modeling**: `sklearn.naive_bayes.GaussianNB`, `MultinomialNB`, `BernoulliNB`.\n",
    "    - `GaussianNB`: For continuous features.\n",
    "    - `MultinomialNB`: For discrete counts (e.g., text).\n",
    "    - `BernoulliNB`: For binary features.\n",
    "- **Text Processing**: `sklearn.feature_extraction.text.CountVectorizer`.\n",
    "    - `CountVectorizer`: Converts text into word count matrices.\n",
    "    - `TfidfVectorizer`: Alternative for weighting words by importance (can be used with MultinomialNB).\n",
    "- **Evaluation**: `sklearn.metrics.accuracy_score`, `classification_report`.\n",
    "    - `accuracy_score`: Overall correctness.\n",
    "    - `classification_report`: Detailed precision, recall, and F1-score.\n",
    "- **Visualization**: `matplotlib.pyplot.contourf()`, `seaborn.scatterplot()`.\n",
    "- **Handling Zero Probabilities**:\n",
    "   - Use **Laplace smoothing** (enabled by default in scikit-learn with `alpha=1.0`) to avoid zero likelihoods when a feature value is absent in training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0a9ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dca8153",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## How Naïve Bayes Works: A Hands-On Example\n",
    "\n",
    "Let’s explore how Naïve Bayes classifies data with a simple example: classifying emails as **spam** or **not spam** based on the presence of two words, \"free\" and \"offer.\"\n",
    "\n",
    "### Training Data\n",
    "We have six emails with binary features (1 = word present, 0 = word absent):\n",
    "- **Spam**: [1, 1], [1, 0], [0, 1] (3 emails)\n",
    "- **Not Spam**: [0, 0], [0, 1], [1, 0] (3 emails)\n",
    "\n",
    "### **The Goal**:  \n",
    "We want to guess if a new email (with both \"free\" and \"offer\") is spam or not spam, using past data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Basic Chances**  \n",
    "- Half of the emails in our training data are spam, and half are not.  \n",
    "  → **Chance of spam** = 50%  \n",
    "  → **Chance of not spam** = 50%  \n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Learn Word Patterns**  \n",
    "We check how often \"free\" and \"offer\" appear in spam vs. not-spam emails:  \n",
    "\n",
    "- **In spam emails**:  \n",
    "  - \"free\" appears in **2 out of 3** spam emails.  \n",
    "  - \"offer\" appears in **2 out of 3** spam emails.  \n",
    "\n",
    "- **In not-spam emails**:  \n",
    "  - \"free\" appears in **1 out of 3** not-spam emails.  \n",
    "  - \"offer\" appears in **1 out of 3** not-spam emails.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Guess the New Email**  \n",
    "The new email has **both \"free\" and \"offer\"**. We calculate two scores:  \n",
    "\n",
    "1. **Spam Score**:  \n",
    "   = (Chance of \"free\" in spam) × (Chance of \"offer\" in spam) × (Chance of spam)  \n",
    "   = \\( \\frac{2}{3} \\times \\frac{2}{3} \\times 0.5 = 0.222 \\)  \n",
    "\n",
    "2. **Not-Spam Score**:  \n",
    "   = (Chance of \"free\" in not-spam) × (Chance of \"offer\" in not-spam) × (Chance of not-spam)  \n",
    "   = \\( \\frac{1}{3} \\times \\frac{1}{3} \\times 0.5 = 0.056 \\)  \n",
    "\n",
    "---\n",
    "\n",
    "### **Result**:  \n",
    "The spam score (**0.222**) is higher than the not-spam score (**0.056**), so the email is marked as **spam**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Why It’s Called \"Naïve\"**:  \n",
    "It assumes \"free\" and \"offer\" don’t affect each other (even though in real life, they might). This simplification helps make quick guesses, even if it’s not 100% realistic.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}