
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>5Ô∏è‚É£ Optimization &amp; Training &#8212; Data Science Books</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-dropdown.css?v=995e94df" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-bootstrap.min.css?v=21c0b90a" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=d567e03f" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'contents/3_ml/7_optimization_and_training';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="ML: Life Cycle" href="6_ml_lifecycle.html" />
    <link rel="prev" title="K-Means Clustering" href="21_clustering.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Data Science Books</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Part I ‚Äî Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../0_maths/0_essential.html">Essential Mathematics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_maths/4_linear_algebra.html">Linear Algebra</a></li>







<li class="toctree-l1"><a class="reference internal" href="../0_maths/2_probability.html">Probability Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_maths/1_descriptive.html">Descriptive Statistics</a></li>








<li class="toctree-l1"><a class="reference internal" href="../0_maths/3_inferential.html">Inferential Statistics</a></li>



<li class="toctree-l1"><a class="reference internal" href="../0_maths/5_calculus.html">Calculus</a></li>







<li class="toctree-l1"><a class="reference internal" href="../0_maths/6_regression_analysis.html">Explanatory and Response Variables</a></li>


<li class="toctree-l1"><a class="reference internal" href="../1_python/1_basics.html">Python Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/2_advanced.html">Advanced Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/3_data_structures.html">Data Structures</a></li>





<li class="toctree-l1"><a class="reference internal" href="../1_python/4_modules_packages.html">Modules &amp; Packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/5_functions.html">Functions &amp; Modular Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/6_oop.html">Object-Oriented Programming</a></li>





<li class="toctree-l1"><a class="reference internal" href="../1_python/8_exceptions.html">Exception Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/9_regex.html">Regular Expressions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_numpy/1_numpy.html">NumPy (<strong>Numerical Python</strong>)</a></li>



<li class="toctree-l1"><a class="reference internal" href="../2_pandas/1_series.html">Pandas Series</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_pandas/2_dataframes.html">Pandas DataFrame</a></li>
















<li class="toctree-l1"><a class="reference internal" href="../2_pandas/3_visualization.html"><strong>What is Data Visualization in Data Science?</strong></a></li>


<li class="toctree-l1"><a class="reference internal" href="../2_pandas/4_eda.html">Exploratory Data Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_pandas/5_feature_engineering.html"><strong>What is Feature Engineering?</strong></a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part II ‚Äî Classical Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="1_foundations.html">ML Foundational</a></li>





<li class="toctree-l1"><a class="reference internal" href="2_data_preparation.html">2Ô∏è‚É£ Data Handling</a></li>





<li class="toctree-l1"><a class="reference internal" href="2_train_test_split.html">Train‚ÄìTest Split</a></li>
<li class="toctree-l1"><a class="reference internal" href="5_model_evaluation.html">4Ô∏è‚É£ Model Evaluation</a></li>




<li class="toctree-l1"><a class="reference internal" href="11_supervised_learning.html"><strong>Supervised Learning</strong></a></li>



<li class="toctree-l1"><a class="reference internal" href="12_regression.html">Regression Algorithms</a></li>







<li class="toctree-l1"><a class="reference internal" href="13_classification.html">Classification Algorithms</a></li>

<li class="toctree-l1"><a class="reference internal" href="10_core_algo.html">Core ML Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_decision_trees.html">Decision Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_ensemble_methods.html">Ensemble Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_svm.html">Support Vector Machine (SVM) in Detail</a></li>


<li class="toctree-l1"><a class="reference internal" href="17_knn.html">k-Nearest Neighbors</a></li>
<li class="toctree-l1"><a class="reference internal" href="18_naive_bayes.html">Naive Bayes</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part III ‚Äî Advanced Machine Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="20_unsupervised_learning.html">Unsupervised Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="21_clustering.html">Clustering Techniques</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">5Ô∏è‚É£ Optimization &amp; Training</a></li>







<li class="toctree-l1 has-children"><a class="reference internal" href="6_ml_lifecycle.html">ML Lifecycle</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="6_training.html">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="6_evaluation.html">Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="6_deployment.html">Deployment</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part IV ‚Äî Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl1_Introduction.html">Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl2_Neuron.html">Neuron</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl3_Libraries.html">Deep Learning Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl4_Terminology.html">Terminology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl5_multi_layer.html">Multi-Layer Perceptron</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl6_first_nn.html">First Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl7_evaluating_model.html">Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl8_multiclass_classification.html">Multiclass Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl9_multiclass_classification_hand.html">Handwritten Digit Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl10_saving_and_loading.html">Saving &amp; Loading Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl11_checkpointing.html">Model Checkpointing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl12_visualizing_model_training.html"><strong>Visualizing Model Training History in Deep Learning</strong></a></li>

<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl13_loss_functions_activation_functions_and_optimizers.html">Loss Functions &amp; Optimizers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part V ‚Äî NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp1.html">NLP Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp2.html">Text Cleaning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp3.html">Text Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp4.html">NLP Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp5.html">Bag of Words, TF-IDF</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp6.html">Sentiment Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp7.html">NLP with SpaCy</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part VI ‚Äî Career &amp; MLOps</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../6_interview/self%20introduction.html">Self Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6_interview/py.html">Python: Interview Guide</a></li>





<li class="toctree-l1"><a class="reference internal" href="../6_interview/pd.html">üìö Pandas: Interview Guide</a></li>


<li class="toctree-l1"><a class="reference internal" href="../6_interview/ml.html">Machine Learning: Interview Guide</a></li>













<li class="toctree-l1"><a class="reference internal" href="../6_interview/git.html">Git</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6_interview/dvc.html">DVC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6_interview/mlflow.html">MLflow</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/coding-fame/Data-science-books" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/coding-fame/Data-science-books/edit/main/contents/3_ml/7_optimization_and_training.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/coding-fame/Data-science-books/issues/new?title=Issue%20on%20page%20%2Fcontents/3_ml/7_optimization_and_training.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/contents/3_ml/7_optimization_and_training.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>5Ô∏è‚É£ Optimization & Training</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">5Ô∏è‚É£ Optimization &amp; Training</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-functions"><strong>Loss Functions</strong></a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-functions-in-machine-learning">Cost Functions in Machine Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-cost-functions">What Are Cost Functions?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#purpose-of-cost-functions">Purpose of Cost Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-the-model-use-cost-functions">How Does the Model Use Cost Functions?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#goal-of-model-training">Goal of Model Training</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-algorithms"><strong>Optimization Algorithms</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">Gradient Descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-gradient-descent">What is Gradient Descent?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-foundation">Mathematical Foundation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-gradient-descent-works">How Gradient Descent Works</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-gradient-descent-process"><strong>Understanding the Gradient Descent Process</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-values"><strong>Example Values</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#steps"><strong>Steps:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-approaches">Implementation Approaches</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-gradient-descent-approaches"><strong>Types of Gradient Descent Approaches</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fixed-step-approach-not-recommended"><strong>1Ô∏è‚É£ Fixed Step Approach (Not Recommended ‚ùå)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-approach-recommended"><strong>2Ô∏è‚É£ Learning Rate Approach (Recommended ‚úÖ)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-comparison">Learning Rate Comparison</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-gradient-descent">Types of Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-gradient-descent-bgd">1. Batch Gradient Descent (BGD)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent-sgd">2. Stochastic Gradient Descent (SGD)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mini-batch-gradient-descent">3. Mini-Batch Gradient Descent</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-linear-regression">Example: Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-function">Cost Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tools-and-libraries">Tools and Libraries</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#one-stop-solution-python-code-example">One-Stop Solution: Python Code Example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code">Code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-explanation">Code Explanation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#manual-gradient-descent">Manual Gradient Descent</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#scikit-learn-implementation">Scikit-learn Implementation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expected-output">Expected Output</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization"><strong>Regularization</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-and-ridge-regression">Lasso and Ridge Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-lasso-and-ridge-regression">Understanding Lasso and Ridge Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation-of-the-code-and-results">Explanation of the Code and Results</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#workflow">Workflow</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expected-observations">Expected Observations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-tips">Practical Tips</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameter-tuning"><strong>Hyperparameter Tuning</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-parameters">Model Parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-hyperparameters">Model Hyperparameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-hyperparameter-tuning">What is Hyperparameter Tuning?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-hyperparameter-tuning-important">Why is Hyperparameter Tuning Important?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-gridsearchcv">What is GridSearchCV?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-gridsearchcv-works">How GridSearchCV Works</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-parameters-of-gridsearchcv">Key Parameters of GridSearchCV</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tools-and-methods-for-hyperparameter-tuning">Tools and Methods for Hyperparameter Tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scikit-learn-tools">1. Scikit-learn Tools</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-libraries">2. Advanced Libraries</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-validation">3. Cross-Validation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scoring-metrics">4. Scoring Metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parallelization">5. Parallelization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1-tuning-a-random-forest-with-gridsearchcv">Example 1: Tuning a Random Forest with GridSearchCV</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-import-libraries-and-load-data">Step 1: Import Libraries and Load Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-define-the-parameter-grid">Step 2: Define the Parameter Grid</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-initialize-and-run-gridsearchcv">Step 3: Initialize and Run GridSearchCV</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-analyze-the-results">Step 4: Analyze the Results</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-2-tuning-xgboost-with-gridsearchcv">Example 2: Tuning XGBoost with GridSearchCV</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Step 1: Import Libraries and Load Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Step 2: Define the Parameter Grid</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Step 3: Initialize and Run GridSearchCV</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Step 4: Analyze the Results</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#alternatives-to-gridsearchcv">Alternatives to GridSearchCV</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-tips-for-hyperparameter-tuning">Practical Tips for Hyperparameter Tuning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ways-to-tune-hyperparameters">Ways to Tune Hyperparameters</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approach-1-manual-tuning-using-train-test-split">üìå Approach 1: Manual Tuning using <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approach-2-k-fold-cross-validation">üìå Approach 2: K-Fold Cross Validation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approach-3-gridsearchcv-exhaustive-search">üìå Approach 3: GridSearchCV (Exhaustive Search)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approach-4-randomizedsearchcv-efficient-search">üìå Approach 4: RandomizedSearchCV (Efficient Search)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#xgboost">XGBoost</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-xgboost"><strong>What is XGBoost?</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-xgboost"><strong>Why Use XGBoost?</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-it-works"><strong>How It Works</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts-and-methods"><strong>Key Concepts and Methods</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-core-mechanics"><strong>a. Core Mechanics</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#b-hyperparameters"><strong>b. Hyperparameters</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-use-cases">Examples of Use Cases</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-predicting-breast-cancer">1. Classification: Predicting Breast Cancer</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Step 1: Import Libraries and Load Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-train-the-xgboost-model">Step 2: Train the XGBoost Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-make-predictions-and-evaluate">Step 3: Make Predictions and Evaluate</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-feature-importance">Step 4: Feature Importance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-hyperparameter-tuning-with-gridsearchcv">Step 5: Hyperparameter Tuning with GridSearchCV</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-forecasting-house-prices">2. Regression: Forecasting House Prices</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Code</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tools-and-methods-summary"><strong>4. Tools and Methods Summary</strong></a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="optimization-training">
<h1>5Ô∏è‚É£ Optimization &amp; Training<a class="headerlink" href="#optimization-training" title="Link to this heading">#</a></h1>
</section>
<hr class="docutils" />
<section id="loss-functions">
<h1><strong>Loss Functions</strong><a class="headerlink" href="#loss-functions" title="Link to this heading">#</a></h1>
<ul class="simple">
<li><p>Cross-Entropy (classification), MSE (regression), Hinge Loss (SVM).</p></li>
</ul>
</section>
<section id="cost-functions-in-machine-learning">
<h1>Cost Functions in Machine Learning<a class="headerlink" href="#cost-functions-in-machine-learning" title="Link to this heading">#</a></h1>
<p>Cost functions, also referred to as loss functions or objective functions, are critical components in machine learning. They measure the difference between a model‚Äôs predicted outputs and the actual target values, providing a quantifiable metric of how well the model performs. The primary goal during training is to minimize this cost by adjusting the model‚Äôs parameters iteratively, thereby improving its predictions.</p>
<section id="what-are-cost-functions">
<h2>What Are Cost Functions?<a class="headerlink" href="#what-are-cost-functions" title="Link to this heading">#</a></h2>
<p>A cost function is a mathematical formula that evaluates the error in a model‚Äôs predictions. It serves as a guide for the optimization process, typically through techniques like gradient descent, where the model parameters are updated to reduce the cost. The choice of cost function depends on the type of machine learning task‚Äîregression or classification‚Äîand the specific problem being solved.</p>
</section>
<section id="purpose-of-cost-functions">
<h2>Purpose of Cost Functions<a class="headerlink" href="#purpose-of-cost-functions" title="Link to this heading">#</a></h2>
<p>The main purpose of a cost function is to:</p>
<ul class="simple">
<li><p>Quantify the model‚Äôs performance on a given dataset.</p></li>
<li><p>Provide a single scalar value to minimize during training.</p></li>
<li><p>Enable the optimization algorithm to adjust the model parameters effectively.</p></li>
</ul>
</section>
<section id="how-does-the-model-use-cost-functions">
<h2>How Does the Model Use Cost Functions?<a class="headerlink" href="#how-does-the-model-use-cost-functions" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>The model makes an <strong>initial prediction</strong> using randomly assigned weights.</p></li>
<li><p>The <strong>cost function</strong> calculates the <strong>error</strong> (difference between predicted and actual values).</p></li>
<li><p>The model <strong>adjusts its weights</strong> using optimization algorithms like <strong>Gradient Descent</strong> to reduce the error.</p></li>
<li><p>This process is repeated iteratively until the model reaches an <strong>optimal state</strong>.</p></li>
</ol>
</section>
<section id="goal-of-model-training">
<h2>Goal of Model Training<a class="headerlink" href="#goal-of-model-training" title="Link to this heading">#</a></h2>
<p>The <strong>primary goal</strong> of training a machine learning model is to <strong>minimize the cost function</strong> by adjusting weights iteratively. This ensures that the model learns patterns effectively and makes accurate predictions.</p>
</section>
</section>
<hr class="docutils" />
<section id="optimization-algorithms">
<h1><strong>Optimization Algorithms</strong><a class="headerlink" href="#optimization-algorithms" title="Link to this heading">#</a></h1>
<ul class="simple">
<li><p>Gradient Descent, Stochastic Gradient Descent (SGD), Adam.</p></li>
</ul>
<section id="gradient-descent">
<h2>Gradient Descent<a class="headerlink" href="#gradient-descent" title="Link to this heading">#</a></h2>
<p>Gradient Descent is a fundamental optimization algorithm widely used in machine learning and deep learning to minimize a cost function. The cost function quantifies how well a model performs on a dataset, and Gradient Descent helps find the optimal model parameters (weights) that yield the lowest cost.</p>
</section>
<hr class="docutils" />
<section id="what-is-gradient-descent">
<h2>What is Gradient Descent?<a class="headerlink" href="#what-is-gradient-descent" title="Link to this heading">#</a></h2>
<p>Gradient Descent is an iterative method that adjusts a model‚Äôs parameters by moving them in the direction that reduces the cost function most effectively. It leverages the gradient‚Äîa vector indicating the direction of steepest increase in the cost function. By moving in the opposite direction (negative gradient), the algorithm decreases the cost.</p>
</section>
<section id="mathematical-foundation">
<h2>Mathematical Foundation<a class="headerlink" href="#mathematical-foundation" title="Link to this heading">#</a></h2>
<p>For a parameter ( \theta ), the update rule is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\theta = \theta - \alpha \cdot \nabla J(\theta)
</pre></div>
</div>
</div>
</div>
<p>Where:</p>
<ul class="simple">
<li><p>( \theta ): The parameter being optimized (e.g., weights in a model).</p></li>
<li><p>( \alpha ): The learning rate, a hyperparameter controlling the step size.</p></li>
<li><p>( \nabla J(\theta) ): The gradient of the cost function ( J ) with respect to ( \theta ).</p></li>
</ul>
<p>The process repeats until the cost function converges‚Äîmeaning it no longer decreases significantly.</p>
</section>
<hr class="docutils" />
<section id="how-gradient-descent-works">
<h2>How Gradient Descent Works<a class="headerlink" href="#how-gradient-descent-works" title="Link to this heading">#</a></h2>
<p>Here‚Äôs the step-by-step process:</p>
<ol class="arabic">
<li><p><strong>Initialize Parameters</strong>: Start with random values for the parameters (e.g., m=0, b=0).</p></li>
<li><p><strong>Predict &amp; Calculate Cost</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">predictions</span> <span class="o">=</span> <span class="n">X</span> <span class="o">*</span> <span class="n">m</span> <span class="o">+</span> <span class="n">b</span>
<span class="n">cost</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><strong>Compute Gradient</strong>: Calculate the gradient of the cost function with respect to each parameter.</p>
<div class="highlight-math notranslate"><div class="highlight"><pre><span></span>\frac{\partial}{\partial m}J(m,b) = \frac{1}{n}\sum_{i=1}^n (y_i - (mx_i + b))(-x_i)
</pre></div>
</div>
</li>
<li><p><strong>Update Parameters</strong>: Adjust parameters by subtracting the product of the learning rate and the gradient.
Adjust weights using learning rate (Œ±):</p>
<div class="highlight-math notranslate"><div class="highlight"><pre><span></span>m := m - \alpha \cdot \frac{\partial J}{\partial m}
</pre></div>
</div>
<div class="highlight-math notranslate"><div class="highlight"><pre><span></span>b := b - \alpha \cdot \frac{\partial J}{\partial b}
</pre></div>
</div>
</li>
<li><p><strong>Repeat</strong>: Iterate until convergence, typically when the cost change becomes negligible.</p></li>
</ol>
<p><strong>Convergence in Gradient Descent</strong>
üöÄ <strong>Convergence</strong> is the stage where gradient descent makes only <strong>tiny changes</strong> in the objective function.</p>
<p>‚úÖ <strong>Convergence Achieved When:</strong></p>
<ul class="simple">
<li><p>Cost changes &lt; tolerance threshold (e.g., 0.001)</p></li>
<li><p>Maximum iterations reached</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Pseudocode Implementation
def gradient_descent(X, y, learning_rate=0.01, epochs=1000):
    m, b = 0, 0  # Initial parameters
    for _ in range(epochs):
        grad_m = calculate_gradient_m(X, y, m, b)
        grad_b = calculate_gradient_b(X, y, m, b)
        m -= learning_rate * grad_m
        b -= learning_rate * grad_b
    return m, b
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="understanding-the-gradient-descent-process">
<h2><strong>Understanding the Gradient Descent Process</strong><a class="headerlink" href="#understanding-the-gradient-descent-process" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Gradient Descent finds the <strong>best-fit line</strong> for a given dataset by minimizing error.</p></li>
<li><p>The error is measured using <strong>Mean Squared Error (MSE)</strong>.</p></li>
<li><p>If we plot <strong>MSE</strong> against model parameters (<code class="docutils literal notranslate"><span class="pre">m</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code>), we get a bowl-shaped curve.</p></li>
</ul>
<section id="example-values">
<h3><strong>Example Values</strong><a class="headerlink" href="#example-values" title="Link to this heading">#</a></h3>
<p><strong>Hypothetical Landscape:</strong></p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>m Value</p></th>
<th class="head"><p>b Value</p></th>
<th class="head"><p>MSE</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>0</p></td>
<td><p>1000</p></td>
</tr>
<tr class="row-odd"><td><p>50</p></td>
<td><p>15</p></td>
<td><p>800</p></td>
</tr>
<tr class="row-even"><td><p>70</p></td>
<td><p>5</p></td>
<td><p>400</p></td>
</tr>
<tr class="row-odd"><td><p>90</p></td>
<td><p>-5</p></td>
<td><p>100</p></td>
</tr>
<tr class="row-even"><td><p>100</p></td>
<td><p>-10</p></td>
<td><p>50</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="steps">
<h3><strong>Steps:</strong><a class="headerlink" href="#steps" title="Link to this heading">#</a></h3>
<p>1Ô∏è‚É£ <strong>Start with initial values</strong> (e.g., <code class="docutils literal notranslate"><span class="pre">m=0,</span> <span class="pre">b=0</span></code>).
2Ô∏è‚É£ Assume initial <strong>MSE = 1000</strong> at (<code class="docutils literal notranslate"><span class="pre">m=0</span></code>, <code class="docutils literal notranslate"><span class="pre">b=0</span></code>).
3Ô∏è‚É£ Slightly <strong>adjust <code class="docutils literal notranslate"><span class="pre">m</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code></strong> and check if the error decreases.
4Ô∏è‚É£ Repeat until we reach the <strong>minimum error (minima).</strong>
5Ô∏è‚É£ The final <code class="docutils literal notranslate"><span class="pre">m</span></code> and <code class="docutils literal notranslate"><span class="pre">b</span></code> values are used in the <strong>prediction function</strong>.</p>
</section>
</section>
<hr class="docutils" />
<section id="implementation-approaches">
<h2>Implementation Approaches<a class="headerlink" href="#implementation-approaches" title="Link to this heading">#</a></h2>
</section>
<section id="types-of-gradient-descent-approaches">
<h2><strong>Types of Gradient Descent Approaches</strong><a class="headerlink" href="#types-of-gradient-descent-approaches" title="Link to this heading">#</a></h2>
<section id="fixed-step-approach-not-recommended">
<h3><strong>1Ô∏è‚É£ Fixed Step Approach (Not Recommended ‚ùå)</strong><a class="headerlink" href="#fixed-step-approach-not-recommended" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Uses <strong>fixed step size</strong> to update parameters.</p></li>
<li><p>May <strong>overshoot</strong> or <strong>miss the global minima</strong>.</p></li>
<li><p><strong>Not efficient</strong> for complex functions.</p></li>
</ul>
</section>
<section id="learning-rate-approach-recommended">
<h3><strong>2Ô∏è‚É£ Learning Rate Approach (Recommended ‚úÖ)</strong><a class="headerlink" href="#learning-rate-approach-recommended" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>A <strong>tunable parameter</strong> that controls the step size in optimization.</p></li>
<li><p>Determines how quickly the algorithm moves towards the <strong>minimum loss</strong>.</p></li>
<li><p><strong>Each step is proportional to the slope</strong> at the current point.</p></li>
</ul>
</section>
<section id="learning-rate-comparison">
<h3>Learning Rate Comparison<a class="headerlink" href="#learning-rate-comparison" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Rate Type</p></th>
<th class="head"><p>Speed</p></th>
<th class="head"><p>Stability</p></th>
<th class="head"><p>Risk</p></th>
<th class="head"><p>Visual Cue</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Small (0.001)</strong></p></td>
<td><p>üê¢ Slow</p></td>
<td><p>üõ°Ô∏è High</p></td>
<td><p>Local minima</p></td>
<td><p>Careful steps</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Medium (0.1)</strong></p></td>
<td><p>üö∂‚ôÇÔ∏è Moderate</p></td>
<td><p>‚öñÔ∏è Balanced</p></td>
<td><p>Minimal</p></td>
<td><p>Optimal path</p></td>
</tr>
<tr class="row-even"><td><p><strong>Large (0.5)</strong></p></td>
<td><p>üöÄ Fast</p></td>
<td><p>üé¢ Low</p></td>
<td><p>Overshooting</p></td>
<td><p>Risky jumps</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<hr class="docutils" />
<section id="types-of-gradient-descent">
<h2>Types of Gradient Descent<a class="headerlink" href="#types-of-gradient-descent" title="Link to this heading">#</a></h2>
<p>Gradient Descent has several variants, each suited to different scenarios:</p>
<section id="batch-gradient-descent-bgd">
<h3>1. Batch Gradient Descent (BGD)<a class="headerlink" href="#batch-gradient-descent-bgd" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Description</strong>: Computes the gradient using the entire dataset at each step.</p></li>
<li><p><strong>Pros</strong>: Stable convergence due to averaging over all data points.</p></li>
<li><p><strong>Cons</strong>: Computationally expensive for large datasets, as it processes everything at once.</p></li>
</ul>
</section>
<section id="stochastic-gradient-descent-sgd">
<h3>2. Stochastic Gradient Descent (SGD)<a class="headerlink" href="#stochastic-gradient-descent-sgd" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Description</strong>: Updates parameters using the gradient from a single, randomly selected data point per iteration.</p></li>
<li><p><strong>Pros</strong>: Faster updates, can escape local minima due to noisy steps.</p></li>
<li><p><strong>Cons</strong>: Noisy updates may lead to erratic convergence.</p></li>
</ul>
</section>
<section id="mini-batch-gradient-descent">
<h3>3. Mini-Batch Gradient Descent<a class="headerlink" href="#mini-batch-gradient-descent" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Description</strong>: Uses a small subset (batch) of data points to compute the gradient.</p></li>
<li><p><strong>Pros</strong>: Balances the stability of BGD and the speed of SGD.</p></li>
<li><p><strong>Cons</strong>: Requires tuning the batch size for optimal performance.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="example-linear-regression">
<h2>Example: Linear Regression<a class="headerlink" href="#example-linear-regression" title="Link to this heading">#</a></h2>
<p>Let‚Äôs apply Gradient Descent to a simple linear regression problem, where the goal is to fit a line ( h_\theta(x) = \theta_0 + \theta_1 x ) to predict a continuous output based on one feature.</p>
<section id="cost-function">
<h3>Cost Function<a class="headerlink" href="#cost-function" title="Link to this heading">#</a></h3>
<p>The cost function is the Mean Squared Error (MSE):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2
</pre></div>
</div>
</div>
</div>
<p>Where:</p>
<ul class="simple">
<li><p>( m ): Number of training examples.</p></li>
<li><p>( h_\theta(x^{(i)}) ): Predicted value for the ( i )-th example.</p></li>
<li><p>( y^{(i)} ): Actual value for the ( i )-th example.</p></li>
</ul>
<p>Gradient Descent will minimize ( J(\theta) ) by adjusting ( \theta_0 ) (intercept) and ( \theta_1 ) (slope).</p>
</section>
</section>
<hr class="docutils" />
<section id="tools-and-libraries">
<h2>Tools and Libraries<a class="headerlink" href="#tools-and-libraries" title="Link to this heading">#</a></h2>
<p>Python offers powerful tools to implement Gradient Descent:</p>
<ul class="simple">
<li><p><strong>NumPy</strong>: For numerical computations and manual implementations.</p></li>
<li><p><strong>Scikit-learn</strong>: Provides optimized machine learning algorithms like linear regression.</p></li>
<li><p><strong>TensorFlow/Keras</strong>: Ideal for deep learning with automatic differentiation.</p></li>
<li><p><strong>PyTorch</strong>: Offers dynamic computation graphs for flexible optimization.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="one-stop-solution-python-code-example">
<h2>One-Stop Solution: Python Code Example<a class="headerlink" href="#one-stop-solution-python-code-example" title="Link to this heading">#</a></h2>
<p>Below is a complete Python script demonstrating Gradient Descent for linear regression. It includes a manual implementation using NumPy and a comparison with scikit-learn‚Äôs optimized version.</p>
<section id="code">
<h3>Code<a class="headerlink" href="#code" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression

# --- Manual Gradient Descent ---

# Generate synthetic data
np.random.seed(42)
X = 2 * np.random.rand(100, 1)  # Feature values
y = 4 + 3 * X + np.random.randn(100, 1)  # Target with noise

# Add bias term (x0 = 1) for intercept
X_b = np.c_[np.ones((100, 1)), X]

# Hyperparameters
learning_rate = 0.1
n_iterations = 1000
m = len(X_b)

# Initialize parameters randomly
theta = np.random.randn(2, 1)

# Gradient Descent loop
for iteration in range(n_iterations):
    gradients = (2/m) * X_b.T.dot(X_b.dot(theta) - y)
    theta = theta - learning_rate * gradients

# Results
print(&quot;Manual Gradient Descent Results:&quot;)
print(f&quot;Theta: {theta.ravel()}&quot;)

# Plot
plt.scatter(X, y, label=&quot;Data&quot;)
plt.plot(X, X_b.dot(theta), color=&#39;red&#39;, label=&quot;Manual GD Fit&quot;)
plt.title(&quot;Manual Gradient Descent&quot;)
plt.legend()
plt.show()

# --- Using Scikit-learn ---

# Train model
model = LinearRegression()
model.fit(X, y)
theta_sklearn = [model.intercept_[0], model.coef_[0][0]]

# Results
print(&quot;\nScikit-learn Results:&quot;)
print(f&quot;Theta: {theta_sklearn}&quot;)

# Plot
plt.scatter(X, y, label=&quot;Data&quot;)
plt.plot(X, model.predict(X), color=&#39;green&#39;, label=&quot;Scikit-learn Fit&quot;)
plt.title(&quot;Scikit-learn Linear Regression&quot;)
plt.legend()
plt.show()
</pre></div>
</div>
</div>
</div>
</section>
<section id="code-explanation">
<h3>Code Explanation<a class="headerlink" href="#code-explanation" title="Link to this heading">#</a></h3>
<section id="manual-gradient-descent">
<h4>Manual Gradient Descent<a class="headerlink" href="#manual-gradient-descent" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Data Generation</strong>: Creates synthetic data with a linear relationship (( y = 4 + 3x + \text{noise} )).</p></li>
<li><p><strong>Bias Term</strong>: Adds a column of ones to ( X ) for the intercept (( \theta_0 )).</p></li>
<li><p><strong>Hyperparameters</strong>: Sets learning rate (( \alpha = 0.1 )) and iterations (1000).</p></li>
<li><p><strong>Initialization</strong>: Starts with random ( \theta ) values.</p></li>
<li><p><strong>Gradient Descent</strong>: Computes gradients and updates ( \theta ) iteratively.</p></li>
<li><p><strong>Output</strong>: Prints ( \theta_0 ) and ( \theta_1 ), plots the fitted line.</p></li>
</ul>
</section>
<section id="scikit-learn-implementation">
<h4>Scikit-learn Implementation<a class="headerlink" href="#scikit-learn-implementation" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Model Training</strong>: Uses <code class="docutils literal notranslate"><span class="pre">LinearRegression</span></code> to fit the data.</p></li>
<li><p><strong>Output</strong>: Extracts intercept and slope, plots the result for comparison.</p></li>
</ul>
</section>
</section>
<section id="expected-output">
<h3>Expected Output<a class="headerlink" href="#expected-output" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Manual GD</strong>: ( \theta ) values close to [4, 3] (due to the data‚Äôs true relationship).</p></li>
<li><p><strong>Scikit-learn</strong>: Similar ( \theta ) values, optimized analytically.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>Gradient Descent is a cornerstone of machine learning optimization. Its variants Batch, Stochastic, and Mini-Batch offer flexibility for different dataset sizes and computational constraints.</p>
</section>
</section>
<hr class="docutils" />
<section id="regularization">
<h1><strong>Regularization</strong><a class="headerlink" href="#regularization" title="Link to this heading">#</a></h1>
<ul class="simple">
<li><p>L1 (Lasso), L2 (Ridge), Dropout (in neural networks).</p></li>
</ul>
<section id="lasso-and-ridge-regression">
<h2>Lasso and Ridge Regression<a class="headerlink" href="#lasso-and-ridge-regression" title="Link to this heading">#</a></h2>
<p>Lasso and Ridge Regression are powerful extensions of linear regression that incorporate <strong>regularization</strong> to prevent overfitting, especially when dealing with high-dimensional datasets or multicollinearity.</p>
</section>
<hr class="docutils" />
<section id="understanding-lasso-and-ridge-regression">
<h2>Understanding Lasso and Ridge Regression<a class="headerlink" href="#understanding-lasso-and-ridge-regression" title="Link to this heading">#</a></h2>
<p>Both Lasso and Ridge Regression modify the standard linear regression objective by adding a <strong>penalty term</strong> to the loss function. In ordinary least squares (OLS) regression, the goal is to minimize the sum of squared errors:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\text{Loss}_{\text{OLS}} = \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
``` 

However, OLS can overfit when there are many features or when features are highly correlated, leading to large coefficient values and poor generalization to new data. Regularization addresses this by constraining the coefficients.

## Ridge Regression (L2 Regularization)
- **Concept**: Ridge Regression adds a penalty based on the **sum of the squared coefficients** (L2 norm) to the OLS loss function.
- **Loss Function**:
  ```
  Minimize: Residual Sum of Squares + Œ± * (Œ£coefficients¬≤)
  ```
  Here, Œ± (`alpha`) is the regularization parameter that controls the strength of the penalty.
- **Effect**: Shrinks the coefficients toward zero but does not set them exactly to zero.
- **Use Case**: Ideal when all features are potentially relevant, and multicollinearity exists (e.g., highly correlated predictors). It reduces the impact of less important features without eliminating them.

## Lasso Regression (L1 Regularization)
- **Concept**: Lasso Regression adds a penalty based on the **sum of the absolute values of the coefficients** (L1 norm).
- **Loss Function**:
  ```
  Minimize: Residual Sum of Squares + Œ± * (Œ£|coefficients|)
  ```
- **Effect**: Can shrink some coefficients to exactly zero, effectively performing **feature selection**.
- **Use Case**: Best when many features are irrelevant or redundant, simplifying the model by excluding unimportant predictors.

## Key Differences
| Aspect               | Ridge Regression                  | Lasso Regression                  |
|----------------------|-----------------------------------|-----------------------------------|
| **Penalty Type**     | L2 norm (\(\sum \beta_j^2\))      | L1 norm (\(\sum |\beta_j|\))     |
| **Coefficient Effect**| Shrinks but keeps all non-zero    | Can set some to zero             |
| **Feature Selection**| No                                | Yes                              |
| **Multicollinearity**| Handles well by shrinking coefficients | May arbitrarily select one from correlated features |

## Additional Method: Elastic Net
- **Concept**: Combines L1 and L2 penalties, offering a balance between Lasso and Ridge.
- **Loss Function**:
    ```  
    Minimize: Residual Sum of Squares + Œª‚ÇÅ * (Œ£ |coefficients|) + Œª‚ÇÇ * (Œ£ coefficients¬≤)  
    ```  
- **Use Case**: Useful when there are groups of correlated features, as it can select entire groups rather than just one.

---

## Tools and Methods
To implement Lasso and Ridge Regression effectively, we rely on the following tools and methods:

- **Python Library**: **Scikit-learn** (`sklearn`) provides robust implementations:
  - `Ridge` and `Lasso` for basic models.
  - `RidgeCV` and `LassoCV` for automatic \(\lambda\) selection via cross-validation.
  - `ElasticNetCV` for combining L1 and L2 penalties.
- **Data Preprocessing**:
  - **Feature Scaling**: Use `StandardScaler` to standardize features (mean=0, variance=1), as regularization is sensitive to feature scales.
  - Handle missing values and encode categorical variables if necessary.
- **Hyperparameter Tuning**: \(\lambda\) (or `alpha` in scikit-learn) controls regularization strength. Cross-validation selects the optimal value.
- **Evaluation Metrics**: Mean Squared Error (MSE) or R-squared to assess model performance.
- **Visualization**: Plot coefficients to compare the effects of regularization.

---

## Example: Synthetic Dataset
To illustrate Lasso and Ridge Regression, we‚Äôll create a synthetic dataset with:
- **Relevant Features**: `X1`, `X2`, `X3` (with coefficients 3, 2, 0.5 in the true model).
- **Irrelevant Feature**: `X4` (random noise).
- **Correlated Feature**: `X5` (highly correlated with `X1`).
- **Target**: \( y = 3X1 + 2X2 + 0.5X3 + \text{noise} \).

We‚Äôll compare Linear Regression, Ridge, Lasso, and Elastic Net.

### Python One-Stop Solution
Below is a complete Python script that generates the data, trains the models, evaluates performance, and visualizes the results.

```python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt

# Generate synthetic data
np.random.seed(42)
n_samples = 100
X1 = np.random.randn(n_samples)
X2 = np.random.randn(n_samples)
X3 = np.random.randn(n_samples)
X4 = np.random.randn(n_samples)  # irrelevant
X5 = X1 + 0.1 * np.random.randn(n_samples)  # correlated with X1
y = 3*X1 + 2*X2 + 0.5*X3 + np.random.randn(n_samples)
X = pd.DataFrame({&#39;X1&#39;: X1, &#39;X2&#39;: X2, &#39;X3&#39;: X3, &#39;X4&#39;: X4, &#39;X5&#39;: X5})

# Split data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define range of alphas for regularization
alphas = np.logspace(-3, 3, 7)  # [0.001, 0.01, 0.1, 1, 10, 100, 1000]

# Train models
# 1. Linear Regression (no regularization)
lr = LinearRegression()
lr.fit(X_train_scaled, y_train)

# 2. Ridge Regression
ridge = RidgeCV(alphas=alphas)
ridge.fit(X_train_scaled, y_train)

# 3. Lasso Regression
lasso = LassoCV(alphas=alphas)
lasso.fit(X_train_scaled, y_train)

# 4. Elastic Net
elastic = ElasticNetCV(alphas=alphas, l1_ratio=[0.1, 0.5, 0.9])
elastic.fit(X_train_scaled, y_train)

# Predict on test set
y_pred_lr = lr.predict(X_test_scaled)
y_pred_ridge = ridge.predict(X_test_scaled)
y_pred_lasso = lasso.predict(X_test_scaled)
y_pred_elastic = elastic.predict(X_test_scaled)

# Evaluate models using MSE
print(&quot;=== Model Performance (MSE) ===&quot;)
print(f&quot;Linear Regression MSE: {mean_squared_error(y_test, y_pred_lr):.4f}&quot;)
print(f&quot;Ridge MSE: {mean_squared_error(y_test, y_pred_ridge):.4f}&quot;)
print(f&quot;Best alpha for Ridge: {ridge.alpha_}&quot;)
print(f&quot;Lasso MSE: {mean_squared_error(y_test, y_pred_lasso):.4f}&quot;)
print(f&quot;Best alpha for Lasso: {lasso.alpha_}&quot;)
print(f&quot;ElasticNet MSE: {mean_squared_error(y_test, y_pred_elastic):.4f}&quot;)
print(f&quot;Best alpha for ElasticNet: {elastic.alpha_}, Best l1_ratio: {elastic.l1_ratio_}&quot;)

# Extract coefficients
coef_lr = lr.coef_
coef_ridge = ridge.coef_
coef_lasso = lasso.coef_
coef_elastic = elastic.coef_

# Plot coefficients
features = X.columns
plt.figure(figsize=(10, 6))
plt.plot(coef_lr, &#39;o&#39;, label=&#39;Linear Regression&#39;)
plt.plot(coef_ridge, &#39;o&#39;, label=&#39;Ridge&#39;)
plt.plot(coef_lasso, &#39;o&#39;, label=&#39;Lasso&#39;)
plt.plot(coef_elastic, &#39;o&#39;, label=&#39;ElasticNet&#39;)
plt.xticks(range(len(features)), features)
plt.ylabel(&#39;Coefficient Value&#39;)
plt.title(&#39;Comparison of Coefficients Across Models&#39;)
plt.legend()
plt.grid(True)
plt.show()

# Feature selection by Lasso
selected_features = features[coef_lasso != 0].tolist()
print(&quot;=== Lasso Feature Selection ===&quot;)
print(f&quot;Features selected by Lasso: {selected_features}&quot;)
print(f&quot;Number of features selected: {len(selected_features)}&quot;)
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="explanation-of-the-code-and-results">
<h2>Explanation of the Code and Results<a class="headerlink" href="#explanation-of-the-code-and-results" title="Link to this heading">#</a></h2>
<section id="workflow">
<h3>Workflow<a class="headerlink" href="#workflow" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Data Generation</strong>:</p>
<ul class="simple">
<li><p>Five features (<code class="docutils literal notranslate"><span class="pre">X1</span></code> to <code class="docutils literal notranslate"><span class="pre">X5</span></code>) are created with <code class="docutils literal notranslate"><span class="pre">X4</span></code> being irrelevant and <code class="docutils literal notranslate"><span class="pre">X5</span></code> correlated with <code class="docutils literal notranslate"><span class="pre">X1</span></code>.</p></li>
<li><p>The target <code class="docutils literal notranslate"><span class="pre">y</span></code> is a linear combination of <code class="docutils literal notranslate"><span class="pre">X1</span></code>, <code class="docutils literal notranslate"><span class="pre">X2</span></code>, and <code class="docutils literal notranslate"><span class="pre">X3</span></code> plus noise.</p></li>
</ul>
</li>
<li><p><strong>Data Preprocessing</strong>:</p>
<ul class="simple">
<li><p>Split into 80% training and 20% test sets.</p></li>
<li><p>Features are scaled using <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code> to ensure regularization works correctly.</p></li>
</ul>
</li>
<li><p><strong>Model Training</strong>:</p>
<ul class="simple">
<li><p><strong>Linear Regression</strong>: No regularization.</p></li>
<li><p><strong>Ridge</strong>: Uses <code class="docutils literal notranslate"><span class="pre">RidgeCV</span></code> to select the best (\lambda) from a logarithmic range.</p></li>
<li><p><strong>Lasso</strong>: Uses <code class="docutils literal notranslate"><span class="pre">LassoCV</span></code> for (\lambda) selection.</p></li>
<li><p><strong>Elastic Net</strong>: Tunes both (\lambda) and <code class="docutils literal notranslate"><span class="pre">l1_ratio</span></code> (mix of L1 and L2 penalties).</p></li>
</ul>
</li>
<li><p><strong>Evaluation</strong>:</p>
<ul class="simple">
<li><p>MSE is calculated for each model on the test set.</p></li>
<li><p>Optimal (\lambda) (and <code class="docutils literal notranslate"><span class="pre">l1_ratio</span></code> for Elastic Net) is reported.</p></li>
</ul>
</li>
<li><p><strong>Visualization</strong>:</p>
<ul class="simple">
<li><p>Coefficients are plotted to show how each model treats the features.</p></li>
<li><p>Lasso typically sets coefficients of irrelevant (<code class="docutils literal notranslate"><span class="pre">X4</span></code>) or redundant (<code class="docutils literal notranslate"><span class="pre">X5</span></code>) features to zero, while Ridge shrinks all coefficients.</p></li>
</ul>
</li>
<li><p><strong>Feature Selection</strong>:</p>
<ul class="simple">
<li><p>Lasso identifies the most relevant features by setting some coefficients to zero.</p></li>
</ul>
</li>
</ol>
</section>
<section id="expected-observations">
<h3>Expected Observations<a class="headerlink" href="#expected-observations" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Linear Regression</strong>: Coefficients may be large, especially for correlated features (<code class="docutils literal notranslate"><span class="pre">X1</span></code> and <code class="docutils literal notranslate"><span class="pre">X5</span></code>).</p></li>
<li><p><strong>Ridge</strong>: All coefficients are non-zero but reduced in magnitude.</p></li>
<li><p><strong>Lasso</strong>: Likely sets <code class="docutils literal notranslate"><span class="pre">X4</span></code> (irrelevant) and possibly <code class="docutils literal notranslate"><span class="pre">X5</span></code> (correlated with <code class="docutils literal notranslate"><span class="pre">X1</span></code>) to zero, selecting <code class="docutils literal notranslate"><span class="pre">X1</span></code>, <code class="docutils literal notranslate"><span class="pre">X2</span></code>, and <code class="docutils literal notranslate"><span class="pre">X3</span></code>.</p></li>
<li><p><strong>Elastic Net</strong>: Behavior depends on <code class="docutils literal notranslate"><span class="pre">l1_ratio</span></code>; closer to Lasso with high <code class="docutils literal notranslate"><span class="pre">l1_ratio</span></code>, closer to Ridge with low <code class="docutils literal notranslate"><span class="pre">l1_ratio</span></code>.</p></li>
<li><p><strong>MSE</strong>: Regularized models may have slightly higher MSE on this small dataset but generalize better in practice.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="practical-tips">
<h2>Practical Tips<a class="headerlink" href="#practical-tips" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Choosing (\lambda)</strong>: Use a wide range (e.g., <code class="docutils literal notranslate"><span class="pre">np.logspace(-3,</span> <span class="pre">3,</span> <span class="pre">7)</span></code>) and let cross-validation decide.</p></li>
<li><p><strong>Real Datasets</strong>: Handle missing values, encode categorical variables, and explore multicollinearity (e.g., via correlation matrices).</p></li>
<li><p><strong>High-Dimensional Data</strong>: Lasso and Elastic Net shine in feature selection for datasets with many predictors (e.g., genomics, text analysis).</p></li>
<li><p><strong>Extensions</strong>: Use <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> for more flexible hyperparameter tuning if needed.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id1">
<h2>Conclusion<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>Lasso and Ridge Regression enhance linear regression by adding regularization to control model complexity. Ridge is excellent for handling multicollinearity and retaining all features, while Lasso excels at feature selection by eliminating irrelevant predictors.</p>
</section>
</section>
<hr class="docutils" />
<section id="hyperparameter-tuning">
<h1><strong>Hyperparameter Tuning</strong><a class="headerlink" href="#hyperparameter-tuning" title="Link to this heading">#</a></h1>
<ul class="simple">
<li><p>Grid Search, Random Search, Bayesian Optimization.</p></li>
</ul>
<p>GridSearchCV and Hyperparameter Tuning</p>
<p>Hyperparameter tuning is a critical step in machine learning to optimize model performance by finding the best settings for a model‚Äôs hyperparameters. Among the tools available for this purpose, <strong>GridSearchCV</strong> from scikit-learn stands out as a robust and widely used method.</p>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>When building a Machine Learning model, two key components play a crucial role:</p>
<section id="model-parameters">
<h3>Model Parameters<a class="headerlink" href="#model-parameters" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Internal values that the model learns automatically from the data.</p></li>
<li><p>Example: The weights in a neural network, or the support vectors in an SVM.</p></li>
</ul>
</section>
<section id="model-hyperparameters">
<h3>Model Hyperparameters<a class="headerlink" href="#model-hyperparameters" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>External configurations set by the programmer to optimize the model‚Äôs performance.</p></li>
<li><p>Example: Learning rate, the number of trees in a random forest, or the kernel type in SVM.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="what-is-hyperparameter-tuning">
<h2>What is Hyperparameter Tuning?<a class="headerlink" href="#what-is-hyperparameter-tuning" title="Link to this heading">#</a></h2>
<p>In machine learning, <strong>hyperparameters</strong> are settings defined before training a model, unlike model parameters, which are learned during training. Examples include the learning rate in gradient boosting, the number of trees in a random forest, or the regularization strength in logistic regression. <strong>Hyperparameter tuning</strong> involves searching for the combination of these settings that maximizes a model‚Äôs performance, typically evaluated using metrics like accuracy, F1-score, or mean squared error.</p>
<section id="why-is-hyperparameter-tuning-important">
<h3>Why is Hyperparameter Tuning Important?<a class="headerlink" href="#why-is-hyperparameter-tuning-important" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Performance Optimization</strong>: Small changes in hyperparameters can lead to significant improvements in model accuracy or other metrics.</p></li>
<li><p><strong>Avoid Overfitting/Underfitting</strong>: Tuning helps strike a balance between bias and variance, ensuring the model generalizes well to unseen data.</p></li>
<li><p><strong>Efficiency</strong>: Automated tuning methods save time and effort compared to manual trial-and-error.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="what-is-gridsearchcv">
<h2>What is GridSearchCV?<a class="headerlink" href="#what-is-gridsearchcv" title="Link to this heading">#</a></h2>
<p><strong>GridSearchCV</strong> (Grid Search with Cross-Validation) is a scikit-learn tool designed to systematically explore a predefined set of hyperparameter combinations. It evaluates each combination using cross-validation and selects the one with the best performance.</p>
<section id="how-gridsearchcv-works">
<h3>How GridSearchCV Works<a class="headerlink" href="#how-gridsearchcv-works" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Define a Parameter Grid</strong>: Create a dictionary where keys are hyperparameter names and values are lists of possible settings to test.</p></li>
<li><p><strong>Cross-Validation</strong>: For each combination in the grid, train and evaluate the model using cross-validation (e.g., k-fold cross-validation).</p></li>
<li><p><strong>Select the Best Model</strong>: Identify the combination yielding the highest cross-validation score, such as accuracy or F1-score.</p></li>
</ol>
</section>
<section id="key-parameters-of-gridsearchcv">
<h3>Key Parameters of GridSearchCV<a class="headerlink" href="#key-parameters-of-gridsearchcv" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>estimator</strong>: The machine learning model to tune (e.g., <code class="docutils literal notranslate"><span class="pre">RandomForestClassifier()</span></code>).</p></li>
<li><p><strong>param_grid</strong>: A dictionary specifying the hyperparameters and their possible values.</p></li>
<li><p><strong>cv</strong>: The number of cross-validation folds (e.g., 5 for 5-fold CV).</p></li>
<li><p><strong>scoring</strong>: The metric to optimize (e.g., <code class="docutils literal notranslate"><span class="pre">'accuracy'</span></code>, <code class="docutils literal notranslate"><span class="pre">'f1'</span></code>, <code class="docutils literal notranslate"><span class="pre">'neg_mean_squared_error'</span></code>).</p></li>
<li><p><strong>n_jobs</strong>: Number of CPU cores to use for parallel processing (e.g., <code class="docutils literal notranslate"><span class="pre">-1</span></code> to use all available cores).</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="tools-and-methods-for-hyperparameter-tuning">
<h2>Tools and Methods for Hyperparameter Tuning<a class="headerlink" href="#tools-and-methods-for-hyperparameter-tuning" title="Link to this heading">#</a></h2>
<p>While GridSearchCV is a cornerstone of hyperparameter tuning, other tools and methods can complement or replace it depending on your needs.</p>
<section id="scikit-learn-tools">
<h3>1. Scikit-learn Tools<a class="headerlink" href="#scikit-learn-tools" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>GridSearchCV</strong>: Exhaustively tests all combinations in the parameter grid.</p></li>
<li><p><strong>RandomizedSearchCV</strong>: Randomly samples a fixed number of combinations, making it faster for large grids.</p></li>
<li><p><strong>HalvingGridSearchCV</strong>: Uses successive halving to allocate resources to promising combinations, improving efficiency.</p></li>
</ul>
</section>
<section id="advanced-libraries">
<h3>2. Advanced Libraries<a class="headerlink" href="#advanced-libraries" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Optuna</strong>: A flexible framework using Bayesian optimization to efficiently search for optimal hyperparameters.</p></li>
<li><p><strong>Hyperopt</strong>: Another Bayesian optimization tool compatible with various models.</p></li>
<li><p><strong>Scikit-optimize</strong>: Provides Bayesian optimization specifically for scikit-learn models.</p></li>
</ul>
</section>
<section id="cross-validation">
<h3>3. Cross-Validation<a class="headerlink" href="#cross-validation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Cross-validation ensures reliable performance estimates. For classification tasks, use <code class="docutils literal notranslate"><span class="pre">StratifiedKFold</span></code> to preserve class distributions across folds.</p></li>
</ul>
</section>
<section id="scoring-metrics">
<h3>4. Scoring Metrics<a class="headerlink" href="#scoring-metrics" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Choose metrics based on your problem:</p>
<ul>
<li><p>Classification: <code class="docutils literal notranslate"><span class="pre">'accuracy'</span></code>, <code class="docutils literal notranslate"><span class="pre">'f1'</span></code>, <code class="docutils literal notranslate"><span class="pre">'roc_auc'</span></code>.</p></li>
<li><p>Regression: <code class="docutils literal notranslate"><span class="pre">'neg_mean_squared_error'</span></code>, <code class="docutils literal notranslate"><span class="pre">'r2'</span></code>.</p></li>
</ul>
</li>
</ul>
</section>
<section id="parallelization">
<h3>5. Parallelization<a class="headerlink" href="#parallelization" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Set <code class="docutils literal notranslate"><span class="pre">n_jobs=-1</span></code> in GridSearchCV to leverage multiple CPU cores.</p></li>
<li><p>For very large datasets, consider distributed frameworks like Dask or Spark.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="example-1-tuning-a-random-forest-with-gridsearchcv">
<h2>Example 1: Tuning a Random Forest with GridSearchCV<a class="headerlink" href="#example-1-tuning-a-random-forest-with-gridsearchcv" title="Link to this heading">#</a></h2>
<p>Let‚Äôs use the <strong>Iris dataset</strong> to tune a <strong>Random Forest Classifier</strong> with GridSearchCV.</p>
<section id="step-1-import-libraries-and-load-data">
<h3>Step 1: Import Libraries and Load Data<a class="headerlink" href="#step-1-import-libraries-and-load-data" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

# Load Iris dataset
iris = load_iris()
X = iris.data
y = iris.target

# Split into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-2-define-the-parameter-grid">
<h3>Step 2: Define the Parameter Grid<a class="headerlink" href="#step-2-define-the-parameter-grid" title="Link to this heading">#</a></h3>
<p>We‚Äôll tune three hyperparameters for the Random Forest:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>: Number of trees.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_depth</span></code>: Maximum depth of each tree.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">min_samples_split</span></code>: Minimum samples required to split a node.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>param_grid = {
    &#39;n_estimators&#39;: [50, 100, 200],
    &#39;max_depth&#39;: [None, 10, 20, 30],
    &#39;min_samples_split&#39;: [2, 5, 10]
}
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-3-initialize-and-run-gridsearchcv">
<h3>Step 3: Initialize and Run GridSearchCV<a class="headerlink" href="#step-3-initialize-and-run-gridsearchcv" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Initialize the Random Forest Classifier
rf = RandomForestClassifier(random_state=42)

# Initialize GridSearchCV with 5-fold cross-validation
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring=&#39;accuracy&#39;, n_jobs=-1)

# Fit the grid search to the data
grid_search.fit(X_train, y_train)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">cv=5</span></code>: Use 5-fold cross-validation.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_jobs=-1</span></code>: Utilize all CPU cores for faster computation.</p></li>
</ul>
</section>
<section id="step-4-analyze-the-results">
<h3>Step 4: Analyze the Results<a class="headerlink" href="#step-4-analyze-the-results" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Best parameters and best score
print(f&quot;Best Parameters: {grid_search.best_params_}&quot;)
print(f&quot;Best Cross-Validation Accuracy: {grid_search.best_score_:.4f}&quot;)

# Test the best model on the test set
best_rf = grid_search.best_estimator_
y_pred = best_rf.predict(X_test)
test_accuracy = accuracy_score(y_test, y_pred)
print(f&quot;Test Accuracy: {test_accuracy:.4f}&quot;)
</pre></div>
</div>
</div>
</div>
<p><strong>Sample Output:</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Best Parameters: {&#39;max_depth&#39;: None, &#39;min_samples_split&#39;: 2, &#39;n_estimators&#39;: 100}
Best Cross-Validation Accuracy: 0.9583
Test Accuracy: 1.0000
</pre></div>
</div>
</div>
</div>
<p>This output indicates that the best Random Forest configuration achieves a cross-validation accuracy of 95.83% and a perfect test accuracy of 100% on the Iris dataset.</p>
</section>
</section>
<hr class="docutils" />
<section id="example-2-tuning-xgboost-with-gridsearchcv">
<h2>Example 2: Tuning XGBoost with GridSearchCV<a class="headerlink" href="#example-2-tuning-xgboost-with-gridsearchcv" title="Link to this heading">#</a></h2>
<p>Now, let‚Äôs tune an <strong>XGBoost Classifier</strong> using the <strong>Breast Cancer dataset</strong>.</p>
<section id="id2">
<h3>Step 1: Import Libraries and Load Data<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score
import xgboost as xgb

# Load dataset
data = load_breast_cancer()
X = data.data
y = data.target

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
</pre></div>
</div>
</div>
</div>
</section>
<section id="id3">
<h3>Step 2: Define the Parameter Grid<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<p>For XGBoost, we‚Äôll tune:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>: Step size for updates.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_depth</span></code>: Maximum tree depth.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>: Number of trees.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">subsample</span></code>: Fraction of samples used per tree.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>param_grid = {
    &#39;learning_rate&#39;: [0.01, 0.1, 0.2],
    &#39;max_depth&#39;: [3, 5, 7],
    &#39;n_estimators&#39;: [50, 100, 200],
    &#39;subsample&#39;: [0.8, 1.0]
}
</pre></div>
</div>
</div>
</div>
</section>
<section id="id4">
<h3>Step 3: Initialize and Run GridSearchCV<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Initialize XGBoost Classifier
xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric=&#39;logloss&#39;)

# Initialize and fit GridSearchCV
grid_search = GridSearchCV(xgb_clf, param_grid, cv=5, scoring=&#39;accuracy&#39;, n_jobs=-1)
grid_search.fit(X_train, y_train)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">use_label_encoder=False</span></code> and <code class="docutils literal notranslate"><span class="pre">eval_metric='logloss'</span></code>: Required for newer XGBoost versions to avoid warnings.</p></li>
</ul>
</section>
<section id="id5">
<h3>Step 4: Analyze the Results<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Results
print(f&quot;Best Parameters: {grid_search.best_params_}&quot;)
print(f&quot;Best CV Accuracy: {grid_search.best_score_:.4f}&quot;)
print(f&quot;Test Accuracy: {accuracy_score(y_test, grid_search.best_estimator_.predict(X_test)):.4f}&quot;)
</pre></div>
</div>
</div>
</div>
<p><strong>Sample Output:</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Best Parameters: {&#39;learning_rate&#39;: 0.1, &#39;max_depth&#39;: 3, &#39;n_estimators&#39;: 100, &#39;subsample&#39;: 1.0}
Best CV Accuracy: 0.9714
Test Accuracy: 0.9737
</pre></div>
</div>
</div>
</div>
<p>This shows that the tuned XGBoost model achieves a cross-validation accuracy of 97.14% and a test accuracy of 97.37%.</p>
</section>
</section>
<hr class="docutils" />
<section id="alternatives-to-gridsearchcv">
<h2>Alternatives to GridSearchCV<a class="headerlink" href="#alternatives-to-gridsearchcv" title="Link to this heading">#</a></h2>
<p>While GridSearchCV is effective, it can be slow for large grids or datasets due to its exhaustive nature. Consider these alternatives:</p>
<ul class="simple">
<li><p><strong>RandomizedSearchCV</strong>: Samples a subset of combinations, reducing computation time while often finding near-optimal settings.</p>
<ul>
<li><p>Example: Replace <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> with <code class="docutils literal notranslate"><span class="pre">RandomizedSearchCV</span></code> and add <code class="docutils literal notranslate"><span class="pre">n_iter=10</span></code> to test 10 random combinations.</p></li>
</ul>
</li>
<li><p><strong>Bayesian Optimization</strong>: Uses probabilistic models to intelligently explore the parameter space (e.g., via Optuna or Hyperopt).</p></li>
<li><p><strong>HalvingGridSearchCV</strong>: Starts with a small subset of data and progressively focuses on promising combinations.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="practical-tips-for-hyperparameter-tuning">
<h2>Practical Tips for Hyperparameter Tuning<a class="headerlink" href="#practical-tips-for-hyperparameter-tuning" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Start with a Coarse Grid</strong>: Test a broad range of values first, then refine around the best ones.</p></li>
<li><p><strong>Use RandomizedSearchCV for Large Grids</strong>: It‚Äôs more efficient when the parameter space is vast.</p></li>
<li><p><strong>Log-Scale for Continuous Parameters</strong>: For parameters like learning rate, use values like <code class="docutils literal notranslate"><span class="pre">[0.001,</span> <span class="pre">0.01,</span> <span class="pre">0.1,</span> <span class="pre">1]</span></code>.</p></li>
<li><p><strong>Early Stopping</strong>: For models like XGBoost, stop training if performance doesn‚Äôt improve (not directly supported in GridSearchCV but available in native XGBoost).</p></li>
<li><p><strong>Feature Engineering</strong>: Good features can reduce the need for extensive tuning.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id6">
<h2>Conclusion<a class="headerlink" href="#id6" title="Link to this heading">#</a></h2>
<p>GridSearchCV is a powerful and straightforward tool for hyperparameter tuning, automating the process of finding the best model configuration through exhaustive search and cross-validation. While it excels in reliability, alternatives like RandomizedSearchCV or Bayesian optimization tools (e.g., Optuna) offer efficiency for larger problems.</p>
</section>
<hr class="docutils" />
<section id="ways-to-tune-hyperparameters">
<h2>Ways to Tune Hyperparameters<a class="headerlink" href="#ways-to-tune-hyperparameters" title="Link to this heading">#</a></h2>
<section id="approach-1-manual-tuning-using-train-test-split">
<h3>üìå Approach 1: Manual Tuning using <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code><a class="headerlink" href="#approach-1-manual-tuning-using-train-test-split" title="Link to this heading">#</a></h3>
<p>A basic method where we split the dataset into training and testing sets and manually adjust parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from sklearn.svm import SVC
from sklearn.model_selection import train_test_split

# Create and train model with manually selected parameters
model = SVC(kernel=&#39;rbf&#39;, C=30, gamma=&#39;auto&#39;)
model.fit(X_train, y_train)
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="approach-2-k-fold-cross-validation">
<h3>üìå Approach 2: K-Fold Cross Validation<a class="headerlink" href="#approach-2-k-fold-cross-validation" title="Link to this heading">#</a></h3>
<p>Instead of a single train-test split, K-Fold Cross Validation divides data into multiple subsets (folds) and trains on different combinations.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from sklearn.model_selection import cross_val_score
from sklearn.svm import SVC
from sklearn.datasets import load_iris

iris = load_iris()
X = iris.data
y = iris.target

# Try different kernel and C values
sc1 = cross_val_score(svm.SVC(kernel=&#39;linear&#39;, C=10, gamma=&#39;auto&#39;), X, y, cv=5)
sc2 = cross_val_score(svm.SVC(kernel=&#39;rbf&#39;, C=10, gamma=&#39;auto&#39;), X, y, cv=5)
sc3 = cross_val_score(svm.SVC(kernel=&#39;rbf&#39;, C=20, gamma=&#39;auto&#39;), X, y, cv=5)
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="approach-3-gridsearchcv-exhaustive-search">
<h3>üìå Approach 3: GridSearchCV (Exhaustive Search)<a class="headerlink" href="#approach-3-gridsearchcv-exhaustive-search" title="Link to this heading">#</a></h3>
<p><strong>GridSearchCV</strong> automates hyperparameter tuning by exhaustively searching through a predefined set of hyperparameters to find the best combination.
It evaluates every combination of hyperparameters and selects the best performing one.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVC
from sklearn.datasets import load_iris

iris = load_iris()

# Define the hyperparameter grid
param_grid = {
    &#39;C&#39;: [1, 10, 20],
    &#39;kernel&#39;: [&#39;rbf&#39;, &#39;linear&#39;]
}

# Initialize GridSearchCV with cross-validation
clf = GridSearchCV(SVC(gamma=&#39;auto&#39;), param_grid, cv=5, return_train_score=False)

# Fit the model and find the best parameters
clf.fit(iris.data, iris.target)

# Print the best parameters and score
print(&quot;Best Parameters:&quot;, clf.best_params_)
print(&quot;Best Score:&quot;, clf.best_score_)
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="approach-4-randomizedsearchcv-efficient-search">
<h3>üìå Approach 4: RandomizedSearchCV (Efficient Search)<a class="headerlink" href="#approach-4-randomizedsearchcv-efficient-search" title="Link to this heading">#</a></h3>
<p>Instead of an exhaustive search, RandomizedSearchCV selects a limited number of parameter combinations randomly, reducing computation time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from sklearn.model_selection import RandomizedSearchCV
from sklearn.svm import SVC
from sklearn.datasets import load_iris

iris = load_iris()

# Define the hyperparameter distribution
param_dist = {
    &#39;C&#39;: [1, 10, 20],
    &#39;kernel&#39;: [&#39;rbf&#39;, &#39;linear&#39;]
}

# Initialize RandomizedSearchCV
rs = RandomizedSearchCV(SVC(gamma=&#39;auto&#39;), param_dist, cv=5, return_train_score=False, n_iter=2)

# Fit the model and find the best parameters
rs.fit(iris.data, iris.target)

# Print the best parameters
print(&quot;Best Parameters:&quot;, rs.best_params_)
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<hr class="docutils" />
<section id="xgboost">
<h1>XGBoost<a class="headerlink" href="#xgboost" title="Link to this heading">#</a></h1>
<p>XGBoost (Extreme Gradient Boosting) is a powerful and efficient machine learning algorithm widely used for structured/tabular data problems such as classification, regression, and ranking.</p>
<hr class="docutils" />
<section id="what-is-xgboost">
<h2><strong>What is XGBoost?</strong><a class="headerlink" href="#what-is-xgboost" title="Link to this heading">#</a></h2>
<p><strong>XGBoost (eXtreme Gradient Boosting)</strong> is an advanced ensemble machine learning algorithm that combines multiple decision trees using gradient boosting.</p>
<p>It builds an ensemble of weak learners (typically decision trees) sequentially, where each tree corrects the errors of its predecessors, guided by gradient descent on a loss function.</p>
</section>
<section id="why-use-xgboost">
<h2><strong>Why Use XGBoost?</strong><a class="headerlink" href="#why-use-xgboost" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>High Performance</strong>: Often outperforms other algorithms in accuracy and speed.</p></li>
<li><p><strong>Flexibility</strong>: Supports classification, regression, ranking, and more.</p></li>
<li><p><strong>Robustness</strong>: Handles missing data, overfitting, and noisy datasets well.</p></li>
<li><p><strong>Feature Importance</strong>: Provides insights into key predictors.</p></li>
</ul>
</section>
<section id="how-it-works">
<h2><strong>How It Works</strong><a class="headerlink" href="#how-it-works" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>Base Learners</strong>: Starts with weak decision trees (shallow trees).</p></li>
<li><p><strong>Gradient Boosting</strong>: Iteratively adds trees that minimize a loss function by following the negative gradient.</p></li>
<li><p><strong>Regularization</strong>: Incorporates L1 (Lasso) and L2 (Ridge) penalties to prevent overfitting.</p></li>
<li><p><strong>Optimization</strong>: Uses advanced techniques like second-order gradients (Hessian) and parallel processing.</p></li>
</ol>
</section>
<hr class="docutils" />
<section id="key-concepts-and-methods">
<h2><strong>Key Concepts and Methods</strong><a class="headerlink" href="#key-concepts-and-methods" title="Link to this heading">#</a></h2>
<section id="a-core-mechanics">
<h3><strong>a. Core Mechanics</strong><a class="headerlink" href="#a-core-mechanics" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Loss Function</strong>:</p>
<ul>
<li><p>Classification: Log-loss (binary/multiclass).</p></li>
<li><p>Regression: Mean Squared Error (MSE) or others (e.g., MAE).</p></li>
</ul>
</li>
<li><p><strong>Gradient and Hessian</strong>: Uses first (gradient) and second (Hessian) derivatives to optimize the loss.</p></li>
<li><p><strong>Tree Building</strong>: Adds trees by splitting based on gain, with regularization terms:
[
\text{Objective} = \sum \text{Loss}(y_i, \hat{y}_i) + \sum \Omega(f_k)
]
where (\Omega(f_k) = \gamma T + \frac{1}{2} \lambda |w|^2) (T = # leaves, w = leaf weights).</p></li>
</ul>
</section>
<section id="b-hyperparameters">
<h3><strong>b. Hyperparameters</strong><a class="headerlink" href="#b-hyperparameters" title="Link to this heading">#</a></h3>
<p>XGBoost‚Äôs performance can be optimized by tuning key parameters:</p>
<ul class="simple">
<li><p><strong>Learning Rate (<code class="docutils literal notranslate"><span class="pre">eta</span></code>)</strong>: Shrinks contribution of each tree (0.01 - 0.3).</p></li>
<li><p><strong>Max Depth (<code class="docutils literal notranslate"><span class="pre">max_depth</span></code>)</strong>: Controls tree complexity (3 - 10).</p></li>
<li><p><strong>Number of Estimators (<code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>)</strong>: Number of trees (50 - 1000).</p></li>
<li><p><strong>Regularization</strong>: <code class="docutils literal notranslate"><span class="pre">lambda</span></code> (L2), <code class="docutils literal notranslate"><span class="pre">alpha</span></code> (L1).</p></li>
<li><p><strong>Subsample</strong>: Fraction of data sampled per tree (0.5‚Äì1).</p></li>
<li><p><strong>Colsample_bytree</strong>: Fraction of features sampled per tree (0.5 - 1.0).</p></li>
</ul>
</section>
</section>
</section>
<hr class="docutils" />
<section id="examples-of-use-cases">
<h1>Examples of Use Cases<a class="headerlink" href="#examples-of-use-cases" title="Link to this heading">#</a></h1>
<section id="classification-predicting-breast-cancer">
<h2>1. Classification: Predicting Breast Cancer<a class="headerlink" href="#classification-predicting-breast-cancer" title="Link to this heading">#</a></h2>
<p>Using the <strong>Breast Cancer Wisconsin dataset</strong>, classify tumors as malignant or benign.</p>
<p>Let‚Äôs implement XGBoost for a classification task using the Breast Cancer dataset.</p>
<section id="id7">
<h3>Step 1: Import Libraries and Load Data<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import xgboost as xgb

# Load dataset
data = load_breast_cancer()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target  # 0 = malignant, 1 = benign

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-2-train-the-xgboost-model">
<h3>Step 2: Train the XGBoost Model<a class="headerlink" href="#step-2-train-the-xgboost-model" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Initialize and train the classifier
xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric=&#39;logloss&#39;)
xgb_clf.fit(X_train, y_train)
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">use_label_encoder=False</span></code>: Avoids a deprecation warning.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">eval_metric='logloss'</span></code>: Optimizes for binary classification.</p></li>
</ul>
</section>
<section id="step-3-make-predictions-and-evaluate">
<h3>Step 3: Make Predictions and Evaluate<a class="headerlink" href="#step-3-make-predictions-and-evaluate" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Predict on test set
y_pred = xgb_clf.predict(X_test)

# Evaluate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f&quot;Accuracy: {accuracy:.2f}&quot;)

# Detailed report
print(&quot;\nClassification Report:&quot;)
print(classification_report(y_test, y_pred, target_names=data.target_names))
</pre></div>
</div>
</div>
</div>
<p><strong>Sample Output:</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Accuracy: 0.97
Classification Report:
              precision    recall  f1-score   support
   malignant       0.98      0.95      0.96        43
      benign       0.97      0.99      0.98        71
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-4-feature-importance">
<h3>Step 4: Feature Importance<a class="headerlink" href="#step-4-feature-importance" title="Link to this heading">#</a></h3>
<p>Visualize the top features contributing to the model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Plot feature importance
xgb.plot_importance(xgb_clf, max_num_features=10)
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-5-hyperparameter-tuning-with-gridsearchcv">
<h3>Step 5: Hyperparameter Tuning with GridSearchCV<a class="headerlink" href="#step-5-hyperparameter-tuning-with-gridsearchcv" title="Link to this heading">#</a></h3>
<p>Optimize the model with a grid search:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from sklearn.model_selection import GridSearchCV

# Define parameter grid
param_grid = {
    &#39;learning_rate&#39;: [0.01, 0.1, 0.2],
    &#39;max_depth&#39;: [3, 5, 7],
    &#39;n_estimators&#39;: [50, 100, 200]
}

# Initialize GridSearchCV
grid_search = GridSearchCV(xgb.XGBClassifier(use_label_encoder=False, eval_metric=&#39;logloss&#39;),
                           param_grid, cv=5, scoring=&#39;accuracy&#39;)

# Fit the model
grid_search.fit(X_train, y_train)

# Results
print(f&quot;Best Parameters: {grid_search.best_params_}&quot;)
print(f&quot;Best CV Accuracy: {grid_search.best_score_:.2f}&quot;)

# Test with best model
best_xgb = grid_search.best_estimator_
y_pred_best = best_xgb.predict(X_test)
print(f&quot;Test Accuracy with Best Model: {accuracy_score(y_test, y_pred_best):.2f}&quot;)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="regression-forecasting-house-prices">
<h2>2. Regression: Forecasting House Prices<a class="headerlink" href="#regression-forecasting-house-prices" title="Link to this heading">#</a></h2>
<p>Using the <strong>Boston Housing dataset</strong>, predict house prices based on features like crime rate and room count.</p>
<p>Now, let‚Äôs predict house prices using the Boston Housing dataset.</p>
<section id="id8">
<h3>Code<a class="headerlink" href="#id8" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from sklearn.datasets import load_boston
from sklearn.metrics import mean_squared_error

# Load data
boston = load_boston()
X = pd.DataFrame(boston.data, columns=boston.feature_names)
y = boston.target

# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the regressor
xgb_reg = xgb.XGBRegressor(objective=&#39;reg:squarederror&#39;)
xgb_reg.fit(X_train, y_train)

# Predict and evaluate
y_pred = xgb_reg.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
print(f&quot;Mean Squared Error: {mse:.2f}&quot;)
</pre></div>
</div>
</div>
</div>
<p><strong>Sample Output:</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Mean Squared Error: 9.52
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="tools-and-methods-summary">
<h2><strong>4. Tools and Methods Summary</strong><a class="headerlink" href="#tools-and-methods-summary" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Modeling</strong>: <code class="docutils literal notranslate"><span class="pre">xgboost.XGBClassifier</span></code>, <code class="docutils literal notranslate"><span class="pre">XGBRegressor</span></code>.</p></li>
<li><p><strong>Evaluation</strong>: <code class="docutils literal notranslate"><span class="pre">sklearn.metrics.accuracy_score</span></code>, <code class="docutils literal notranslate"><span class="pre">mean_squared_error</span></code>.</p></li>
<li><p><strong>Tuning</strong>: <code class="docutils literal notranslate"><span class="pre">sklearn.model_selection.GridSearchCV</span></code>, early stopping.</p></li>
<li><p><strong>Visualization</strong>: <code class="docutils literal notranslate"><span class="pre">matplotlib.pyplot</span></code>, <code class="docutils literal notranslate"><span class="pre">xgboost.plot_importance</span></code>.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from sklearn.metrics import classification_report

# Detailed evaluation
print(&quot;Classification Report:\n&quot;, classification_report(y_test, y_pred))
xgb.plot_importance(xgb_clf)
plt.show()
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./contents\3_ml"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="21_clustering.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">K-Means Clustering</p>
      </div>
    </a>
    <a class="right-next"
       href="6_ml_lifecycle.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">ML: Life Cycle</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">5Ô∏è‚É£ Optimization &amp; Training</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-functions"><strong>Loss Functions</strong></a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-functions-in-machine-learning">Cost Functions in Machine Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-cost-functions">What Are Cost Functions?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#purpose-of-cost-functions">Purpose of Cost Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-does-the-model-use-cost-functions">How Does the Model Use Cost Functions?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#goal-of-model-training">Goal of Model Training</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#optimization-algorithms"><strong>Optimization Algorithms</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent">Gradient Descent</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-gradient-descent">What is Gradient Descent?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-foundation">Mathematical Foundation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-gradient-descent-works">How Gradient Descent Works</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-the-gradient-descent-process"><strong>Understanding the Gradient Descent Process</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-values"><strong>Example Values</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#steps"><strong>Steps:</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#implementation-approaches">Implementation Approaches</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-gradient-descent-approaches"><strong>Types of Gradient Descent Approaches</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fixed-step-approach-not-recommended"><strong>1Ô∏è‚É£ Fixed Step Approach (Not Recommended ‚ùå)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-approach-recommended"><strong>2Ô∏è‚É£ Learning Rate Approach (Recommended ‚úÖ)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-comparison">Learning Rate Comparison</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-gradient-descent">Types of Gradient Descent</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-gradient-descent-bgd">1. Batch Gradient Descent (BGD)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent-sgd">2. Stochastic Gradient Descent (SGD)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mini-batch-gradient-descent">3. Mini-Batch Gradient Descent</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-linear-regression">Example: Linear Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cost-function">Cost Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tools-and-libraries">Tools and Libraries</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#one-stop-solution-python-code-example">One-Stop Solution: Python Code Example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code">Code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-explanation">Code Explanation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#manual-gradient-descent">Manual Gradient Descent</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#scikit-learn-implementation">Scikit-learn Implementation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expected-output">Expected Output</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization"><strong>Regularization</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-and-ridge-regression">Lasso and Ridge Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-lasso-and-ridge-regression">Understanding Lasso and Ridge Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation-of-the-code-and-results">Explanation of the Code and Results</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#workflow">Workflow</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#expected-observations">Expected Observations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-tips">Practical Tips</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameter-tuning"><strong>Hyperparameter Tuning</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">Introduction</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-parameters">Model Parameters</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#model-hyperparameters">Model Hyperparameters</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-hyperparameter-tuning">What is Hyperparameter Tuning?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-is-hyperparameter-tuning-important">Why is Hyperparameter Tuning Important?</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-gridsearchcv">What is GridSearchCV?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-gridsearchcv-works">How GridSearchCV Works</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-parameters-of-gridsearchcv">Key Parameters of GridSearchCV</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tools-and-methods-for-hyperparameter-tuning">Tools and Methods for Hyperparameter Tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scikit-learn-tools">1. Scikit-learn Tools</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#advanced-libraries">2. Advanced Libraries</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-validation">3. Cross-Validation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scoring-metrics">4. Scoring Metrics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#parallelization">5. Parallelization</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-1-tuning-a-random-forest-with-gridsearchcv">Example 1: Tuning a Random Forest with GridSearchCV</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-import-libraries-and-load-data">Step 1: Import Libraries and Load Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-define-the-parameter-grid">Step 2: Define the Parameter Grid</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-initialize-and-run-gridsearchcv">Step 3: Initialize and Run GridSearchCV</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-analyze-the-results">Step 4: Analyze the Results</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-2-tuning-xgboost-with-gridsearchcv">Example 2: Tuning XGBoost with GridSearchCV</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Step 1: Import Libraries and Load Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Step 2: Define the Parameter Grid</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Step 3: Initialize and Run GridSearchCV</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Step 4: Analyze the Results</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#alternatives-to-gridsearchcv">Alternatives to GridSearchCV</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-tips-for-hyperparameter-tuning">Practical Tips for Hyperparameter Tuning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ways-to-tune-hyperparameters">Ways to Tune Hyperparameters</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approach-1-manual-tuning-using-train-test-split">üìå Approach 1: Manual Tuning using <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approach-2-k-fold-cross-validation">üìå Approach 2: K-Fold Cross Validation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approach-3-gridsearchcv-exhaustive-search">üìå Approach 3: GridSearchCV (Exhaustive Search)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approach-4-randomizedsearchcv-efficient-search">üìå Approach 4: RandomizedSearchCV (Efficient Search)</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#xgboost">XGBoost</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-xgboost"><strong>What is XGBoost?</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-xgboost"><strong>Why Use XGBoost?</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-it-works"><strong>How It Works</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts-and-methods"><strong>Key Concepts and Methods</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-core-mechanics"><strong>a. Core Mechanics</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#b-hyperparameters"><strong>b. Hyperparameters</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#examples-of-use-cases">Examples of Use Cases</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-predicting-breast-cancer">1. Classification: Predicting Breast Cancer</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Step 1: Import Libraries and Load Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-train-the-xgboost-model">Step 2: Train the XGBoost Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-make-predictions-and-evaluate">Step 3: Make Predictions and Evaluate</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-feature-importance">Step 4: Feature Importance</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-hyperparameter-tuning-with-gridsearchcv">Step 5: Hyperparameter Tuning with GridSearchCV</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-forecasting-house-prices">2. Regression: Forecasting House Prices</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Code</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tools-and-methods-summary"><strong>4. Tools and Methods Summary</strong></a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gajanesh
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright ¬© 2025 Gajanesh. All rights reserved..
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>