{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "395d01c5",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (K-NN) Algorithm\n",
    "\n",
    "The **K-Nearest Neighbors (K-NN)** algorithm is a simple yet powerful supervised machine learning technique used for both **classification** and **regression** tasks. It’s an **instance-based learning** algorithm, meaning it doesn’t build a model during training but instead memorizes the training data and makes predictions by comparing new data points to stored examples. \n",
    "\n",
    "## **What is the K-Nearest Neighbors (K-NN) Algorithm?**\n",
    "K-NN is a **non-parametric**, instance-based learning algorithm that classifies or predicts the value of a data point based on the majority class (for classification) or average value (for regression) of its \\(k\\) nearest neighbors in the feature space. \n",
    "It’s “lazy” because it doesn’t build a model during training—instead, it memorizes the training data and performs computations at prediction time.\n",
    "\n",
    "*Instance based learning*\n",
    "Instead of learning a fixed model (like a linear equation), it relies on the training data itself to make predictions. When a new data point is introduced, K-NN identifies the **K closest data points** (neighbors) from the training set and uses their labels or values to predict the outcome.\n",
    "\n",
    "## **Why Use K-NN?**\n",
    "- **Simplicity**: Intuitive and easy to implement.\n",
    "- **No Assumptions**: Doesn’t assume data distribution (non-parametric).\n",
    "- **Versatility**: Works for classification (e.g., spam detection) and regression (e.g., house price prediction).\n",
    "- **Adaptability**: Effective for small datasets and local patterns.\n",
    "\n",
    "### Key Characteristics\n",
    "- **Instance-Based Learning:** The algorithm compares new data points with existing data points based on similarity.\n",
    "- **Non-Parametric**: Makes no assumptions about data distribution\n",
    "- **Lazy Learner:** It does not learn from the training data immediately but stores it and uses it only when making a prediction.\n",
    "- **Distance-Based Classification:** It assigns a new data point to the class that is most common among its `K` nearest neighbors.\n",
    "\n",
    "---\n",
    "\n",
    "## **How It Works**\n",
    "1. **Training**: Store the training data (features \\(X\\) and labels \\(y\\)).\n",
    "\n",
    "1. **Choose the Number of Neighbors (K):**  \n",
    "   - Select a value for K, which determines how many neighbors influence the prediction.  \n",
    "   - Small K (e.g., 1) can lead to noisy, overfitted predictions, while large K smooths the decision boundary but may underfit.\n",
    "\n",
    "2. **Calculate Distances:**  \n",
    "   - For a new data point, compute its distance to all points in the training set.  \n",
    "   - Common distance metrics include:\n",
    "     - **Euclidean Distance:** √((x₁ - x₂)² + (y₁ - y₂)²)  \n",
    "     - **Manhattan Distance:** |x₁ - x₂| + |y₁ - y₂|  \n",
    "     - **Minkowski Distance:** A generalization of Euclidean and Manhattan distances.\n",
    "\n",
    "3. **Find the K Nearest Neighbors:**  \n",
    "   - Sort the distances and select the K points with the smallest distances.\n",
    "\n",
    "4. **Make a Prediction:**  \n",
    "   - **Classification:** Use majority voting among the K neighbors’ labels.  \n",
    "   - **Regression:** Compute the average (or weighted average) of the K neighbors’ values.\n",
    "\n",
    "## **Hyperparameters**\n",
    "- `n_neighbors` (\\(k\\)): Number of neighbors (small \\(k\\) = sensitive to noise, large \\(k\\) = smoother).\n",
    "- `weights`: Uniform (equal weight) or distance-based (closer neighbors weigh more).\n",
    "- `metric`: Distance measure (e.g., \"euclidean\", \"manhattan\").\n",
    "- `p`: Power parameter for Minkowski distance (p=2 for Euclidean, p=1 for Manhattan).\n",
    "\n",
    "---\n",
    "\n",
    "## Choosing the Optimal K Value\n",
    "### Key Considerations\n",
    "- **Small K**: High variance, low bias (risk of overfitting)\n",
    "- **Large K**: High bias, low variance (risk of underfitting)\n",
    "\n",
    "### Selection Methods\n",
    "1. **Elbow Method**: Plot accuracy vs K values\n",
    "2. **Cross-Validation**: Use k-fold validation to find optimal K\n",
    "3. **Square Root Rule**: K ≈ √n (n = training samples)\n",
    "\n",
    "---\n",
    "## Strengths of K-NN\n",
    "\n",
    "- **Simple and Intuitive:** Easy to understand and implement.\n",
    "- **No Training Phase:** Instantly usable since it stores data instead of building a model.\n",
    "- **Versatile:** Works for both classification and regression.\n",
    "- **Flexible:** Handles non-linear relationships well.\n",
    "\n",
    "## Weaknesses of K-NN\n",
    "\n",
    "- **Slow Predictions:** Computing distances for large datasets is time-consuming.\n",
    "- **Memory Intensive:** Must store the entire training set.\n",
    "- **Curse of Dimensionality:** Struggles in high-dimensional spaces where data becomes sparse.\n",
    "- **Sensitive to Noise:** Irrelevant or noisy features can distort distance calculations.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Practical Examples**\n",
    "\n",
    "### **Example 1: Classification (Iris Dataset)**\n",
    "\n",
    "Let’s illustrate K-NN with the **Iris dataset**, a classic dataset containing 150 samples of iris flowers with four features (sepal length, sepal width, petal length, petal width) and three classes (Setosa, Versicolor, Virginica). We’ll implement K-NN in Python using scikit-learn.\n",
    "\n",
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fc3731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b423ad",
   "metadata": {},
   "source": [
    "### Step 2: Load and Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37b47cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features (4 columns)\n",
    "y = iris.target  # Labels (0, 1, 2 for the three classes)\n",
    "\n",
    "# Split into training and testing sets (70% train, 30% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357e10f1",
   "metadata": {},
   "source": [
    "### Step 3: Choose K and Train the Model\n",
    "\n",
    "Let’s start with **K=3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12181baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the K-NN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# Train the model (stores the training data)\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abe50fd",
   "metadata": {},
   "source": [
    "### Step 4: Make Predictions and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d07104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict labels for the test set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Display confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7679ad",
   "metadata": {},
   "source": [
    "**Sample Output:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be2a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "Accuracy: 0.98\n",
    "Confusion Matrix:\n",
    " [[19  0  0]\n",
    "  [ 0 13  0]\n",
    "  [ 0  1 12]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d56ef2",
   "metadata": {},
   "source": [
    "- Accuracy of 98% means the model correctly classified 44 out of 45 test samples.\n",
    "- The confusion matrix shows one misclassification (a Virginica flower predicted as Versicolor).\n",
    "\n",
    "---\n",
    "\n",
    "## Tools and Methods for K-NN\n",
    "\n",
    "To optimize K-NN, you can leverage several tools and techniques:\n",
    "\n",
    "### 1. Feature Scaling\n",
    "Since K-NN relies on distance, features with different scales (e.g., one in meters, another in kilometers) can skew results. Use **StandardScaler** or **MinMaxScaler** to normalize data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb77a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Retrain with scaled data\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = knn.predict(X_test_scaled)\n",
    "print(f\"Accuracy with Scaling: {accuracy_score(y_test, y_pred_scaled):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9aee41",
   "metadata": {},
   "source": [
    "### 2. Choosing the Optimal K\n",
    "Test different K values to find the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcefbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test K from 1 to 20\n",
    "accuracies = []\n",
    "for k in range(1, 21):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    y_pred = knn.predict(X_test_scaled)\n",
    "    accuracies.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# Plot accuracy vs. K\n",
    "plt.plot(range(1, 21), accuracies, marker='o')\n",
    "plt.xlabel('Number of Neighbors (K)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs. K')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e72d03e",
   "metadata": {},
   "source": [
    "This plot helps identify the K value that maximizes accuracy.\n",
    "\n",
    "### 3. Distance Metrics\n",
    "Experiment with different metrics (e.g., `'euclidean'`, `'manhattan'`) by setting the `metric` parameter in `KNeighborsClassifier`.\n",
    "\n",
    "### 4. Weighted Voting\n",
    "Give closer neighbors more influence by setting `weights='distance'` (default is `'uniform'`, where all neighbors have equal weight)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9057f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_weighted = KNeighborsClassifier(n_neighbors=3, weights='distance')\n",
    "knn_weighted.fit(X_train_scaled, y_train)\n",
    "print(f\"Accuracy with Weighted Voting: {accuracy_score(y_test, knn_weighted.predict(X_test_scaled)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b69ad9",
   "metadata": {},
   "source": [
    "### 5. Hyperparameter Tuning with GridSearchCV\n",
    "Automate the search for the best K, weights, and metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b370a866",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'n_neighbors': range(1, 21),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'metric': ['euclidean', 'manhattan']\n",
    "}\n",
    "\n",
    "# Initialize and run GridSearchCV\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
    "grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Results\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "# Test accuracy with best model\n",
    "best_knn = grid_search.best_estimator_\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, best_knn.predict(X_test_scaled)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c356854",
   "metadata": {},
   "source": [
    "### 6. Cross-Validation\n",
    "Evaluate model stability across data splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18d3918",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "scores = cross_val_score(knn, X_train_scaled, y_train, cv=5)\n",
    "print(f\"Cross-Validation Scores: {scores}\")\n",
    "print(f\"Average CV Score: {scores.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa122077",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **Example 2: Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9afe73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Synthetic regression data\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = np.sin(X) + np.random.normal(0, 0.1, X.shape)\n",
    "\n",
    "# Fit K-NN regression\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=5, weights=\"distance\")\n",
    "knn_reg.fit(X, y.ravel())\n",
    "\n",
    "# Predict\n",
    "y_pred = knn_reg.predict(X)\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "print(\"MSE:\", mse)\n",
    "\n",
    "# Visualize\n",
    "plt.scatter(X, y, label=\"Data\", alpha=0.5)\n",
    "plt.plot(X, y_pred, color=\"red\", label=\"K-NN Fit\")\n",
    "plt.title(\"K-NN Regression (k=5)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1635dc",
   "metadata": {},
   "source": [
    "---\n",
    "## **4. Tools and Methods Summary**\n",
    "- **Modeling**: `sklearn.neighbors.KNeighborsClassifier`, `KNeighborsRegressor`.\n",
    "- **Evaluation**: `sklearn.metrics.accuracy_score`, `mean_squared_error`.\n",
    "- **Tuning**: `sklearn.model_selection.GridSearchCV`.\n",
    "- **Visualization**: `matplotlib.pyplot.contourf()`, `seaborn.scatterplot()`.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
