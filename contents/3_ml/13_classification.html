
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Classification Algorithms &#8212; Data Science Books</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-dropdown.css?v=995e94df" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-bootstrap.min.css?v=21c0b90a" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=d567e03f" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'contents/3_ml/13_classification';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="ML: Types of Models" href="10_core_algo.html" />
    <link rel="prev" title="Regression Algorithms" href="12_regression.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Data Science Books</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Part I ‚Äî Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../0_maths/0_essential.html">Essential Mathematics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_maths/4_linear_algebra.html">Linear Algebra</a></li>







<li class="toctree-l1"><a class="reference internal" href="../0_maths/2_probability.html">Probability Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_maths/1_descriptive.html">Descriptive Statistics</a></li>








<li class="toctree-l1"><a class="reference internal" href="../0_maths/3_inferential.html">Inferential Statistics</a></li>



<li class="toctree-l1"><a class="reference internal" href="../0_maths/5_calculus.html">Calculus</a></li>







<li class="toctree-l1"><a class="reference internal" href="../0_maths/6_regression_analysis.html">Explanatory and Response Variables</a></li>


<li class="toctree-l1"><a class="reference internal" href="../1_python/1_basics.html">Python Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/2_advanced.html">Advanced Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/3_data_structures.html">Data Structures</a></li>





<li class="toctree-l1"><a class="reference internal" href="../1_python/4_modules_packages.html">Modules &amp; Packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/5_functions.html">Functions &amp; Modular Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/6_oop.html">Object-Oriented Programming</a></li>





<li class="toctree-l1"><a class="reference internal" href="../1_python/8_exceptions.html">Exception Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/9_regex.html">Regular Expressions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_numpy/1_numpy.html">NumPy (<strong>Numerical Python</strong>)</a></li>



<li class="toctree-l1"><a class="reference internal" href="../2_pandas/1_series.html">Pandas Series</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_pandas/2_dataframes.html">Pandas DataFrame</a></li>
















<li class="toctree-l1"><a class="reference internal" href="../2_pandas/3_visualization.html"><strong>What is Data Visualization in Data Science?</strong></a></li>


<li class="toctree-l1"><a class="reference internal" href="../2_pandas/4_eda.html">Exploratory Data Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_pandas/5_feature_engineering.html"><strong>What is Feature Engineering?</strong></a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part II ‚Äî Classical Machine Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="1_foundations.html">ML Foundational</a></li>





<li class="toctree-l1"><a class="reference internal" href="2_data_preparation.html">2Ô∏è‚É£ Data Handling</a></li>





<li class="toctree-l1"><a class="reference internal" href="2_train_test_split.html">Train‚ÄìTest Split</a></li>
<li class="toctree-l1"><a class="reference internal" href="5_model_evaluation.html">4Ô∏è‚É£ Model Evaluation</a></li>




<li class="toctree-l1"><a class="reference internal" href="11_supervised_learning.html"><strong>Supervised Learning</strong></a></li>



<li class="toctree-l1"><a class="reference internal" href="12_regression.html">Regression Algorithms</a></li>







<li class="toctree-l1 current active"><a class="current reference internal" href="#">Classification Algorithms</a></li>

<li class="toctree-l1"><a class="reference internal" href="10_core_algo.html">Core ML Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_decision_trees.html">Decision Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_ensemble_methods.html">Ensemble Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_svm.html">Support Vector Machine (SVM) in Detail</a></li>


<li class="toctree-l1"><a class="reference internal" href="17_knn.html">k-Nearest Neighbors</a></li>
<li class="toctree-l1"><a class="reference internal" href="18_naive_bayes.html">Naive Bayes</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part III ‚Äî Advanced Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="20_unsupervised_learning.html">Unsupervised Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="21_clustering.html">Clustering Techniques</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="7_optimization_and_training.html">5Ô∏è‚É£ Optimization &amp; Training</a></li>







<li class="toctree-l1 has-children"><a class="reference internal" href="6_ml_lifecycle.html">ML Lifecycle</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="6_training.html">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="6_evaluation.html">Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="6_deployment.html">Deployment</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part IV ‚Äî Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl1_Introduction.html">Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl2_Neuron.html">Neuron</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl3_Libraries.html">Deep Learning Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl4_Terminology.html">Terminology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl5_multi_layer.html">Multi-Layer Perceptron</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl6_first_nn.html">First Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl7_evaluating_model.html">Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl8_multiclass_classification.html">Multiclass Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl9_multiclass_classification_hand.html">Handwritten Digit Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl10_saving_and_loading.html">Saving &amp; Loading Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl11_checkpointing.html">Model Checkpointing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl12_visualizing_model_training.html"><strong>Visualizing Model Training History in Deep Learning</strong></a></li>

<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl13_loss_functions_activation_functions_and_optimizers.html">Loss Functions &amp; Optimizers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part V ‚Äî NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp1.html">NLP Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp2.html">Text Cleaning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp3.html">Text Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp4.html">NLP Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp5.html">Bag of Words, TF-IDF</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp6.html">Sentiment Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp7.html">NLP with SpaCy</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part VI ‚Äî Career &amp; MLOps</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../6_interview/self%20introduction.html">Self Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6_interview/py.html">Python: Interview Guide</a></li>





<li class="toctree-l1"><a class="reference internal" href="../6_interview/pd.html">üìö Pandas: Interview Guide</a></li>


<li class="toctree-l1"><a class="reference internal" href="../6_interview/ml.html">Machine Learning: Interview Guide</a></li>













<li class="toctree-l1"><a class="reference internal" href="../6_interview/git.html">Git</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6_interview/dvc.html">DVC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6_interview/mlflow.html">MLflow</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/coding-fame/Data-science-books" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/coding-fame/Data-science-books/edit/main/contents/3_ml/13_classification.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/coding-fame/Data-science-books/issues/new?title=Issue%20on%20page%20%2Fcontents/3_ml/13_classification.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/contents/3_ml/13_classification.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Classification Algorithms</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Classification Algorithms</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#ml-logistic-regression">üéØ ML: Logistic Regression</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-logistic-regression"><strong>What is Logistic Regression?</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-logistic-regression"><strong>Why Use Logistic Regression?</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-formulation"><strong>Mathematical Formulation</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-sigmoid-function">Logistic (Sigmoid) Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-logistic-regression">Types of Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-for-binary-classification">Logistic Regression for Binary Classification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumptions-of-logistic-regression">Assumptions of Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-for-multiclass">Logistic Regression for Multiclass</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-it"><strong>Why Use It?</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approaches"><strong>Approaches</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts-and-methods"><strong>Key Concepts and Methods</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-vs-rest-ovr"><strong>1. One-vs-Rest (OvR)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multinomial-logistic-regression-softmax-approach">2. Multinomial Logistic Regression (Softmax Approach)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-features-of-logisticregression-in-scikit-learn">Key Features of <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> in scikit-learn</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-logistic-regression-on-the-breast-cancer-dataset">Example: Logistic Regression on the Breast Cancer Dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#steps">Steps:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#complete-python-code-solution">Complete Python Code Solution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation-of-the-code">Explanation of the Code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization-in-logistic-regression">Regularization in Logistic Regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-multiclass-logistic-regression-with-the-iris-dataset">Example: Multiclass Logistic Regression with the Iris Dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-by-step-workflow">Step-by-Step Workflow</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Explanation of the Code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-output">Sample Output</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-considerations">Additional Considerations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-scaling">1. Feature Scaling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization-tuning">2. Regularization Tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#alternative-libraries">3. Alternative Libraries</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-boundaries">4. Decision Boundaries</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tools-and-methods-summary"><strong>4. Tools and Methods Summary</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion"><strong>Conclusion</strong></a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="classification-algorithms">
<h1>Classification Algorithms<a class="headerlink" href="#classification-algorithms" title="Link to this heading">#</a></h1>
<ul class="simple">
<li><p><strong>Logistic Regression</strong>:</p>
<ul>
<li><p>Predicts probabilities using the logistic function (sigmoid). Outputs class labels via thresholds (e.g., 0.5).</p></li>
</ul>
</li>
<li><p><strong>Support Vector Machines (SVM)</strong>:</p>
<ul>
<li><p>Finds the optimal hyperplane to separate classes with the maximum margin.</p></li>
</ul>
</li>
<li><p><strong>k-Nearest Neighbors (k-NN)</strong>:</p>
<ul>
<li><p>Assigns a class based on the majority vote of the ( k ) closest training examples.</p></li>
</ul>
</li>
<li><p><strong>Naive Bayes</strong>:</p>
<ul>
<li><p>Uses Bayes‚Äô theorem with the ‚Äúnaive‚Äù assumption of feature independence.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="ml-logistic-regression">
<h1>üéØ ML: Logistic Regression<a class="headerlink" href="#ml-logistic-regression" title="Link to this heading">#</a></h1>
<p>Logistic regression is a supervised learning technique used to solve classification problems. It predicts a categorical dependent variable using a given set of independent variables.</p>
<p>Despite its name, it is not a regression algorithm but a classification technique that models the probability of a binary response based on one or more predictor variables.</p>
<hr class="docutils" />
<section id="what-is-logistic-regression">
<h2><strong>What is Logistic Regression?</strong><a class="headerlink" href="#what-is-logistic-regression" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Logistic regression is a statistical method used for classification, despite its name suggesting regression.</p></li>
<li><p>It predicts the probability that a given input belongs to a particular class, making it ideal for binary outcomes (e.g., spam vs. not spam) and extendable to multiclass problems.</p></li>
<li><p>It models assumes the relationship (linear) between features and the log-odds of the probability of the outcome using the logistic (<strong>sigmoid</strong>) function.</p></li>
</ul>
</section>
<section id="why-use-logistic-regression">
<h2><strong>Why Use Logistic Regression?</strong><a class="headerlink" href="#why-use-logistic-regression" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Probability Output</strong>: Predicts probabilities, not just class labels.</p></li>
<li><p><strong>Interpretability</strong>: Coefficients reveal feature importance.</p></li>
<li><p><strong>Efficiency</strong>: Works well with linearly separable data and small-to-medium datasets.</p></li>
<li><p><strong>Foundation</strong>: Basis for understanding more complex models (e.g., neural networks).</p></li>
</ul>
</section>
<section id="mathematical-formulation">
<h2><strong>Mathematical Formulation</strong><a class="headerlink" href="#mathematical-formulation" title="Link to this heading">#</a></h2>
<p>For binary classification:</p>
<ol class="arabic simple">
<li><p><strong>Linear Combination</strong>: ( z = w_0 + w_1x_1 + w_2x_2 + \cdots + w_nx_n ), where (w_i) are weights and (x_i) are features.</p></li>
<li><p><strong>Sigmoid Function</strong></p></li>
<li><p><strong>Decision Rule</strong>: Predict class 1 if (P(y=1|X) &gt; 0.5), else class 0.</p></li>
</ol>
<section id="logistic-sigmoid-function">
<h3>Logistic (Sigmoid) Function<a class="headerlink" href="#logistic-sigmoid-function" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Formula</strong>: <code class="docutils literal notranslate"><span class="pre">P(y=1|X)</span> <span class="pre">=</span> <span class="pre">œÉ(z)</span> <span class="pre">=</span> <span class="pre">1</span> <span class="pre">/</span> <span class="pre">(1</span> <span class="pre">+</span> <span class="pre">e^(-z))</span></code></p></li>
<li><p>The logistic (sigmoid) function maps any real value to a range between 0 and 1.</p></li>
<li><p>The output probability helps determine whether the predicted class is 0 or 1 using a threshold value.</p></li>
<li><p><strong>Threshold Concept</strong>:</p>
<ul>
<li><p><strong>If output ‚â• 0.5</strong>, classify as <strong>1</strong> (Will buy insurance).</p></li>
<li><p><strong>If output &lt; 0.5</strong>, classify as <strong>0</strong> (Will not buy insurance).</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="types-of-logistic-regression">
<h2>Types of Logistic Regression<a class="headerlink" href="#types-of-logistic-regression" title="Link to this heading">#</a></h2>
<p>üõ†Ô∏è Binary vs Multiclass Showdown</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Aspect</p></th>
<th class="head"><p>Binary Classification</p></th>
<th class="head"><p>Multiclass</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Classes</strong></p></td>
<td><p>2</p></td>
<td><p>3+</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Activation</strong></p></td>
<td><p>Sigmoid</p></td>
<td><p>Softmax</p></td>
</tr>
<tr class="row-even"><td><p><strong>Output</strong></p></td>
<td><p>Single Probability</p></td>
<td><p>Probability Distribution</p></td>
</tr>
<tr class="row-odd"><td><p><strong>SKLearn Parameter</strong></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">multi_class='ovr'</span></code></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">multi_class='multinomial'</span></code></p></td>
</tr>
<tr class="row-even"><td><p><strong>Example</strong></p></td>
<td><p>Spam Detection üìß</p></td>
<td><p>Animal Classification üêïüêàüêÑ</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Decision Boundary</strong></p></td>
<td><p>Single threshold (0.5)</p></td>
<td><p>Multiple probability checks</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<hr class="docutils" />
<section id="logistic-regression-for-binary-classification">
<h2>Logistic Regression for Binary Classification<a class="headerlink" href="#logistic-regression-for-binary-classification" title="Link to this heading">#</a></h2>
<ul>
<li><p><strong>Model</strong>: Logistic regression uses the <strong>sigmoid function</strong> to model the probability that an input belongs to the positive class (e.g., class 1):</p>
<div class="highlight-math notranslate"><div class="highlight"><pre><span></span>   P(y=1 | x) = 1 /(1 + e^-(w^T x + b))
</pre></div>
</div>
<p>Here, (w) is the weight vector, (x) is the input feature vector, and (b) is the bias term. The output is a value between 0 and 1, interpreted as a probability.</p>
</li>
<li><p><strong>Decision Rule</strong>: If (P(y=1 | x) &gt; 0.5), predict class 1; otherwise, predict class 0.</p></li>
<li><p><strong>Training</strong>: The model optimizes the weights (w) and bias (b) by minimizing a loss function, typically the <strong>binary cross-entropy</strong>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="assumptions-of-logistic-regression">
<h2>Assumptions of Logistic Regression<a class="headerlink" href="#assumptions-of-logistic-regression" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>Linear Relationship</strong>: There is a linear relationship between the log-odds of the dependent variable and the independent variables, use polynomial features for non-linearity.</p></li>
<li><p><strong>Independence of Observations</strong>: Each observation is independent of the others.</p></li>
<li><p><strong>No Multicollinearity</strong>: The independent variables should not be highly correlated with each other.</p></li>
<li><p><strong>Large Sample Size</strong>: Logistic Regression performs better with larger datasets.</p></li>
</ol>
</section>
<hr class="docutils" />
<section id="logistic-regression-for-multiclass">
<h2>Logistic Regression for Multiclass<a class="headerlink" href="#logistic-regression-for-multiclass" title="Link to this heading">#</a></h2>
<p>Multiclass logistic regression generalizes binary logistic regression to classify data into three or more categories (e.g., classifying flowers into setosa, versicolor, or virginica). It predicts the probability of each class and assigns the instance to the class with the highest probability.</p>
<section id="why-use-it">
<h3><strong>Why Use It?</strong><a class="headerlink" href="#why-use-it" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Probability Estimates</strong>: Provides probabilities for all classes.</p></li>
<li><p><strong>Simplicity</strong>: Extends linear methods to multiclass without needing complex models.</p></li>
<li><p><strong>Foundation</strong>: Underpins neural networks and softmax layers in deep learning.</p></li>
</ul>
</section>
<section id="approaches">
<h3><strong>Approaches</strong><a class="headerlink" href="#approaches" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>One-vs-Rest (OvR) / One-vs-All (OvA)</strong>: Trains a separate binary classifier for each class against all others.</p></li>
<li><p><strong>Softmax Regression (Multinomial)</strong>: Models all classes jointly using the softmax function, optimizing a single loss function.</p></li>
</ol>
</section>
</section>
<section id="key-concepts-and-methods">
<h2><strong>Key Concepts and Methods</strong><a class="headerlink" href="#key-concepts-and-methods" title="Link to this heading">#</a></h2>
<section id="one-vs-rest-ovr">
<h3><strong>1. One-vs-Rest (OvR)</strong><a class="headerlink" href="#one-vs-rest-ovr" title="Link to this heading">#</a></h3>
<p>The <strong>One-vs-Rest</strong> (also called One-vs-All) method breaks down a multiclass problem into multiple binary classification tasks.</p>
<ul class="simple">
<li><p><strong>How It Works</strong>:</p>
<ul>
<li><p>For (k) classes, train (k) separate binary logistic regression models.</p></li>
<li><p>Each model distinguishes one class (positive) from all other classes combined (negative).</p></li>
<li><p>Example: For three classes (A, B, C):</p>
<ol class="arabic simple">
<li><p>Model 1: A vs. (B + C)</p></li>
<li><p>Model 2: B vs. (A + C)</p></li>
<li><p>Model 3: C vs. (A + B)</p></li>
</ol>
</li>
</ul>
</li>
<li><p><strong>Prediction</strong>: Choose the class with the highest probability across all classifiers.</p></li>
<li><p><strong>Loss</strong>: Binary cross-entropy for each classifier.</p></li>
</ul>
</section>
<section id="multinomial-logistic-regression-softmax-approach">
<h3>2. Multinomial Logistic Regression (Softmax Approach)<a class="headerlink" href="#multinomial-logistic-regression-softmax-approach" title="Link to this heading">#</a></h3>
<p>The <strong>Multinomial</strong> or <strong>Softmax</strong> approach models all classes simultaneously using a single model.</p>
<ul>
<li><p><strong>How It Works</strong>:</p></li>
<li><p>Instead of the sigmoid function, it uses the <strong>softmax function</strong> to compute probabilities across all (k) classes.</p>
<ul>
<li><p>For an input (x) and class (j), the probability is:</p>
<div class="highlight-math notranslate"><div class="highlight"><pre><span></span>P(y=j | x) = \frac{e^{w_j^T x + b_j}}{\sum_{m=1}^{k} e^{w_m^T x + b_m}}
</pre></div>
</div>
</li>
</ul>
</li>
<li><p>(w_j) and (b_j) are the weight vector and bias for class (j).</p></li>
<li><p>The denominator normalizes the probabilities so they sum to 1.</p></li>
<li><p><strong>Prediction</strong>:</p>
<ul class="simple">
<li><p>The predicted class is the one with the highest probability.</p></li>
</ul>
</li>
<li><p><strong>Training</strong>:</p>
<ul class="simple">
<li><p>The model optimizes a single set of weights and biases for all classes by minimizing the <strong>cross-entropy loss</strong> (a generalization of binary cross-entropy).</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="key-features-of-logisticregression-in-scikit-learn">
<h2>Key Features of <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> in scikit-learn<a class="headerlink" href="#key-features-of-logisticregression-in-scikit-learn" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong><code class="docutils literal notranslate"><span class="pre">multi_class</span></code> Parameter</strong>:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">'ovr'</span></code>: Uses the One-vs-Rest approach.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'multinomial'</span></code>: Uses the Softmax approach.</p></li>
</ul>
</li>
<li><p><strong><code class="docutils literal notranslate"><span class="pre">solver</span></code> Parameter</strong>:</p>
<ul>
<li><p>Optimization algorithms to fit the model:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">'lbfgs'</span></code>: Suitable for small to medium datasets; supports multinomial.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'newton-cg'</span></code>: Newton‚Äôs method; supports multinomial.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'sag'</span></code> and <code class="docutils literal notranslate"><span class="pre">'saga'</span></code>: Stochastic gradient-based; supports multinomial, good for large datasets.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'liblinear'</span></code>: Only supports OvR, not multinomial.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Regularization</strong>:</p>
<ul>
<li><p>Controlled by the <code class="docutils literal notranslate"><span class="pre">C</span></code> parameter (inverse of regularization strength). Smaller <code class="docutils literal notranslate"><span class="pre">C</span></code> means stronger regularization to prevent overfitting.</p></li>
</ul>
</li>
<li><p><strong>Preprocessing</strong>:</p>
<ul>
<li><p>Feature scaling (e.g., <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code>) is recommended to standardize features, aiding convergence.</p></li>
</ul>
</li>
<li><p><strong>Evaluation Metrics</strong>:</p>
<ul>
<li><p>Accuracy, confusion matrix, log loss, and cross-validation scores.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="example-logistic-regression-on-the-breast-cancer-dataset">
<h2>Example: Logistic Regression on the Breast Cancer Dataset<a class="headerlink" href="#example-logistic-regression-on-the-breast-cancer-dataset" title="Link to this heading">#</a></h2>
<p>To illustrate Logistic Regression, we will use the <strong>Breast Cancer Wisconsin dataset</strong>, a binary classification problem where the goal is to predict whether a tumor is malignant (1) or benign (0) based on various features.</p>
<section id="steps">
<h3>Steps:<a class="headerlink" href="#steps" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Load and preprocess the dataset</strong>.</p></li>
<li><p><strong>Split the data</strong> into training and testing sets.</p></li>
<li><p><strong>Train the Logistic Regression model</strong>.</p></li>
<li><p><strong>Evaluate the model</strong> using metrics like accuracy, precision, recall, and the confusion matrix.</p></li>
<li><p><strong>Visualize the decision boundary</strong> (for a simplified 2D case).</p></li>
</ol>
</section>
<hr class="docutils" />
<section id="complete-python-code-solution">
<h3>Complete Python Code Solution<a class="headerlink" href="#complete-python-code-solution" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import necessary libraries</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">pandas</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">pd</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_breast_cancer</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">classification_report</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.decomposition</span><span class="w"> </span><span class="kn">import</span> <span class="n">PCA</span>

<span class="c1"># Step 1: Load the Breast Cancer dataset</span>
<span class="n">data</span> <span class="o">=</span> <span class="n">load_breast_cancer</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">data</span>  <span class="c1"># Features</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">target</span>  <span class="c1"># Target (0: malignant, 1: benign)</span>

<span class="c1"># Step 2: Preprocess the data (standardization)</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Step 3: Split the data into training and testing sets (80% train, 20% test)</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Step 4: Train the Logistic Regression model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>  <span class="c1"># Increase max_iter if convergence issues</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Step 5: Make predictions on the test set</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># Step 6: Evaluate the model</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">precision</span> <span class="o">=</span> <span class="n">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">recall</span> <span class="o">=</span> <span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="n">conf_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model Evaluation Metrics:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Precision: </span><span class="si">{</span><span class="n">precision</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Recall: </span><span class="si">{</span><span class="n">recall</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Classification Report:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">classification_report</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">target_names</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">target_names</span><span class="p">))</span>

<span class="c1"># Step 7: Visualize the decision boundary (using PCA for dimensionality reduction to 2D)</span>
<span class="n">pca</span> <span class="o">=</span> <span class="n">PCA</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">X_pca</span> <span class="o">=</span> <span class="n">pca</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_scaled</span><span class="p">)</span>

<span class="c1"># Train Logistic Regression on the 2D PCA-transformed data</span>
<span class="n">model_pca</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
<span class="n">model_pca</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_pca</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Create a meshgrid for plotting the decision boundary</span>
<span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">))</span>

<span class="c1"># Predict on the meshgrid points</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">model_pca</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Plot the decision boundary</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;coolwarm&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_pca</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;coolwarm&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Logistic Regression Decision Boundary (PCA-reduced Data)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;PCA Component 1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;PCA Component 2&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="explanation-of-the-code">
<h3>Explanation of the Code<a class="headerlink" href="#explanation-of-the-code" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Loading the Dataset</strong>:</p>
<ul class="simple">
<li><p>The Breast Cancer dataset is loaded using <code class="docutils literal notranslate"><span class="pre">load_breast_cancer()</span></code> from Scikit-learn. It contains 569 samples with 30 features.</p></li>
</ul>
</li>
<li><p><strong>Preprocessing</strong>:</p>
<ul class="simple">
<li><p>The features are standardized using <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code> to ensure they have a mean of 0 and a standard deviation of 1, which is important for Logistic Regression.</p></li>
</ul>
</li>
<li><p><strong>Splitting the Data</strong>:</p>
<ul class="simple">
<li><p>The dataset is split into 80% training and 20% testing sets using <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code>.</p></li>
</ul>
</li>
<li><p><strong>Training the Model</strong>:</p>
<ul class="simple">
<li><p>A Logistic Regression model is trained on the training data with <code class="docutils literal notranslate"><span class="pre">LogisticRegression(max_iter=1000)</span></code> to ensure convergence.</p></li>
</ul>
</li>
<li><p><strong>Making Predictions</strong>:</p>
<ul class="simple">
<li><p>The model predicts the labels for the test set using <code class="docutils literal notranslate"><span class="pre">model.predict()</span></code>.</p></li>
</ul>
</li>
<li><p><strong>Evaluating the Model</strong>:</p>
<ul class="simple">
<li><p><strong>Accuracy</strong>: The proportion of correct predictions.</p></li>
<li><p><strong>Precision</strong>: The proportion of true positives among predicted positives.</p></li>
<li><p><strong>Recall</strong>: The proportion of true positives among actual positives.</p></li>
<li><p><strong>Confusion Matrix</strong>: A table showing true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN).</p></li>
<li><p><strong>Classification Report</strong>: A summary of precision, recall, and F1-score for both classes.</p></li>
</ul>
</li>
<li><p><strong>Visualizing the Decision Boundary</strong>:</p>
<ul class="simple">
<li><p>Since the dataset has 30 features, we use <strong>PCA</strong> (Principal Component Analysis) to reduce it to 2 dimensions for visualization.</p></li>
<li><p>The decision boundary is plotted using a meshgrid, showing how the model separates the two classes in the reduced feature space.</p></li>
</ul>
</li>
</ol>
</section>
<hr class="docutils" />
<section id="regularization-in-logistic-regression">
<h3>Regularization in Logistic Regression<a class="headerlink" href="#regularization-in-logistic-regression" title="Link to this heading">#</a></h3>
<p>Logistic Regression in Scikit-learn supports <strong>regularization</strong> to prevent overfitting, especially when dealing with high-dimensional data. Regularization adds a penalty term to the cost function, discouraging overly complex models.</p>
<ul class="simple">
<li><p><strong>L1 Regularization (Lasso)</strong>: Encourages sparsity in the coefficients (some coefficients become zero).</p></li>
<li><p><strong>L2 Regularization (Ridge)</strong>: Shrinks coefficients but keeps all features.</p></li>
</ul>
<p>In the code, you can specify the regularization type using the <code class="docutils literal notranslate"><span class="pre">penalty</span></code> parameter:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">penalty='l2'</span></code>: Uses L2 regularization (default).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">C</span></code>: Inverse of regularization strength (smaller values mean stronger regularization).</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="example-multiclass-logistic-regression-with-the-iris-dataset">
<h2>Example: Multiclass Logistic Regression with the Iris Dataset<a class="headerlink" href="#example-multiclass-logistic-regression-with-the-iris-dataset" title="Link to this heading">#</a></h2>
<p>Let‚Äôs implement a complete workflow using the Iris dataset, which contains 150 samples of flowers classified into three species (setosa, versicolor, virginica) based on four features (sepal length, sepal width, petal length, petal width).</p>
<section id="step-by-step-workflow">
<h3>Step-by-Step Workflow<a class="headerlink" href="#step-by-step-workflow" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Load Data</strong></p></li>
<li><p><strong>Preprocess (Split and Scale)</strong></p></li>
<li><p><strong>Train the Model</strong></p></li>
<li><p><strong>Evaluate Performance</strong></p></li>
<li><p><strong>Visualize Decision Boundaries</strong></p></li>
</ol>
<p>Here‚Äôs the complete Python code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import libraries</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.datasets</span><span class="w"> </span><span class="kn">import</span> <span class="n">load_iris</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">cross_val_score</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.linear_model</span><span class="w"> </span><span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.metrics</span><span class="w"> </span><span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">log_loss</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.preprocessing</span><span class="w"> </span><span class="kn">import</span> <span class="n">StandardScaler</span>

<span class="c1"># 1. Load the Iris dataset</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span>  <span class="c1"># Features: [sepal length, sepal width, petal length, petal width]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>  <span class="c1"># Classes: [0, 1, 2] for setosa, versicolor, virginica</span>

<span class="c1"># 2. Preprocess the data</span>
<span class="c1"># Split into training and testing sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Scale the features</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">X_train_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span>
<span class="n">X_test_scaled</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># # OvR Model</span>
<span class="c1"># model_ovr = LogisticRegression(multi_class=&quot;ovr&quot;, solver=&quot;liblinear&quot;)</span>
<span class="c1"># model_ovr.fit(X_train, y_train)</span>
<span class="c1"># y_pred_ovr = model_ovr.predict(X_test)</span>
<span class="c1"># print(&quot;OvR Accuracy:&quot;, accuracy_score(y_test, y_pred_ovr))</span>

<span class="c1"># 3. Train the model (Softmax approach)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;multinomial&#39;</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># 4. Make predictions and evaluate</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test_scaled</span><span class="p">)</span>  <span class="c1"># Probability for each class</span>

<span class="c1"># Accuracy</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Accuracy: </span><span class="si">{</span><span class="n">accuracy</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Confusion Matrix</span>
<span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Confusion Matrix:&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">cm</span><span class="p">)</span>

<span class="c1"># Log Loss</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">log_loss</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">probs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Log Loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Cross-validation (5-fold)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cross-validation accuracies: </span><span class="si">{</span><span class="n">scores</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean accuracy: </span><span class="si">{</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 5. Visualize decision boundary (using petal length and petal width)</span>
<span class="n">X_two</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">:</span><span class="mi">4</span><span class="p">]</span>  <span class="c1"># Select petal length and petal width</span>
<span class="n">X_train_two</span><span class="p">,</span> <span class="n">X_test_two</span><span class="p">,</span> <span class="n">y_train_two</span><span class="p">,</span> <span class="n">y_test_two</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_two</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># Train a model on two features</span>
<span class="n">model_two</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;multinomial&#39;</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">)</span>
<span class="n">model_two</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_two</span><span class="p">,</span> <span class="n">y_train_two</span><span class="p">)</span>

<span class="c1"># Create a meshgrid for plotting</span>
<span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X_two</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X_two</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X_two</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="n">X_two</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span>
<span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">))</span>

<span class="c1"># Predict on the meshgrid</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">model_two</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="c1"># Plot decision boundary and training points</span>
<span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_train_two</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">X_train_two</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">y_train_two</span><span class="p">,</span> <span class="n">edgecolors</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="s1">&#39;viridis&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Petal Length (cm)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Petal Width (cm)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Decision Boundary for Logistic Regression (Iris Dataset)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Additional Insights: Feature Coefficients</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model Coefficients (per class):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">coef_</span><span class="p">)</span>  <span class="c1"># Shape: (n_classes, n_features)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Intercepts (per class):&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">intercept_</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="id1">
<h3>Explanation of the Code<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Loading Data</strong>:</p>
<ul class="simple">
<li><p>The Iris dataset is loaded using <code class="docutils literal notranslate"><span class="pre">load_iris()</span></code>. Features are stored in <code class="docutils literal notranslate"><span class="pre">X</span></code> (150x4), and labels in <code class="docutils literal notranslate"><span class="pre">y</span></code> (150,).</p></li>
</ul>
</li>
<li><p><strong>Preprocessing</strong>:</p>
<ul class="simple">
<li><p><strong>Splitting</strong>: 80% training, 20% testing.</p></li>
<li><p><strong>Scaling</strong>: <code class="docutils literal notranslate"><span class="pre">StandardScaler</span></code> standardizes features to have zero mean and unit variance, improving model convergence.</p></li>
</ul>
</li>
<li><p><strong>Training</strong>:</p>
<ul class="simple">
<li><p>The <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> model uses <code class="docutils literal notranslate"><span class="pre">multi_class='multinomial'</span></code> for the Softmax approach and <code class="docutils literal notranslate"><span class="pre">solver='lbfgs'</span></code> for optimization.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">C=1.0</span></code> controls regularization strength (can be tuned via cross-validation).</p></li>
</ul>
</li>
<li><p><strong>Evaluation</strong>:</p>
<ul class="simple">
<li><p><strong>Accuracy</strong>: Proportion of correct predictions.</p></li>
<li><p><strong>Confusion Matrix</strong>: Shows true vs. predicted labels for each class.</p></li>
<li><p><strong>Log Loss</strong>: Measures the quality of probability predictions (lower is better).</p></li>
<li><p><strong>Cross-Validation</strong>: 5-fold CV provides a robust performance estimate.</p></li>
</ul>
</li>
<li><p><strong>Visualization</strong>:</p>
<ul class="simple">
<li><p>Uses petal length and width (features 2 and 3) to plot decision boundaries in 2D.</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">contourf</span></code> plot shows decision regions, and <code class="docutils literal notranslate"><span class="pre">scatter</span></code> overlays training points.</p></li>
</ul>
</li>
<li><p><strong>Coefficients</strong>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">model.coef_</span></code> (3x4 matrix) shows the weight of each feature for each class.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">model.intercept_</span></code> (array of length 3) gives the bias for each class.</p></li>
</ul>
</li>
</ol>
</section>
<section id="sample-output">
<h3>Sample Output<a class="headerlink" href="#sample-output" title="Link to this heading">#</a></h3>
<p>Running this code might yield:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Accuracy</span><span class="p">:</span> <span class="mf">0.9667</span>
<span class="n">Confusion</span> <span class="n">Matrix</span><span class="p">:</span>
<span class="p">[[</span><span class="mi">10</span>  <span class="mi">0</span>  <span class="mi">0</span><span class="p">]</span>
 <span class="p">[</span> <span class="mi">0</span>  <span class="mi">9</span>  <span class="mi">1</span><span class="p">]</span>
 <span class="p">[</span> <span class="mi">0</span>  <span class="mi">0</span> <span class="mi">10</span><span class="p">]]</span>
<span class="n">Log</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.1352</span>
<span class="n">Cross</span><span class="o">-</span><span class="n">validation</span> <span class="n">accuracies</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.9667</span> <span class="mf">0.9667</span> <span class="mf">0.9333</span> <span class="mf">0.9333</span> <span class="mf">1.0000</span><span class="p">]</span>
<span class="n">Mean</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">0.9600</span>
</pre></div>
</div>
<ul class="simple">
<li><p>The model achieves ~96% accuracy.</p></li>
<li><p>The confusion matrix shows minimal misclassifications.</p></li>
<li><p>A plot displays linear decision boundaries separating the three classes.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="additional-considerations">
<h2>Additional Considerations<a class="headerlink" href="#additional-considerations" title="Link to this heading">#</a></h2>
<section id="feature-scaling">
<h3>1. Feature Scaling<a class="headerlink" href="#feature-scaling" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Scaling is crucial when features have different ranges (e.g., sepal length vs. petal width). Without scaling, features with larger ranges might dominate the model.</p></li>
</ul>
</section>
<section id="regularization-tuning">
<h3>2. Regularization Tuning<a class="headerlink" href="#regularization-tuning" title="Link to this heading">#</a></h3>
<ul>
<li><p>Use <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code> to tune <code class="docutils literal notranslate"><span class="pre">C</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.model_selection</span><span class="w"> </span><span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="n">param_grid</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">]}</span>
<span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">multi_class</span><span class="o">=</span><span class="s1">&#39;multinomial&#39;</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s1">&#39;lbfgs&#39;</span><span class="p">),</span> <span class="n">param_grid</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train_scaled</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best C: </span><span class="si">{</span><span class="n">grid</span><span class="o">.</span><span class="n">best_params_</span><span class="p">[</span><span class="s1">&#39;C&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="alternative-libraries">
<h3>3. Alternative Libraries<a class="headerlink" href="#alternative-libraries" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>TensorFlow/PyTorch</strong>: For custom implementations or integration with neural networks.</p>
<ul>
<li><p>Example: Define a softmax layer with categorical cross-entropy loss.</p></li>
</ul>
</li>
<li><p>Scikit-learn is preferred for simplicity and standard tasks.</p></li>
</ul>
</section>
<section id="decision-boundaries">
<h3>4. Decision Boundaries<a class="headerlink" href="#decision-boundaries" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Logistic regression produces <strong>linear decision boundaries</strong>. For non-linear problems, consider kernel methods or other classifiers (e.g., SVM, neural networks).</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="tools-and-methods-summary">
<h2><strong>4. Tools and Methods Summary</strong><a class="headerlink" href="#tools-and-methods-summary" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Modeling</strong>: <code class="docutils literal notranslate"><span class="pre">sklearn.linear_model.LogisticRegression</span></code>.</p></li>
<li><p><strong>Manual</strong>: <code class="docutils literal notranslate"><span class="pre">numpy</span></code> for sigmoid, gradient descent.</p></li>
<li><p><strong>Evaluation</strong>: <code class="docutils literal notranslate"><span class="pre">sklearn.metrics.accuracy_score</span></code>, <code class="docutils literal notranslate"><span class="pre">log_loss</span></code>, <code class="docutils literal notranslate"><span class="pre">confusion_matrix</span></code>.</p></li>
<li><p><strong>Visualization</strong>: <code class="docutils literal notranslate"><span class="pre">matplotlib.pyplot.contourf()</span></code>, <code class="docutils literal notranslate"><span class="pre">seaborn.scatterplot()</span></code>.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="conclusion">
<h2><strong>Conclusion</strong><a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>Logistic regression is a versatile, interpretable algorithm for classification, bridging linear models and probabilistic outputs via the sigmoid function.</p>
<hr class="docutils" />
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./contents\3_ml"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="12_regression.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Regression Algorithms</p>
      </div>
    </a>
    <a class="right-next"
       href="10_core_algo.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">ML: Types of Models</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Classification Algorithms</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#ml-logistic-regression">üéØ ML: Logistic Regression</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-logistic-regression"><strong>What is Logistic Regression?</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-logistic-regression"><strong>Why Use Logistic Regression?</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-formulation"><strong>Mathematical Formulation</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-sigmoid-function">Logistic (Sigmoid) Function</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-logistic-regression">Types of Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-for-binary-classification">Logistic Regression for Binary Classification</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#assumptions-of-logistic-regression">Assumptions of Logistic Regression</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logistic-regression-for-multiclass">Logistic Regression for Multiclass</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-it"><strong>Why Use It?</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#approaches"><strong>Approaches</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts-and-methods"><strong>Key Concepts and Methods</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#one-vs-rest-ovr"><strong>1. One-vs-Rest (OvR)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multinomial-logistic-regression-softmax-approach">2. Multinomial Logistic Regression (Softmax Approach)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-features-of-logisticregression-in-scikit-learn">Key Features of <code class="docutils literal notranslate"><span class="pre">LogisticRegression</span></code> in scikit-learn</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-logistic-regression-on-the-breast-cancer-dataset">Example: Logistic Regression on the Breast Cancer Dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#steps">Steps:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#complete-python-code-solution">Complete Python Code Solution</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation-of-the-code">Explanation of the Code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization-in-logistic-regression">Regularization in Logistic Regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-multiclass-logistic-regression-with-the-iris-dataset">Example: Multiclass Logistic Regression with the Iris Dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-by-step-workflow">Step-by-Step Workflow</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Explanation of the Code</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-output">Sample Output</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-considerations">Additional Considerations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-scaling">1. Feature Scaling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization-tuning">2. Regularization Tuning</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#alternative-libraries">3. Alternative Libraries</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#decision-boundaries">4. Decision Boundaries</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tools-and-methods-summary"><strong>4. Tools and Methods Summary</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion"><strong>Conclusion</strong></a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gajanesh
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright ¬© 2025 Gajanesh. All rights reserved..
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>