
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>4Ô∏è‚É£ Model Evaluation &#8212; Data Science Books</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-dropdown.css?v=995e94df" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-bootstrap.min.css?v=21c0b90a" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=d567e03f" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'contents/3_ml/5_model_evaluation';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Supervised Learning" href="11_supervised_learning.html" />
    <link rel="prev" title="Train &amp; Test Datasets in Python" href="2_train_test_split.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Data Science Books</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Part I ‚Äî Foundations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../0_maths/0_essential.html">Essential Mathematics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_maths/4_linear_algebra.html">Linear Algebra</a></li>







<li class="toctree-l1"><a class="reference internal" href="../0_maths/2_probability.html">Probability Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="../0_maths/1_descriptive.html">Descriptive Statistics</a></li>








<li class="toctree-l1"><a class="reference internal" href="../0_maths/3_inferential.html">Inferential Statistics</a></li>



<li class="toctree-l1"><a class="reference internal" href="../0_maths/5_calculus.html">Calculus</a></li>







<li class="toctree-l1"><a class="reference internal" href="../0_maths/6_regression_analysis.html">Explanatory and Response Variables</a></li>


<li class="toctree-l1"><a class="reference internal" href="../1_python/1_basics.html">Python Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/2_advanced.html">Advanced Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/3_data_structures.html">Data Structures</a></li>





<li class="toctree-l1"><a class="reference internal" href="../1_python/4_modules_packages.html">Modules &amp; Packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/5_functions.html">Functions &amp; Modular Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/6_oop.html">Object-Oriented Programming</a></li>





<li class="toctree-l1"><a class="reference internal" href="../1_python/8_exceptions.html">Exception Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/9_regex.html">Regular Expressions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_numpy/1_numpy.html">NumPy (<strong>Numerical Python</strong>)</a></li>



<li class="toctree-l1"><a class="reference internal" href="../2_pandas/1_series.html">Pandas Series</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_pandas/2_dataframes.html">Pandas DataFrame</a></li>
















<li class="toctree-l1"><a class="reference internal" href="../2_pandas/3_visualization.html"><strong>What is Data Visualization in Data Science?</strong></a></li>


<li class="toctree-l1"><a class="reference internal" href="../2_pandas/4_eda.html">Exploratory Data Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_pandas/5_feature_engineering.html"><strong>What is Feature Engineering?</strong></a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part II ‚Äî Classical Machine Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="1_foundations.html">ML Foundational</a></li>





<li class="toctree-l1"><a class="reference internal" href="2_data_preparation.html">2Ô∏è‚É£ Data Handling</a></li>





<li class="toctree-l1"><a class="reference internal" href="2_train_test_split.html">Train‚ÄìTest Split</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">4Ô∏è‚É£ Model Evaluation</a></li>




<li class="toctree-l1"><a class="reference internal" href="11_supervised_learning.html"><strong>Supervised Learning</strong></a></li>



<li class="toctree-l1"><a class="reference internal" href="12_regression.html">Regression Algorithms</a></li>







<li class="toctree-l1"><a class="reference internal" href="13_classification.html">Classification Algorithms</a></li>

<li class="toctree-l1"><a class="reference internal" href="10_core_algo.html">Core ML Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="14_decision_trees.html">Decision Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="15_ensemble_methods.html">Ensemble Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="16_svm.html">Support Vector Machine (SVM) in Detail</a></li>


<li class="toctree-l1"><a class="reference internal" href="17_knn.html">k-Nearest Neighbors</a></li>
<li class="toctree-l1"><a class="reference internal" href="18_naive_bayes.html">Naive Bayes</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part III ‚Äî Advanced Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="20_unsupervised_learning.html">Unsupervised Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="21_clustering.html">Clustering Techniques</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="7_optimization_and_training.html">5Ô∏è‚É£ Optimization &amp; Training</a></li>







<li class="toctree-l1 has-children"><a class="reference internal" href="6_ml_lifecycle.html">ML Lifecycle</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="6_training.html">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="6_evaluation.html">Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="6_deployment.html">Deployment</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part IV ‚Äî Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl1_Introduction.html">Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl2_Neuron.html">Neuron</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl3_Libraries.html">Deep Learning Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl4_Terminology.html">Terminology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl5_multi_layer.html">Multi-Layer Perceptron</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl6_first_nn.html">First Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl7_evaluating_model.html">Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl8_multiclass_classification.html">Multiclass Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl9_multiclass_classification_hand.html">Handwritten Digit Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl10_saving_and_loading.html">Saving &amp; Loading Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl11_checkpointing.html">Model Checkpointing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl12_visualizing_model_training.html"><strong>Visualizing Model Training History in Deep Learning</strong></a></li>

<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl13_loss_functions_activation_functions_and_optimizers.html">Loss Functions &amp; Optimizers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part V ‚Äî NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp1.html">NLP Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp2.html">Text Cleaning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp3.html">Text Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp4.html">NLP Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp5.html">Bag of Words, TF-IDF</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp6.html">Sentiment Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp7.html">NLP with SpaCy</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part VI ‚Äî Career &amp; MLOps</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../6_interview/self%20introduction.html">Self Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6_interview/py.html">Python: Interview Guide</a></li>





<li class="toctree-l1"><a class="reference internal" href="../6_interview/pd.html">üìö Pandas: Interview Guide</a></li>


<li class="toctree-l1"><a class="reference internal" href="../6_interview/ml.html">Machine Learning: Interview Guide</a></li>













<li class="toctree-l1"><a class="reference internal" href="../6_interview/git.html">Git</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6_interview/dvc.html">DVC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6_interview/mlflow.html">MLflow</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/coding-fame/Data-science-books" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/coding-fame/Data-science-books/edit/main/contents/3_ml/5_model_evaluation.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/coding-fame/Data-science-books/issues/new?title=Issue%20on%20page%20%2Fcontents/3_ml/5_model_evaluation.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/contents/3_ml/5_model_evaluation.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>4Ô∏è‚É£ Model Evaluation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">4Ô∏è‚É£ Model Evaluation</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-metrics"><strong>Regression Metrics</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distance-based-error-explained">Distance-Based Error Explained</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error-mse">1. Mean Squared Error (MSE)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-absolute-error-mae">2. Mean Absolute Error (MAE)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#python-implementation">Python Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#root-mean-squared-error-rmse">3. Root Mean Squared Error (RMSE)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Python Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tools-and-methods">Tools and Methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#one-stop-solution-python-code-example">One-Stop Solution: Python Code Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-takeaways">Key Takeaways</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-metrics"><strong>Classification Metrics</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-cross-entropy-log-loss">1. Binary Cross-Entropy (Log Loss)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-cross-entropy">2. Categorical Cross-Entropy</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-use"><strong>When to Use</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example"><strong>Example</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sparse-categorical-cross-entropy">3. Sparse Categorical Cross-Entropy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hinge-loss">4. Hinge Loss</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Tools and Methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">One-Stop Solution: Python Code Example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-explanation">Code Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-output-approximate">Sample Output (Approximate)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering-metrics"><strong>Clustering Metrics</strong></a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-matrix">Confusion Matrix</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-confusion-matrix"><strong>What is a Confusion Matrix?</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-a-confusion-matrix"><strong>Why Use a Confusion Matrix?</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#structure"><strong>Structure</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-a-confusion-matrix">Example of a Confusion Matrix</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics-derived-from-the-confusion-matrix">Metrics Derived from the Confusion Matrix</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#components"><strong>Components</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy"><strong>1. Accuracy</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#precision"><strong>2. Precision</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recall-sensitivity"><strong>3. Recall (Sensitivity)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#f1-score"><strong>4. F1 Score</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#specificity"><strong>5. Specificity</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">One-Stop Solution: Python Code Example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Code Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-output">Sample Output</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-matrix-plot">Confusion Matrix Plot</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-report">Classification Report</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#auc-and-roc-curve"><strong>AUC and ROC Curve</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-example"><strong>Code Example</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tools-and-methods-summary"><strong>Tools and Methods Summary</strong></a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="model-evaluation">
<h1>4Ô∏è‚É£ Model Evaluation<a class="headerlink" href="#model-evaluation" title="Link to this heading">#</a></h1>
<p><strong>Model Evaluation Metrics</strong></p>
</section>
<hr class="docutils" />
<section id="regression-metrics">
<h1><strong>Regression Metrics</strong><a class="headerlink" href="#regression-metrics" title="Link to this heading">#</a></h1>
<ul class="simple">
<li><p><strong>Mean Squared Error (MSE)</strong>: Average of squared errors.</p></li>
<li><p><strong>Root Mean Squared Error (RMSE)</strong>: (\sqrt{MSE}).</p></li>
<li><p><strong>Mean Absolute Error (MAE)</strong>: Average of absolute errors.</p></li>
<li><p><strong>R¬≤ (R-Squared)</strong>: Proportion of variance explained by the model (0 to 1).</p></li>
</ul>
<p>Common Cost Functions for Regression
In regression tasks, models predict continuous numerical values (e.g., house prices, temperatures). <strong>Cost functions</strong> quantify prediction accuracy by measuring the ‚Äúdistance‚Äù between predicted and actual values.</p>
<p>‚úÖ <strong>Purpose of Cost Functions:</strong></p>
<ul class="simple">
<li><p>Guide model training by providing feedback on prediction errors.</p></li>
<li><p>Enable optimization algorithms (e.g., Gradient Descent) to adjust model weights.</p></li>
</ul>
<section id="distance-based-error-explained">
<h2>Distance-Based Error Explained<a class="headerlink" href="#distance-based-error-explained" title="Link to this heading">#</a></h2>
<p>For each data point:</p>
<ul class="simple">
<li><p><strong>Actual Value</strong>: $y$</p></li>
<li><p><strong>Predicted Value</strong>: $y‚Äô$</p></li>
<li><p><strong>Error</strong>: $y - y‚Äô$</p></li>
</ul>
<p>Cost functions aggregate these errors across the dataset to measure overall model performance.</p>
</section>
<section id="mean-squared-error-mse">
<h2>1. Mean Squared Error (MSE)<a class="headerlink" href="#mean-squared-error-mse" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>MSE is the most widely used cost function in regression.</p></li>
<li><p>MSE is calculated as the <strong>average of the squared differences</strong> between predicted (y`) and actual (y) target values.</p></li>
<li><p>Squaring the errors amplifies larger discrepancies, making MSE sensitive to outliers.</p></li>
</ul>
<p><strong>Formula</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>MSE = (1/n) * Œ£ (yi - yi&#39;)¬≤
</pre></div>
</div>
</div>
</div>
<p>Where:</p>
<ul class="simple">
<li><p><strong>n</strong> = Number of data points</p></li>
<li><p><strong>yi</strong> = Actual target value</p></li>
<li><p><strong>yi‚Äô</strong> = Predicted value</p></li>
</ul>
<p><strong>When to Use</strong>:</p>
<ul class="simple">
<li><p>When large errors should be penalized more heavily.</p></li>
<li><p>Ideal for regression tasks where errors are assumed to be normally distributed.</p></li>
</ul>
<p><strong>Example</strong>:<br />
Suppose we‚Äôre predicting house prices. If a model predicts $300,000 for a house that actually costs $350,000, the squared error is:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>(350,000 - 300,000)¬≤ = 2,500,000,000
``` 
Averaging such errors across all predictions yields the MSE.

### Python Implementation  
```python
from sklearn.metrics import mean_squared_error

# Sample data: Actual vs. Predicted values
expected = [1.0] * 11
predicted = [1.0, 0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1, 0.0]

mse = mean_squared_error(expected, predicted)
print(f&quot;MSE: {mse:.2f}&quot;)  # Output: MSE: 0.35
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="mean-absolute-error-mae">
<h2>2. Mean Absolute Error (MAE)<a class="headerlink" href="#mean-absolute-error-mae" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>MAE is calculated as the <strong>average absolute difference</strong> between predicted and actual values.</p></li>
<li><p>Unlike MSE, it doesn‚Äôt square errors, making it less sensitive to outliers.</p></li>
</ul>
<p><strong>Formula</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>MAE = (1/n) * Œ£ |yi - yi&#39;|
</pre></div>
</div>
</div>
</div>
<p><strong>When to Use</strong>:</p>
<ul class="simple">
<li><p>When robustness to outliers is desired.</p></li>
<li><p>Suitable when all errors should be weighted equally, regardless of magnitude.</p></li>
</ul>
<p><strong>Example</strong>:<br />
Using the same house price scenario:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>|350,000 - 300,000| = 50,000
</pre></div>
</div>
</div>
</div>
<p>MAE averages these absolute differences, providing a simple measure of average error magnitude.</p>
<section id="python-implementation">
<h3>Python Implementation<a class="headerlink" href="#python-implementation" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from sklearn.metrics import mean_absolute_error

mae = mean_absolute_error(expected, predicted)
print(f&quot;MAE: {mae:.2f}&quot;)  # Output: MAE: 0.50
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="root-mean-squared-error-rmse">
<h2>3. Root Mean Squared Error (RMSE)<a class="headerlink" href="#root-mean-squared-error-rmse" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>RMSE is the square root of MSE, returning the error to the same units as the target variable.</p></li>
<li><p>It retains MSE‚Äôs sensitivity to larger errors but is more interpretable.</p></li>
</ul>
<p><strong>Formula</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>RMSE = sqrt(MSE)
</pre></div>
</div>
</div>
</div>
<p><strong>When to Use</strong>:</p>
<ul class="simple">
<li><p>When you need an error metric in the same units as the target variable.</p></li>
<li><p>Often used to report model performance in an intuitive way.</p></li>
</ul>
<p><strong>Example</strong>:<br />
If MSE is 2,500,000,000 (from the earlier example), then:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>RMSE = sqrt(2,500,000,000) = 50,000
</pre></div>
</div>
</div>
</div>
<p>This indicates an average error of $50,000 in house price predictions.</p>
<section id="id1">
<h3>Python Implementation<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>rmse = mean_squared_error(expected, predicted, squared=False)
print(f&quot;RMSE: {rmse:.2f}&quot;)  # Output: RMSE: 0.59
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="tools-and-methods">
<h2>Tools and Methods<a class="headerlink" href="#tools-and-methods" title="Link to this heading">#</a></h2>
<p>Several Python libraries simplify the implementation of these cost functions:</p>
<ul class="simple">
<li><p><strong>Scikit-learn</strong>: Provides <code class="docutils literal notranslate"><span class="pre">mean_squared_error</span></code>, <code class="docutils literal notranslate"><span class="pre">mean_absolute_error</span></code>, and supports custom extensions.</p></li>
<li><p><strong>TensorFlow</strong>: Offers <code class="docutils literal notranslate"><span class="pre">tf.keras.losses</span></code> with <code class="docutils literal notranslate"><span class="pre">MeanSquaredError</span></code>, <code class="docutils literal notranslate"><span class="pre">MeanAbsoluteError</span></code>, <code class="docutils literal notranslate"><span class="pre">Huber</span></code>, etc.</p></li>
<li><p><strong>PyTorch</strong>: Includes <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> modules like <code class="docutils literal notranslate"><span class="pre">MSELoss</span></code>, <code class="docutils literal notranslate"><span class="pre">L1Loss</span></code> (MAE), and <code class="docutils literal notranslate"><span class="pre">SmoothL1Loss</span></code> (Huber-like).</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="one-stop-solution-python-code-example">
<h2>One-Stop Solution: Python Code Example<a class="headerlink" href="#one-stop-solution-python-code-example" title="Link to this heading">#</a></h2>
<p>This script uses the <strong>California Housing dataset</strong> to train a linear regression model and compute MSE, MAE, RMSE, Huber Loss, and Log-Cosh Loss.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np
import tensorflow as tf
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

# Load the California Housing dataset
data = fetch_california_housing()
X, y = data.data, data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a linear regression model
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# 1. Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred)
print(f&quot;Mean Squared Error (MSE): {mse:.4f}&quot;)

# 2. Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred)
print(f&quot;Mean Absolute Error (MAE): {mae:.4f}&quot;)

# 3. Root Mean Squared Error (RMSE)
rmse = np.sqrt(mse)
print(f&quot;Root Mean Squared Error (RMSE): {rmse:.4f}&quot;)

# 4. Huber Loss (using TensorFlow)
huber = tf.keras.losses.Huber(delta=1.0)
huber_loss = huber(y_test, y_pred).numpy()
print(f&quot;Huber Loss (delta=1.0): {huber_loss:.4f}&quot;)

# 5. Log-Cosh Loss (manual implementation)
log_cosh_loss = np.mean(np.log(np.cosh(y_test - y_pred)))
print(f&quot;Log-Cosh Loss: {log_cosh_loss:.4f}&quot;)
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="key-takeaways">
<h2>Key Takeaways<a class="headerlink" href="#key-takeaways" title="Link to this heading">#</a></h2>
<p>1Ô∏è‚É£ <strong>MSE</strong>: Use when large errors must be flagged (e.g., fraud detection).<br />
2Ô∏è‚É£ <strong>RMSE</strong>: Default choice for model comparison and reporting.<br />
3Ô∏è‚É£ <strong>MAE</strong>: Best for noisy datasets or when outliers should not dominate error calculations.</p>
<p>üîç <strong>Pro Tip</strong>: Always visualize residuals (prediction errors) to understand error distribution and metric suitability!</p>
</section>
</section>
<hr class="docutils" />
<section id="classification-metrics">
<h1><strong>Classification Metrics</strong><a class="headerlink" href="#classification-metrics" title="Link to this heading">#</a></h1>
<ul class="simple">
<li><p><strong>Accuracy</strong>: (Correct Predictions) / (Total Predictions).</p></li>
<li><p><strong>Precision</strong>: (True Positives) / (True Positives + False Positives).</p></li>
<li><p><strong>Recall</strong>: (True Positives) / (True Positives + False Negatives).</p></li>
<li><p><strong>F1-Score</strong>: Harmonic mean of precision and recall.</p></li>
<li><p><strong>ROC-AUC</strong>: Area under the ROC curve (measures class separation).</p></li>
<li><p><strong>Confusion Matrix</strong>: Table showing true vs. predicted classes.</p></li>
</ul>
<hr class="docutils" />
<p>Common Cost Functions for Classification</p>
<section id="binary-cross-entropy-log-loss">
<h2>1. Binary Cross-Entropy (Log Loss)<a class="headerlink" href="#binary-cross-entropy-log-loss" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Binary cross-entropy measures the difference between predicted probabilities and true binary labels (0 or 1).</p></li>
<li><p>It‚Äôs widely used in binary classification tasks and penalizes predictions more heavily as they deviate from the true label.</p></li>
</ul>
<p><strong>Formula:</strong><br />
For a single sample:<br />
BCE = - [y * log(≈∑) + (1 - y) * log(1 - ≈∑)]</p>
<p>For a dataset with <em>n</em> samples:<br />
BCE = - (1/n) * Œ£ [y·µ¢ * log(≈∑·µ¢) + (1 - y·µ¢) * log(1 - ≈∑·µ¢)], for i = 1 to n</p>
<p>Where:</p>
<ul class="simple">
<li><p><em>y·µ¢</em> = True label (0 or 1)</p></li>
<li><p><em>≈∑·µ¢</em> = Predicted probability for the positive class (between 0 and 1)</p></li>
</ul>
<p><strong>When to Use:</strong></p>
<ul class="simple">
<li><p>Binary classification problems (e.g., spam vs. not spam).</p></li>
<li><p>Models that output probabilities (e.g., logistic regression).</p></li>
</ul>
<p><strong>Example:</strong></p>
<ul class="simple">
<li><p>If <em>y = 1</em> and <em>≈∑ = 0.9</em>:<br />
BCE = - [1 * log(0.9) + (1 - 1) * log(1 - 0.9)]<br />
= - log(0.9) ‚âà 0.105</p></li>
<li><p>If <em>y = 1</em> and <em>≈∑ = 0.1</em>:<br />
BCE = - log(0.1) ‚âà 2.303</p></li>
</ul>
<p>The loss increases significantly for poor predictions.</p>
</section>
<hr class="docutils" />
<section id="categorical-cross-entropy">
<h2>2. Categorical Cross-Entropy<a class="headerlink" href="#categorical-cross-entropy" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Categorical cross-entropy extends binary cross-entropy to multi-class classification.</p></li>
<li><p>It compares the true label distribution (typically one-hot encoded) with the predicted probability distribution across all classes.</p></li>
</ul>
<p><strong>Formula:</strong></p>
<p>For a single sample with C classes:<br />
<code class="docutils literal notranslate"><span class="pre">CCE</span> <span class="pre">=</span> <span class="pre">-</span> <span class="pre">Œ£</span> <span class="pre">(y_c</span> <span class="pre">*</span> <span class="pre">log(yÃÇ_c))</span></code></p>
<p>For a dataset:<br />
<code class="docutils literal notranslate"><span class="pre">CCE</span> <span class="pre">=</span> <span class="pre">-</span> <span class="pre">(1/n)</span> <span class="pre">*</span> <span class="pre">Œ£</span> <span class="pre">Œ£</span> <span class="pre">(y_i,c</span> <span class="pre">*</span> <span class="pre">log(yÃÇ_i,c))</span></code></p>
<p>Where:</p>
<ul class="simple">
<li><p>y_i,c = 1 if the sample belongs to class c, otherwise 0.</p></li>
<li><p>yÃÇ_i,c = Predicted probability for class c.</p></li>
</ul>
<hr class="docutils" />
<section id="when-to-use">
<h3><strong>When to Use</strong><a class="headerlink" href="#when-to-use" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Multi-class classification (e.g., classifying images into 10 digit categories).</p></li>
<li><p>Models that output probability distributions (e.g., softmax output).</p></li>
</ul>
</section>
<section id="example">
<h3><strong>Example</strong><a class="headerlink" href="#example" title="Link to this heading">#</a></h3>
<p>For a 3-class problem:</p>
<ul class="simple">
<li><p>True label = class 2 (one-hot: [0, 1, 0])</p></li>
<li><p>Predicted probabilities = [0.1, 0.7, 0.2]</p></li>
</ul>
<p>Calculation:<br />
<code class="docutils literal notranslate"><span class="pre">CCE</span> <span class="pre">=</span> <span class="pre">-</span> <span class="pre">(0</span> <span class="pre">*</span> <span class="pre">log(0.1)</span> <span class="pre">+</span> <span class="pre">1</span> <span class="pre">*</span> <span class="pre">log(0.7)</span> <span class="pre">+</span> <span class="pre">0</span> <span class="pre">*</span> <span class="pre">log(0.2))</span></code><br />
<code class="docutils literal notranslate"><span class="pre">CCE</span> <span class="pre">=</span> <span class="pre">-</span> <span class="pre">log(0.7)</span> <span class="pre">‚âà</span> <span class="pre">0.357</span></code></p>
</section>
</section>
<hr class="docutils" />
<section id="sparse-categorical-cross-entropy">
<h2>3. Sparse Categorical Cross-Entropy<a class="headerlink" href="#sparse-categorical-cross-entropy" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Similar to categorical cross-entropy, but designed for integer labels instead of one-hot encoded vectors.</p></li>
<li><p>It‚Äôs more memory-efficient, especially with many classes.</p></li>
</ul>
<p><strong>Formula</strong>:<br />
For a single sample:<br />
Sparse CCE = - log(≈∑_y)</p>
<p>Where:</p>
<ul class="simple">
<li><p>y = True class index</p></li>
<li><p>≈∑_y = Predicted probability for the true class</p></li>
</ul>
<p><strong>When to Use</strong></p>
<ul class="simple">
<li><p>Multi-class classification with integer labels</p></li>
<li><p>Large number of classes where one-hot encoding is impractical</p></li>
</ul>
<p><strong>Example</strong><br />
True label = class 2 (index 2)<br />
Predicted probabilities = [0.1, 0.7, 0.2]</p>
<p>Calculation:<br />
<code class="docutils literal notranslate"><span class="pre">Sparse</span> <span class="pre">CCE</span> <span class="pre">=</span> <span class="pre">-</span> <span class="pre">log(0.7)</span> <span class="pre">‚âà</span> <span class="pre">0.357</span></code></p>
</section>
<hr class="docutils" />
<section id="hinge-loss">
<h2>4. Hinge Loss<a class="headerlink" href="#hinge-loss" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p>Hinge loss is used in Support Vector Machines (SVMs) for binary classification.</p></li>
<li><p>It encourages correct classification with a margin, penalizing predictions that are correct but too close to the decision boundary.</p></li>
</ul>
<p>For a single sample:<br />
Hinge = max(0, 1 - y * ≈∑)</p>
<p>Where:</p>
<ul class="simple">
<li><p>y = True label (-1 or 1)</p></li>
<li><p>≈∑ = Predicted score (not a probability)</p></li>
</ul>
<p><strong>When to Use</strong></p>
<ul class="simple">
<li><p>SVM-based classification</p></li>
<li><p>Models outputting decision scores rather than probabilities</p></li>
</ul>
<p><strong>Example</strong></p>
<ol class="arabic simple">
<li><p>If y = 1, ≈∑ = 0.8:<br />
Hinge = max(0, 1 - (1 * 0.8)) = max(0, 0.2) = <strong>0.2</strong></p></li>
<li><p>If y = 1, ≈∑ = -0.5:<br />
Hinge = max(0, 1 - (1 * -0.5)) = max(0, 1.5) = <strong>1.5</strong></p></li>
</ol>
</section>
<hr class="docutils" />
<section id="id2">
<h2>Tools and Methods<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<p>Python libraries provide built-in implementations of these cost functions:</p>
<ul class="simple">
<li><p><strong>Scikit-learn</strong>: <code class="docutils literal notranslate"><span class="pre">log_loss</span></code> (cross-entropy), <code class="docutils literal notranslate"><span class="pre">hinge_loss</span></code>.</p></li>
<li><p><strong>TensorFlow</strong>: <code class="docutils literal notranslate"><span class="pre">tf.keras.losses</span></code> (e.g., <code class="docutils literal notranslate"><span class="pre">BinaryCrossentropy</span></code>, <code class="docutils literal notranslate"><span class="pre">CategoricalCrossentropy</span></code>, <code class="docutils literal notranslate"><span class="pre">SparseCategoricalCrossentropy</span></code>, <code class="docutils literal notranslate"><span class="pre">Hinge</span></code>).</p></li>
<li><p><strong>PyTorch</strong>: <code class="docutils literal notranslate"><span class="pre">torch.nn</span></code> (e.g., <code class="docutils literal notranslate"><span class="pre">CrossEntropyLoss</span></code>, <code class="docutils literal notranslate"><span class="pre">BCELoss</span></code>).</p></li>
<li><p><strong>NumPy</strong>: For manual implementations or custom loss functions.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id3">
<h2>One-Stop Solution: Python Code Example<a class="headerlink" href="#id3" title="Link to this heading">#</a></h2>
<p>This script uses the <strong>Iris dataset</strong> for multi-class classification and a synthetic dataset for binary classification, computing the cost functions discussed.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np
import tensorflow as tf
from sklearn.metrics import log_loss
from sklearn.datasets import load_iris, make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC

# --- Binary Classification ---
# Generate synthetic binary data
X_bin, y_bin = make_classification(n_samples=100, n_features=2, n_classes=2, random_state=42)
X_train_bin, X_test_bin, y_train_bin, y_test_bin = train_test_split(X_bin, y_bin, test_size=0.2, random_state=42)

# Train logistic regression
model_bin = LogisticRegression()
model_bin.fit(X_train_bin, y_train_bin)
y_pred_prob_bin = model_bin.predict_proba(X_test_bin)[:, 1]  # Probability of class 1

# 1. Binary Cross-Entropy
bce = log_loss(y_test_bin, y_pred_prob_bin)
print(f&quot;Binary Cross-Entropy (Log Loss): {bce:.4f}&quot;)

# 2. Hinge Loss (using SVM)
model_svm = SVC(kernel=&#39;linear&#39;)
model_svm.fit(X_train_bin, y_train_bin)
y_pred_decision = model_svm.decision_function(X_test_bin)  # Decision scores
y_test_bin_svm = 2 * y_test_bin - 1  # Convert 0/1 to -1/1
hinge_loss = np.mean(np.maximum(0, 1 - y_test_bin_svm * y_pred_decision))
print(f&quot;Hinge Loss: {hinge_loss:.4f}&quot;)

# --- Multi-Class Classification ---
# Load Iris dataset
iris = load_iris()
X_iris, y_iris = iris.data, iris.target
X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42)

# Train logistic regression
model_multi = LogisticRegression(multi_class=&#39;ovr&#39;, max_iter=200)
model_multi.fit(X_train_iris, y_train_iris)
y_pred_prob_multi = model_multi.predict_proba(X_test_iris)

# 3. Categorical Cross-Entropy
cce = log_loss(y_test_iris, y_pred_prob_multi)
print(f&quot;Categorical Cross-Entropy: {cce:.4f}&quot;)

# 4. Sparse Categorical Cross-Entropy (TensorFlow)
sparse_cce = tf.keras.losses.SparseCategoricalCrossentropy()
sparse_cce_loss = sparse_cce(y_test_iris, y_pred_prob_multi).numpy()
print(f&quot;Sparse Categorical Cross-Entropy: {sparse_cce_loss:.4f}&quot;)

# 5. Focal Loss (manual implementation for binary)
def focal_loss(y_true, y_pred, gamma=2.0, alpha=0.25):
    pt = y_pred * y_true + (1 - y_pred) * (1 - y_true)
    return -alpha * (1 - pt) ** gamma * np.log(pt + 1e-10)  # Add small constant to avoid log(0)

focal_loss_value = np.mean([focal_loss(y, p) for y, p in zip(y_test_bin, y_pred_prob_bin)])
print(f&quot;Focal Loss (gamma=2, alpha=0.25): {focal_loss_value:.4f}&quot;)
</pre></div>
</div>
</div>
</div>
<section id="code-explanation">
<h3>Code Explanation<a class="headerlink" href="#code-explanation" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Binary Classification</strong>:</p>
<ul>
<li><p>Uses <code class="docutils literal notranslate"><span class="pre">make_classification</span></code> to create synthetic data.</p></li>
<li><p>Computes <strong>Binary Cross-Entropy</strong> with <code class="docutils literal notranslate"><span class="pre">log_loss</span></code>.</p></li>
<li><p>Computes <strong>Hinge Loss</strong> manually using SVM decision scores (labels converted to -1/1).</p></li>
</ul>
</li>
<li><p><strong>Multi-Class Classification</strong>:</p>
<ul>
<li><p>Uses the Iris dataset.</p></li>
<li><p>Computes <strong>Categorical Cross-Entropy</strong> with <code class="docutils literal notranslate"><span class="pre">log_loss</span></code>.</p></li>
<li><p>Computes <strong>Sparse Categorical Cross-Entropy</strong> with TensorFlow.</p></li>
</ul>
</li>
<li><p><strong>Focal Loss</strong>:</p>
<ul>
<li><p>Manually implemented for the binary case, adding a small constant to prevent log(0) errors.</p></li>
</ul>
</li>
</ul>
</section>
<section id="sample-output-approximate">
<h3>Sample Output (Approximate)<a class="headerlink" href="#sample-output-approximate" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Binary Cross-Entropy (Log Loss): 0.2456
Hinge Loss: 0.1234
Categorical Cross-Entropy: 0.1234
Sparse Categorical Cross-Entropy: 0.1234
Focal Loss (gamma=2, alpha=0.25): 0.0567
</pre></div>
</div>
</div>
</div>
<p><em>(Actual values vary based on data splits and model performance.)</em></p>
</section>
</section>
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>Cost functions are critical for training classification models:</p>
<ul class="simple">
<li><p><strong>Binary Cross-Entropy</strong>: Ideal for binary tasks with probability outputs.</p></li>
<li><p><strong>Categorical Cross-Entropy</strong>: Suited for multi-class problems with one-hot labels.</p></li>
<li><p><strong>Sparse Categorical Cross-Entropy</strong>: Efficient for multi-class integer labels.</p></li>
<li><p><strong>Hinge Loss</strong>: Best for SVMs and margin maximization.</p></li>
<li><p><strong>Focal Loss</strong>: Effective for imbalanced datasets by focusing on hard examples.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="id4">
<h2>Conclusion<a class="headerlink" href="#id4" title="Link to this heading">#</a></h2>
<p>Cost functions are indispensable in machine learning, serving as the foundation for evaluating and optimizing models.</p>
</section>
</section>
<hr class="docutils" />
<section id="clustering-metrics">
<h1><strong>Clustering Metrics</strong><a class="headerlink" href="#clustering-metrics" title="Link to this heading">#</a></h1>
<ul class="simple">
<li><p>Silhouette Score, Davies-Bouldin Index.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="confusion-matrix">
<h1>Confusion Matrix<a class="headerlink" href="#confusion-matrix" title="Link to this heading">#</a></h1>
<p>A <strong>Confusion Matrix</strong> is a powerful tool used in machine learning to evaluate the performance of classification models.</p>
<hr class="docutils" />
<section id="what-is-a-confusion-matrix">
<h2><strong>What is a Confusion Matrix?</strong><a class="headerlink" href="#what-is-a-confusion-matrix" title="Link to this heading">#</a></h2>
<p>A confusion matrix is a tabular representation of a classification model‚Äôs performance, comparing predicted labels to actual labels. It summarizes the counts of correct and incorrect predictions across all classes, providing a detailed breakdown beyond simple accuracy.</p>
</section>
<section id="why-use-a-confusion-matrix">
<h2><strong>Why Use a Confusion Matrix?</strong><a class="headerlink" href="#why-use-a-confusion-matrix" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Granular Insight</strong>: Reveals specific errors (e.g., false positives vs. false negatives).</p></li>
<li><p><strong>Class Imbalance</strong>: Highlights performance in imbalanced datasets where accuracy alone is misleading.</p></li>
<li><p><strong>Derived Metrics</strong>: Basis for precision, recall, F1-score, and more.</p></li>
<li><p><strong>Decision Making</strong>: Helps assess model suitability for specific tasks (e.g., minimizing false negatives in medical diagnosis).</p></li>
</ul>
</section>
<section id="structure">
<h2><strong>Structure</strong><a class="headerlink" href="#structure" title="Link to this heading">#</a></h2>
<p>For a binary classification problem (positive vs. negative):</p>
<ul class="simple">
<li><p><strong>True Positive (TP)</strong>: The model correctly predicts the positive class (e.g., identifying a sick patient as sick).</p></li>
<li><p><strong>True Negative (TN)</strong>: The model correctly predicts the negative class (e.g., identifying a healthy patient as healthy).</p></li>
<li><p><strong>False Positive (FP)</strong>: The model incorrectly predicts the positive class (a Type I error). (e.g., saying a healthy patient is sick)</p></li>
<li><p><strong>False Negative (FN)</strong>: The model incorrectly predicts the negative class (a Type II error). (e.g., saying a sick patient is healthy)</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Predicted Positive</p></th>
<th class="head"><p>Predicted Negative</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Actual Positive</strong></p></td>
<td><p>TP</p></td>
<td><p>FN</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Actual Negative</strong></p></td>
<td><p>FP</p></td>
<td><p>TN</p></td>
</tr>
</tbody>
</table>
</div>
<p>For multiclass, it extends to a (k \times k) matrix where (k) is the number of classes.</p>
</section>
<hr class="docutils" />
<section id="example-of-a-confusion-matrix">
<h2>Example of a Confusion Matrix<a class="headerlink" href="#example-of-a-confusion-matrix" title="Link to this heading">#</a></h2>
<p>Let‚Äôs consider a binary classification problem: predicting whether an email is <strong>spam</strong> (positive class) or <strong>not spam</strong> (negative class). Suppose a model makes predictions on 175 emails, with the following results:</p>
<ul class="simple">
<li><p><strong>TP</strong>: 50 spam emails correctly classified as spam.</p></li>
<li><p><strong>TN</strong>: 100 non-spam emails correctly classified as non-spam.</p></li>
<li><p><strong>FP</strong>: 15 non-spam emails incorrectly classified as spam.</p></li>
<li><p><strong>FN</strong>: 10 spam emails incorrectly classified as non-spam.</p></li>
</ul>
<p>The confusion matrix would look like this:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>Predicted Spam</p></th>
<th class="head"><p>Predicted Not Spam</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Actual Spam</strong></p></td>
<td><p>50 (TP)</p></td>
<td><p>10 (FN)</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Actual Not Spam</strong></p></td>
<td><p>15 (FP)</p></td>
<td><p>100 (TN)</p></td>
</tr>
</tbody>
</table>
</div>
<p>This table shows the model correctly classified 150 emails (TP + TN = 50 + 100) and misclassified 25 (FP + FN = 15 + 10).</p>
</section>
<hr class="docutils" />
<section id="metrics-derived-from-the-confusion-matrix">
<h2>Metrics Derived from the Confusion Matrix<a class="headerlink" href="#metrics-derived-from-the-confusion-matrix" title="Link to this heading">#</a></h2>
<p>The confusion matrix enables the calculation of several key performance metrics:</p>
<section id="components">
<h3><strong>Components</strong><a class="headerlink" href="#components" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>Diagonal</strong>: Correct predictions (TP for each class in multiclass).</p></li>
<li><p><strong>Off-Diagonal</strong>: Errors (FP and FN for binary; misclassifications for multiclass).</p></li>
</ul>
</section>
<section id="accuracy">
<h3><strong>1. Accuracy</strong><a class="headerlink" href="#accuracy" title="Link to this heading">#</a></h3>
<ul>
<li><p><strong>What It Means</strong>: The percentage of correct predictions.</p></li>
<li><p><strong>Formula</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">TP</span> <span class="o">+</span> <span class="n">TN</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">TP</span> <span class="o">+</span> <span class="n">FP</span> <span class="o">+</span> <span class="n">FN</span> <span class="o">+</span> <span class="n">TN</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><strong>Example</strong>: If TP = 50, TN = 105, FP = 10, FN = 15, then:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="mi">50</span> <span class="o">+</span> <span class="mi">100</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">50</span> <span class="o">+</span> <span class="mi">100</span> <span class="o">+</span> <span class="mi">15</span> <span class="o">+</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="mi">150</span><span class="o">/</span><span class="mi">175</span> <span class="c1"># 0.857 --&gt; 85.7%</span>
</pre></div>
</div>
</li>
<li><p><strong>When to Use</strong>: Good for balanced data, but misleading if one class dominates.</p></li>
</ul>
</section>
<section id="precision">
<h3><strong>2. Precision</strong><a class="headerlink" href="#precision" title="Link to this heading">#</a></h3>
<ul>
<li><p><strong>What It Means</strong>: How many predicted positives are actually correct.</p></li>
<li><p><strong>Formula</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">recall</span> <span class="o">=</span> <span class="n">TP</span> <span class="o">/</span> <span class="p">(</span><span class="n">TP</span> <span class="o">+</span> <span class="n">FP</span><span class="p">)</span>  <span class="c1"># &quot;How many did we catch?&quot;</span>
</pre></div>
</div>
</li>
<li><p>Interpretation: The proportion of predicted positives that are actually positive (how precise the positive predictions are).</p></li>
<li><p><strong>Example</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">recall</span> <span class="o">=</span> <span class="mi">50</span> <span class="o">/</span> <span class="p">(</span><span class="mi">50</span> <span class="o">+</span> <span class="mi">15</span><span class="p">)</span>  <span class="c1"># 76.9%</span>
</pre></div>
</div>
</li>
<li><p><strong>When to Use</strong>: Important when false positives are costly (e.g., cancer screening).</p></li>
</ul>
</section>
<section id="recall-sensitivity">
<h3><strong>3. Recall (Sensitivity)</strong><a class="headerlink" href="#recall-sensitivity" title="Link to this heading">#</a></h3>
<ul>
<li><p><strong>What It Means</strong>: How many actual positives the model finds.</p></li>
<li><p><strong>Formula</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">recall</span> <span class="o">=</span> <span class="n">TP</span> <span class="o">/</span> <span class="p">(</span><span class="n">TP</span> <span class="o">+</span> <span class="n">FN</span><span class="p">)</span>  <span class="c1"># &quot;How many did we catch?&quot;</span>
</pre></div>
</div>
</li>
<li><p>Interpretation: The proportion of actual positives correctly identified (how well the model captures positives).</p></li>
<li><p><strong>Example</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">recall</span> <span class="o">=</span> <span class="mi">50</span> <span class="o">/</span> <span class="p">(</span><span class="mi">50</span> <span class="o">+</span> <span class="mi">10</span><span class="p">)</span>  <span class="c1"># 83%</span>
</pre></div>
</div>
</li>
<li><p><strong>When to Use</strong>: Critical when missing positives is risky (e.g., detecting diseases).</p></li>
</ul>
</section>
<section id="f1-score">
<h3><strong>4. F1 Score</strong><a class="headerlink" href="#f1-score" title="Link to this heading">#</a></h3>
<ul>
<li><p><strong>What It Means</strong>: A balance between precision and recall.</p></li>
<li><p><strong>Formula</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">F1</span> <span class="n">Score</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">Precision</span> <span class="o">*</span> <span class="n">Recall</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">Precision</span> <span class="o">+</span> <span class="n">Recall</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Interpretation: The harmonic mean of precision and recall, balancing both when they‚Äôre equally important.</p></li>
<li><p><strong>Example</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">F1</span> <span class="n">Score</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="mf">0.769</span> <span class="o">*</span> <span class="mf">0.833</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mf">0.769</span> <span class="o">+</span> <span class="mf">0.833</span><span class="p">)</span> <span class="c1"># 80%</span>
</pre></div>
</div>
</li>
<li><p><strong>When to Use</strong>: When you want a single score to compare models.</p></li>
</ul>
</section>
<section id="specificity">
<h3><strong>5. Specificity</strong><a class="headerlink" href="#specificity" title="Link to this heading">#</a></h3>
<ul>
<li><p>Formula:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Specificity</span> <span class="o">=</span> <span class="n">TN</span> <span class="o">/</span> <span class="p">(</span><span class="n">TN</span> <span class="o">+</span> <span class="n">FP</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Interpretation: The proportion of actual negatives correctly identified.</p></li>
<li><p>Example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">F1</span> <span class="n">Score</span> <span class="o">=</span> <span class="mi">100</span> <span class="o">/</span> <span class="p">(</span><span class="mi">100</span> <span class="o">+</span> <span class="mi">15</span><span class="p">)</span> <span class="c1"># 87%</span>
</pre></div>
</div>
</li>
</ul>
<p>These metrics provide a comprehensive view of the model‚Äôs performance, tailored to different priorities (e.g., minimizing false positives vs. maximizing true positives).</p>
</section>
</section>
<hr class="docutils" />
<section id="id5">
<h2>One-Stop Solution: Python Code Example<a class="headerlink" href="#id5" title="Link to this heading">#</a></h2>
<p>This example uses the <strong>Iris dataset</strong> (a multi-class classification problem with three classes: setosa, versicolor, virginica) to train a model, compute the confusion matrix, visualize it, and calculate metrics.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay

# Load the Iris dataset
iris = load_iris()
X, y = iris.data, iris.target  # Features and target labels

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a Random Forest classifier
model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)  # Make predictions on the test set

# Compute the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Visualize the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=iris.target_names)
disp.plot(cmap=&#39;Blues&#39;)
plt.title(&quot;Confusion Matrix - Iris Dataset&quot;)
plt.show()

# Print a detailed classification report
print(&quot;Classification Report:&quot;)
print(classification_report(y_test, y_pred, target_names=iris.target_names))
</pre></div>
</div>
</div>
</div>
<section id="id6">
<h3>Code Explanation<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Data Loading and Splitting</strong>:</p>
<ul class="simple">
<li><p>The Iris dataset is loaded with 150 samples across 3 classes.</p></li>
<li><p>It‚Äôs split into 80% training and 20% testing data (<code class="docutils literal notranslate"><span class="pre">test_size=0.2</span></code>).</p></li>
</ul>
</li>
<li><p><strong>Model Training</strong>:</p>
<ul class="simple">
<li><p>A <code class="docutils literal notranslate"><span class="pre">RandomForestClassifier</span></code> is trained on the training data.</p></li>
</ul>
</li>
<li><p><strong>Confusion Matrix Computation and Visualization</strong>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">confusion_matrix</span></code> computes the matrix comparing <code class="docutils literal notranslate"><span class="pre">y_test</span></code> (actual labels) and <code class="docutils literal notranslate"><span class="pre">y_pred</span></code> (predicted labels).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ConfusionMatrixDisplay</span></code> creates a heatmap of the matrix, labeled with class names (setosa, versicolor, virginica).</p></li>
</ul>
</li>
<li><p><strong>Classification Report</strong>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">classification_report</span></code> outputs precision, recall, F1-score, and support (number of samples) for each class, plus overall averages.</p></li>
</ul>
</li>
</ol>
</section>
<section id="sample-output">
<h3>Sample Output<a class="headerlink" href="#sample-output" title="Link to this heading">#</a></h3>
<section id="confusion-matrix-plot">
<h4>Confusion Matrix Plot<a class="headerlink" href="#confusion-matrix-plot" title="Link to this heading">#</a></h4>
<p>A 3x3 heatmap might look like this (values depend on the random split):</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>           Predicted
           | Setosa | Versicolor | Virginica |
Actual     |--------|------------|-----------|
Setosa     |   10   |     0      |     0     |
Versicolor |    0   |     9      |     1     |
Virginica  |    0   |     1      |     9     |
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p>Diagonal values (10, 9, 9) are TPs for each class.</p></li>
<li><p>Off-diagonal values (0s and 1s) are FPs and FNs.</p></li>
</ul>
</section>
<section id="classification-report">
<h4>Classification Report<a class="headerlink" href="#classification-report" title="Link to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Classification Report:
              precision    recall  f1-score   support
      setosa       1.00      1.00      1.00        10
  versicolor       0.90      0.90      0.90        10
   virginica       0.90      0.90      0.90        10
    accuracy                           0.93        30
   macro avg       0.93      0.93      0.93        30
weighted avg       0.93      0.93      0.93        30
</pre></div>
</div>
</div>
</div>
<ul class="simple">
<li><p><strong>Setosa</strong>: Perfectly classified (precision, recall, F1 = 1.00).</p></li>
<li><p><strong>Versicolor/Virginica</strong>: Minor errors (e.g., one misclassification each), yielding 0.90 for precision, recall, and F1.</p></li>
<li><p><strong>Accuracy</strong>: 93% overall.</p></li>
</ul>
<p><em>(Note: Exact numbers may vary due to randomness in the split and model.)</em></p>
</section>
</section>
</section>
<hr class="docutils" />
<section id="auc-and-roc-curve">
<h2><strong>AUC and ROC Curve</strong><a class="headerlink" href="#auc-and-roc-curve" title="Link to this heading">#</a></h2>
<p>The <strong>Area Under the Curve (AUC)</strong> measures how well the model separates classes. It comes from the <strong>ROC Curve</strong>, which plots Recall (True Positive Rate) against False Positive Rate.</p>
<ul class="simple">
<li><p><strong>AUC = 1</strong>: Perfect model.</p></li>
<li><p><strong>AUC = 0.5</strong>: No better than guessing.</p></li>
<li><p><strong>AUC &lt; 0.5</strong>: Worse than random.</p></li>
</ul>
<section id="code-example">
<h3><strong>Code Example</strong><a class="headerlink" href="#code-example" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from sklearn.metrics import roc_curve, auc

# Get prediction probabilities
y_scores = model.predict_proba(X_test)[:, 1]

# Calculate ROC curve
fpr, tpr, _ = roc_curve(y_test, y_scores)
roc_auc = auc(fpr, tpr)

# Plot it
plt.plot(fpr, tpr, label=f&#39;AUC = {roc_auc:.2f}&#39;)
plt.xlabel(&#39;False Positive Rate&#39;)
plt.ylabel(&#39;True Positive Rate&#39;)
plt.title(&#39;ROC Curve: Titanic Survival&#39;)
plt.legend()
plt.show()
</pre></div>
</div>
</div>
</div>
<p>A higher AUC means better performance.</p>
</section>
</section>
<hr class="docutils" />
<section id="tools-and-methods-summary">
<h2><strong>Tools and Methods Summary</strong><a class="headerlink" href="#tools-and-methods-summary" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">sklearn.metrics.confusion_matrix</span></code>: Computes the confusion matrix from true and predicted labels.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">sklearn.metrics.classification_report</span></code>: Provides a detailed report with precision, recall, F1-score, and support.</p></li>
<li><p><strong>Visualization</strong>: <code class="docutils literal notranslate"><span class="pre">seaborn.heatmap()</span></code>, <code class="docutils literal notranslate"><span class="pre">matplotlib.pyplot</span></code> for custom plots.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Custom metrics
accuracy = (TP + TN) / (TP + TN + FP + FN)
precision = TP / (TP + FP) if (TP + FP) &gt; 0 else 0
recall = TP / (TP + FN) if (TP + FN) &gt; 0 else 0
f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) &gt; 0 else 0
print(f&quot;Accuracy: {accuracy:.2f}, Precision: {precision:.2f}, Recall: {recall:.2f}, F1: {f1:.2f}&quot;)
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./contents\3_ml"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="2_train_test_split.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Train &amp; Test Datasets in Python</p>
      </div>
    </a>
    <a class="right-next"
       href="11_supervised_learning.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><strong>Supervised Learning</strong></p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">4Ô∏è‚É£ Model Evaluation</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-metrics"><strong>Regression Metrics</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#distance-based-error-explained">Distance-Based Error Explained</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-squared-error-mse">1. Mean Squared Error (MSE)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mean-absolute-error-mae">2. Mean Absolute Error (MAE)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#python-implementation">Python Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#root-mean-squared-error-rmse">3. Root Mean Squared Error (RMSE)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Python Implementation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tools-and-methods">Tools and Methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#one-stop-solution-python-code-example">One-Stop Solution: Python Code Example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-takeaways">Key Takeaways</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-metrics"><strong>Classification Metrics</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-cross-entropy-log-loss">1. Binary Cross-Entropy (Log Loss)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#categorical-cross-entropy">2. Categorical Cross-Entropy</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#when-to-use"><strong>When to Use</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example"><strong>Example</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#sparse-categorical-cross-entropy">3. Sparse Categorical Cross-Entropy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hinge-loss">4. Hinge Loss</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Tools and Methods</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">One-Stop Solution: Python Code Example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-explanation">Code Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-output-approximate">Sample Output (Approximate)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#clustering-metrics"><strong>Clustering Metrics</strong></a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-matrix">Confusion Matrix</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-confusion-matrix"><strong>What is a Confusion Matrix?</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-use-a-confusion-matrix"><strong>Why Use a Confusion Matrix?</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#structure"><strong>Structure</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-of-a-confusion-matrix">Example of a Confusion Matrix</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics-derived-from-the-confusion-matrix">Metrics Derived from the Confusion Matrix</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#components"><strong>Components</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#accuracy"><strong>1. Accuracy</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#precision"><strong>2. Precision</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#recall-sensitivity"><strong>3. Recall (Sensitivity)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#f1-score"><strong>4. F1 Score</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#specificity"><strong>5. Specificity</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">One-Stop Solution: Python Code Example</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Code Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sample-output">Sample Output</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-matrix-plot">Confusion Matrix Plot</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-report">Classification Report</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#auc-and-roc-curve"><strong>AUC and ROC Curve</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#code-example"><strong>Code Example</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tools-and-methods-summary"><strong>Tools and Methods Summary</strong></a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gajanesh
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright ¬© 2025 Gajanesh. All rights reserved..
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>