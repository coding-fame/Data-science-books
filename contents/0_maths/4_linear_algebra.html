
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Linear Algebra &#8212; Data Science Books</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-dropdown.css?v=995e94df" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-bootstrap.min.css?v=21c0b90a" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=d567e03f" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'contents/0_maths/4_linear_algebra';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Probability in ML" href="2_probability.html" />
    <link rel="prev" title="Essential Maths" href="0_essential.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Data Science Books</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Part I ‚Äî Foundations</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="0_essential.html">Essential Mathematics</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Linear Algebra</a></li>







<li class="toctree-l1"><a class="reference internal" href="2_probability.html">Probability Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_descriptive.html">Descriptive Statistics</a></li>








<li class="toctree-l1"><a class="reference internal" href="3_inferential.html">Inferential Statistics</a></li>



<li class="toctree-l1"><a class="reference internal" href="5_calculus.html">Calculus</a></li>







<li class="toctree-l1"><a class="reference internal" href="6_regression_analysis.html">Explanatory and Response Variables</a></li>


<li class="toctree-l1"><a class="reference internal" href="../1_python/1_basics.html">Python Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/2_advanced.html">Advanced Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/3_data_structures.html">Data Structures</a></li>





<li class="toctree-l1"><a class="reference internal" href="../1_python/4_modules_packages.html">Modules &amp; Packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/5_functions.html">Functions &amp; Modular Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/6_oop.html">Object-Oriented Programming</a></li>





<li class="toctree-l1"><a class="reference internal" href="../1_python/8_exceptions.html">Exception Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/9_regex.html">Regular Expressions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_numpy/1_numpy.html">NumPy (<strong>Numerical Python</strong>)</a></li>



<li class="toctree-l1"><a class="reference internal" href="../2_pandas/1_series.html">Pandas Series</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_pandas/2_dataframes.html">Pandas DataFrame</a></li>
















<li class="toctree-l1"><a class="reference internal" href="../2_pandas/3_visualization.html"><strong>What is Data Visualization in Data Science?</strong></a></li>


<li class="toctree-l1"><a class="reference internal" href="../2_pandas/4_eda.html">Exploratory Data Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_pandas/5_feature_engineering.html"><strong>What is Feature Engineering?</strong></a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part II ‚Äî Classical Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../3_ml/1_foundations.html">ML Foundational</a></li>





<li class="toctree-l1"><a class="reference internal" href="../3_ml/2_data_preparation.html">2Ô∏è‚É£ Data Handling</a></li>





<li class="toctree-l1"><a class="reference internal" href="../3_ml/2_train_test_split.html">Train‚ÄìTest Split</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/5_model_evaluation.html">4Ô∏è‚É£ Model Evaluation</a></li>




<li class="toctree-l1"><a class="reference internal" href="../3_ml/11_supervised_learning.html"><strong>Supervised Learning</strong></a></li>



<li class="toctree-l1"><a class="reference internal" href="../3_ml/12_regression.html">Regression Algorithms</a></li>







<li class="toctree-l1"><a class="reference internal" href="../3_ml/13_classification.html">Classification Algorithms</a></li>

<li class="toctree-l1"><a class="reference internal" href="../3_ml/10_core_algo.html">Core ML Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/14_decision_trees.html">Decision Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/15_ensemble_methods.html">Ensemble Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/16_svm.html">Support Vector Machine (SVM) in Detail</a></li>


<li class="toctree-l1"><a class="reference internal" href="../3_ml/17_knn.html">k-Nearest Neighbors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/18_naive_bayes.html">Naive Bayes</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part III ‚Äî Advanced Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../3_ml/20_unsupervised_learning.html">Unsupervised Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../3_ml/21_clustering.html">Clustering Techniques</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/7_optimization_and_training.html">5Ô∏è‚É£ Optimization &amp; Training</a></li>







<li class="toctree-l1 has-children"><a class="reference internal" href="../3_ml/6_ml_lifecycle.html">ML Lifecycle</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../3_ml/6_training.html">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3_ml/6_evaluation.html">Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3_ml/6_deployment.html">Deployment</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part IV ‚Äî Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl1_Introduction.html">Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl2_Neuron.html">Neuron</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl3_Libraries.html">Deep Learning Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl4_Terminology.html">Terminology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl5_multi_layer.html">Multi-Layer Perceptron</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl6_first_nn.html">First Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl7_evaluating_model.html">Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl8_multiclass_classification.html">Multiclass Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl9_multiclass_classification_hand.html">Handwritten Digit Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl10_saving_and_loading.html">Saving &amp; Loading Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl11_checkpointing.html">Model Checkpointing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl12_visualizing_model_training.html"><strong>Visualizing Model Training History in Deep Learning</strong></a></li>

<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl13_loss_functions_activation_functions_and_optimizers.html">Loss Functions &amp; Optimizers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part V ‚Äî NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp1.html">NLP Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp2.html">Text Cleaning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp3.html">Text Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp4.html">NLP Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp5.html">Bag of Words, TF-IDF</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp6.html">Sentiment Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp7.html">NLP with SpaCy</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part VI ‚Äî Career &amp; MLOps</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../6_interview/self%20introduction.html">Self Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6_interview/py.html">Python: Interview Guide</a></li>





<li class="toctree-l1"><a class="reference internal" href="../6_interview/pd.html">üìö Pandas: Interview Guide</a></li>


<li class="toctree-l1"><a class="reference internal" href="../6_interview/ml.html">Machine Learning: Interview Guide</a></li>













<li class="toctree-l1"><a class="reference internal" href="../6_interview/git.html">Git</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6_interview/dvc.html">DVC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6_interview/mlflow.html">MLflow</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/coding-fame/Data-science-books" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/coding-fame/Data-science-books/edit/main/contents/0_maths/4_linear_algebra.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/coding-fame/Data-science-books/issues/new?title=Issue%20on%20page%20%2Fcontents/0_maths/4_linear_algebra.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/contents/0_maths/4_linear_algebra.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Linear Algebra</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Linear Algebra</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-algebra-the-foundation-of-ml-dl">Linear Algebra: The Foundation of ML &amp; DL</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-linear-algebra">Introduction to Linear Algebra</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-solving-techniques">Problem-Solving Techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#methods-to-solve-ax-b">Methods to Solve ( Ax = b )</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solution-types">Solution Types</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-in-machine-learning">Applications in Machine Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression">1. Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#principal-component-analysis-pca">2. Principal Component Analysis (PCA)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks">3. Neural Networks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#non-linear-vs-linear-systems">Non-Linear vs Linear Systems</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-linear-algebra-matters-in-ai-ml">Why Linear Algebra Matters in AI/ML</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#critical-linear-algebra-operations">Critical Linear Algebra Operations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#vectors-the-building-blocks">Vectors: The Building Blocks</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-vectors">1. Introduction to Vectors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#essential-vector-properties">2. Essential Vector Properties</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#magnitude-length">Magnitude (Length)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#direction">Direction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-types">Vector Types</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fundamental-vector-operations">3. Fundamental Vector Operations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-operations">Basic Operations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dot-product">Dot Product</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#norm-comparisons">Norm Comparisons</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-applications">4. Machine Learning Applications</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-representation">Feature Representation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#similarity-analysis">Similarity Analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#geometric-transformations">Geometric Transformations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-vectors-matter-in-ml">5. Why Vectors Matter in ML</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#matrices-the-foundation-of-data">üåü Matrices: The Foundation of Data</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-matrices">What Are Matrices?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-matrices">Types of Matrices</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-operations">Key Operations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#special-matrices-and-properties">Special Matrices and Properties</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-ml">Connection to ML</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-factorization-revealing-hidden-structures">Matrix Factorization: Revealing Hidden Structures</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-matrix-factorization">What is Matrix Factorization?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-techniques">Key Techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#singular-value-decomposition-svd">Singular Value Decomposition (SVD)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eigen-decomposition">Eigen Decomposition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Principal Component Analysis (PCA)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Connection to ML</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-moore-penrose-pseudoinverse-mpp">4Ô∏è‚É£ <strong>The Moore-Penrose Pseudoinverse (MPP)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#trace-operator">5Ô∏è‚É£ <strong>Trace Operator</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#tensors-beyond-matrices">Tensors: Beyond Matrices</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-tensors">What Are Tensors?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-hierarchy">Tensor Hierarchy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-applications">Deep Learning Applications</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-dl">Connection to DL</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#why-linear-algebra-matters">Why Linear Algebra Matters</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-takeaways">Key Takeaways</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-transformations-in-machine-learning">Linear Transformations in Machine Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-linear-transformations">What Are Linear Transformations?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-properties">Key Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-it-works">How It Works</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-linear-transformations">Common Linear Transformations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling-adjusting-size">1. Scaling ‚Äì Adjusting Size</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#transformation-matrix">Transformation Matrix</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#python-example">Python Example</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation">Explanation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rotation-spinning-objects">2. Rotation ‚Äì Spinning Objects</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Transformation Matrix</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Python Example</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Explanation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shearing-the-leaning-tower-effect">3. Shearing ‚Äì The ‚ÄúLeaning Tower‚Äù Effect</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Transformation Matrix</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Python Example</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Explanation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reflection-the-mirror-effect">4. Reflection ‚Äì The Mirror Effect</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Transformation Matrix</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Python Example</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">Explanation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-common-transformations">Summary of Common Transformations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">Applications in Machine Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-notes">Final Notes</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="linear-algebra">
<h1>Linear Algebra<a class="headerlink" href="#linear-algebra" title="Link to this heading">#</a></h1>
</section>
<hr class="docutils" />
<section id="linear-algebra-the-foundation-of-ml-dl">
<h1>Linear Algebra: The Foundation of ML &amp; DL<a class="headerlink" href="#linear-algebra-the-foundation-of-ml-dl" title="Link to this heading">#</a></h1>
<p>Linear algebra is essential for ML and DL, providing the tools to represent data, build models, and perform efficient computations. It translates real-world problems into mathematical formats that machines can understand.</p>
<blockquote>
<div><p><strong>Key Insight</strong>: ‚ÄúLinear algebra is to machine learning what grammar is to language‚Äîit structures data and models into a coherent framework.‚Äù ‚Äì AI Researcher</p>
</div></blockquote>
<hr class="docutils" />
<section id="introduction-to-linear-algebra">
<h2>Introduction to Linear Algebra<a class="headerlink" href="#introduction-to-linear-algebra" title="Link to this heading">#</a></h2>
<section id="core-concepts">
<h3>Core Concepts<a class="headerlink" href="#core-concepts" title="Link to this heading">#</a></h3>
<p>Linear algebra focuses on three main objects:</p>
<ul class="simple">
<li><p><strong>Vectors</strong>: Ordered lists of numbers representing data points or directions.</p>
<ul>
<li><p><em>Example</em>: <code class="docutils literal notranslate"><span class="pre">[2,</span> <span class="pre">3]</span></code> can represent a point in 2D space or features like [height, weight].</p></li>
</ul>
</li>
<li><p><strong>Matrices</strong>: Rectangular arrays that organize data or perform transformations.</p>
<ul>
<li><p><em>Example</em>: <code class="docutils literal notranslate"><span class="pre">[[1,</span> <span class="pre">2],</span> <span class="pre">[3,</span> <span class="pre">4]]</span></code> can represent a dataset or a linear transformation.</p></li>
</ul>
</li>
<li><p><strong>Tensors</strong>: Multi-dimensional arrays generalizing vectors and matrices, crucial in DL for handling complex data like images.</p>
<ul>
<li><p><em>Example</em>: A 3D tensor might represent an image with height, width, and color channels.</p></li>
</ul>
</li>
</ul>
<p>These objects enable:</p>
<ol class="arabic simple">
<li><p><strong>Data Representation</strong>: Structuring datasets as matrices or tensors.</p></li>
<li><p><strong>Model Architecture</strong>: Building models like linear regression and neural networks.</p></li>
<li><p><strong>Efficient Computation</strong>: Leveraging matrix operations for faster processing, especially on GPUs.</p></li>
</ol>
</section>
</section>
<hr class="docutils" />
<section id="problem-solving-techniques">
<h2>Problem-Solving Techniques<a class="headerlink" href="#problem-solving-techniques" title="Link to this heading">#</a></h2>
<p>A fundamental task in linear algebra is solving systems of linear equations, often represented as ( Ax = b ), where:</p>
<ul class="simple">
<li><p>( A ) is the matrix of coefficients,</p></li>
<li><p>( x ) is the vector of unknowns,</p></li>
<li><p>( b ) is the result vector.</p></li>
</ul>
<section id="methods-to-solve-ax-b">
<h3>Methods to Solve ( Ax = b )<a class="headerlink" href="#methods-to-solve-ax-b" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Substitution</strong>: Solve for one variable and substitute into other equations.</p>
<ul class="simple">
<li><p><em>Best for</em>: Small systems with few variables.</p></li>
</ul>
</li>
<li><p><strong>Elimination</strong>: Add or subtract equations to eliminate variables.</p>
<ul class="simple">
<li><p><em>Best for</em>: Systems with clear patterns or when substitution is tedious.</p></li>
</ul>
</li>
<li><p><strong>Matrix Inversion</strong>: Compute ( x = A^{-1}b ) if ( A ) is invertible (i.e., ( \det(A) \neq 0 )).</p>
<ul class="simple">
<li><p><em>Best for</em>: Square matrices with unique solutions.</p></li>
</ul>
</li>
</ol>
</section>
<section id="solution-types">
<h3>Solution Types<a class="headerlink" href="#solution-types" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Scenario</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Example (2D)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Unique Solution</strong></p></td>
<td><p>One exact solution</p></td>
<td><p>Two intersecting lines</p></td>
</tr>
<tr class="row-odd"><td><p><strong>No Solution</strong></p></td>
<td><p>Inconsistent system</p></td>
<td><p>Parallel lines</p></td>
</tr>
<tr class="row-even"><td><p><strong>Infinite Solutions</strong></p></td>
<td><p>Dependent system</p></td>
<td><p>Overlapping lines</p></td>
</tr>
</tbody>
</table>
</div>
<p>Here‚Äôs a flowchart to determine the solution type:</p>
</section>
</section>
<hr class="docutils" />
<section id="applications-in-machine-learning">
<h2>Applications in Machine Learning<a class="headerlink" href="#applications-in-machine-learning" title="Link to this heading">#</a></h2>
<section id="linear-regression">
<h3>1. Linear Regression<a class="headerlink" href="#linear-regression" title="Link to this heading">#</a></h3>
<p>Linear regression models the relationship between input features and a target variable using the equation:
[ y = Xw + b ]</p>
<ul class="simple">
<li><p><strong>( X )</strong>: Feature matrix (rows are data points, columns are features).</p></li>
<li><p><strong>( y )</strong>: Target vector (e.g., house prices).</p></li>
<li><p><strong>( w )</strong>: Weights (coefficients for each feature).</p></li>
<li><p><strong>( b )</strong>: Bias term.</p></li>
</ul>
<p><strong>How It Works</strong>: The model learns ( w ) and ( b ) to minimize prediction errors, often using matrix operations for efficiency.</p>
<p><strong>Code Example</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np

# Feature matrix: [Bias (1), Size (sqft), Bedrooms]
X = np.array([[1, 1500, 3], [1, 2000, 4], [1, 1200, 2]])
y = np.array([300000, 400000, 250000])

# Solve for weights using least squares
weights, _, _, _ = np.linalg.lstsq(X, y, rcond=None)
print(f&quot;Weights: {weights.round(2)}&quot;)  # [bias, size_weight, bedroom_weight]
</pre></div>
</div>
</div>
</div>
</section>
<section id="principal-component-analysis-pca">
<h3>2. Principal Component Analysis (PCA)<a class="headerlink" href="#principal-component-analysis-pca" title="Link to this heading">#</a></h3>
<p>PCA reduces data dimensionality by finding directions (principal components) of maximum variance.</p>
<ul class="simple">
<li><p><strong>How</strong>: It uses eigenvalue decomposition of the covariance matrix to project data onto fewer dimensions while retaining key information.</p></li>
<li><p><strong>Use Case</strong>: Visualizing high-dimensional data or speeding up ML algorithms.</p></li>
</ul>
</section>
<section id="neural-networks">
<h3>3. Neural Networks<a class="headerlink" href="#neural-networks" title="Link to this heading">#</a></h3>
<p>Neural networks rely heavily on linear algebra:</p>
<ul class="simple">
<li><p><strong>Weight Matrices</strong>: Transform inputs between layers via matrix multiplication.</p></li>
<li><p><strong>Activation Functions</strong>: Introduce non-linearity (e.g., ReLU, sigmoid).</p></li>
<li><p><strong>Backpropagation</strong>: Computes gradients using matrix calculus to update weights.</p></li>
</ul>
<p><strong>Example</strong>: In a simple neural network layer, the output is ( \sigma(Wx + b) ), where ( W ) is the weight matrix, ( x ) is the input vector, ( b ) is the bias, and ( \sigma ) is the activation function.</p>
</section>
</section>
<hr class="docutils" />
<section id="non-linear-vs-linear-systems">
<h2>Non-Linear vs Linear Systems<a class="headerlink" href="#non-linear-vs-linear-systems" title="Link to this heading">#</a></h2>
<p>While linear algebra handles linear relationships, many real-world problems are non-linear. Here‚Äôs when to use alternative methods:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Scenario</p></th>
<th class="head"><p>Method</p></th>
<th class="head"><p>Example</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Polynomial Relationships</p></td>
<td><p>Polynomial Regression</p></td>
<td><p>( y = ax^2 + bx + c )</p></td>
</tr>
<tr class="row-odd"><td><p>Exponential Growth</p></td>
<td><p>Logarithmic Transforms</p></td>
<td><p>Population growth modeling</p></td>
</tr>
<tr class="row-even"><td><p>Periodic Patterns</p></td>
<td><p>Fourier Analysis</p></td>
<td><p>Signal processing</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Code Example</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Non-linear transformation
quadratic = lambda x: x**2 + 2*x + 1
exponential = lambda x: np.exp(x)
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="why-linear-algebra-matters-in-ai-ml">
<h2>Why Linear Algebra Matters in AI/ML<a class="headerlink" href="#why-linear-algebra-matters-in-ai-ml" title="Link to this heading">#</a></h2>
<p>Linear algebra is indispensable for:</p>
<ul class="simple">
<li><p><strong>Data Structuring</strong>: Representing datasets as matrices or tensors.</p></li>
<li><p><strong>Model Design</strong>: Building and optimizing models like regression and neural networks.</p></li>
<li><p><strong>Efficient Computation</strong>: Leveraging matrix operations for parallel processing on GPUs.</p></li>
</ul>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Aspect</p></th>
<th class="head"><p>ML Application</p></th>
<th class="head"><p>Benefit</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Data Organization</strong></p></td>
<td><p>Tensors in TensorFlow/PyTorch</p></td>
<td><p>Efficient batch processing</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Model Optimization</strong></p></td>
<td><p>Gradient Descent</p></td>
<td><p>Faster convergence via matrix ops</p></td>
</tr>
<tr class="row-even"><td><p><strong>Dimensionality Reduction</strong></p></td>
<td><p>PCA/SVD</p></td>
<td><p>Noise reduction &amp; feature extraction</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<hr class="docutils" />
<section id="critical-linear-algebra-operations">
<h2>Critical Linear Algebra Operations<a class="headerlink" href="#critical-linear-algebra-operations" title="Link to this heading">#</a></h2>
<p>These operations are the workhorses of ML algorithms:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Operation</p></th>
<th class="head"><p>Formula</p></th>
<th class="head"><p>ML Use Case</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Dot Product</strong></p></td>
<td><p>( \mathbf{a} \cdot \mathbf{b} = \sum a_i b_i )</p></td>
<td><p>Similarity measurement</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Matrix Multiplication</strong></p></td>
<td><p>( C = AB )</p></td>
<td><p>Neural network forward pass</p></td>
</tr>
<tr class="row-even"><td><p><strong>Eigen Decomposition</strong></p></td>
<td><p>( A = Q \Lambda Q^{-1} )</p></td>
<td><p>PCA implementation</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Singular Value Decomposition</strong></p></td>
<td><p>( A = U \Sigma V^T )</p></td>
<td><p>Recommendation systems</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<hr class="docutils" />
<section id="conclusion">
<h2>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h2>
<p>Linear algebra provides the mathematical foundation for:</p>
<ul class="simple">
<li><p><strong>Structuring data</strong> as vectors, matrices, and tensors.</p></li>
<li><p><strong>Designing models</strong> through operations like matrix multiplication and decomposition.</p></li>
<li><p><strong>Optimizing computations</strong> for speed and efficiency in ML/DL.</p></li>
</ul>
<p>Mastering these concepts is essential for building and understanding machine learning systems.</p>
</section>
</section>
<hr class="docutils" />
<section id="vectors-the-building-blocks">
<h1>Vectors: The Building Blocks<a class="headerlink" href="#vectors-the-building-blocks" title="Link to this heading">#</a></h1>
<p>Vectors are the fundamental units in linear algebra, representing both physical quantities (like velocity) and abstract data (like feature sets in ML). They are essential for translating real-world observations into mathematical models.</p>
<hr class="docutils" />
<section id="introduction-to-vectors">
<h2>1. Introduction to Vectors<a class="headerlink" href="#introduction-to-vectors" title="Link to this heading">#</a></h2>
<p>A vector is an ordered list of numbers that conveys both <strong>magnitude</strong> (size) and <strong>direction</strong>. In ML, vectors represent data points, features, or model parameters.</p>
<p><em>Key Characteristics</em>*:</p>
<ul class="simple">
<li><p>Represented as <code class="docutils literal notranslate"><span class="pre">[x‚ÇÅ,</span> <span class="pre">x‚ÇÇ,</span> <span class="pre">...,</span> <span class="pre">x‚Çô]</span></code> in n-dimensional space</p></li>
<li><p>Can model both physical quantities (force, velocity) and abstract data (features, embeddings)</p></li>
</ul>
<p><strong>Example</strong>:</p>
<ul class="simple">
<li><p>A 2D vector <code class="docutils literal notranslate"><span class="pre">[3,</span> <span class="pre">4]</span></code> can represent:</p>
<ul>
<li><p>A point 3 units right and 4 units up from the origin.</p></li>
<li><p>Features like [age, income] in a dataset.</p></li>
</ul>
</li>
</ul>
</section>
<hr class="docutils" />
<section id="essential-vector-properties">
<h2>2. Essential Vector Properties<a class="headerlink" href="#essential-vector-properties" title="Link to this heading">#</a></h2>
<section id="magnitude-length">
<h3>Magnitude (Length)<a class="headerlink" href="#magnitude-length" title="Link to this heading">#</a></h3>
<p>The magnitude, or L2 norm, measures a vector‚Äôs size:
[ | \mathbf{v} | = \sqrt{v_1^2 + v_2^2 + \cdots + v_n^2} ]</p>
<p><strong>Code Example</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np
v = np.array([3, 4])
print(f&quot;Magnitude: {np.linalg.norm(v):.1f}&quot;)  # Output: 5.0
</pre></div>
</div>
</div>
</div>
</section>
<section id="direction">
<h3>Direction<a class="headerlink" href="#direction" title="Link to this heading">#</a></h3>
<p>The direction is represented by a unit vector (magnitude = 1):
[ \hat{v} = \frac{\mathbf{v}}{| \mathbf{v} |} ]</p>
<p><strong>Code Example</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>unit_v = v / np.linalg.norm(v)  # [0.6, 0.8]
</pre></div>
</div>
</div>
</div>
</section>
<section id="vector-types">
<h3>Vector Types<a class="headerlink" href="#vector-types" title="Link to this heading">#</a></h3>
<p>Vectors can be row or column vectors, which matters in matrix operations.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Type</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Shape in Python</p></th>
<th class="head"><p>Use Case</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Row Vector</strong></p></td>
<td><p>Horizontal array</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">(1,</span> <span class="pre">n)</span></code></p></td>
<td><p>Feature vectors</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Column Vector</strong></p></td>
<td><p>Vertical array</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">(n,</span> <span class="pre">1)</span></code></p></td>
<td><p>Linear transformations</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Code Example</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>row = np.array([[1, 2, 3]])    # Shape: (1, 3)
column = row.T                 # Shape: (3, 1)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="fundamental-vector-operations">
<h2>3. Fundamental Vector Operations<a class="headerlink" href="#fundamental-vector-operations" title="Link to this heading">#</a></h2>
<section id="basic-operations">
<h3>Basic Operations<a class="headerlink" href="#basic-operations" title="Link to this heading">#</a></h3>
<p>Vectors support addition, scalar multiplication, and more.</p>
<p><strong>Code Example</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>v = np.array([3, 4])
w = np.array([1, 2])
print(v + w)  # [4, 6]
print(3 * v)  # [9, 12]
</pre></div>
</div>
</div>
</div>
</section>
<section id="dot-product">
<h3>Dot Product<a class="headerlink" href="#dot-product" title="Link to this heading">#</a></h3>
<p>The dot product measures similarity or projection between vectors:
[ \mathbf{a} \cdot \mathbf{b} = \sum_{i=1}^n a_i b_i ]</p>
<p><strong>Applications</strong>:</p>
<ul class="simple">
<li><p>Calculating cosine similarity in recommendation systems.</p></li>
<li><p>Computing weighted sums in neural networks.</p></li>
</ul>
<p><strong>Code Example</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>a = np.array([1, 2, 3])
b = np.array([4, 5, 6])
print(np.dot(a, b))  # 32
</pre></div>
</div>
</div>
</div>
</section>
<section id="norm-comparisons">
<h3>Norm Comparisons<a class="headerlink" href="#norm-comparisons" title="Link to this heading">#</a></h3>
<p>Different norms measure vector size in various ways:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Norm Type</p></th>
<th class="head"><p>Formula</p></th>
<th class="head"><p>Application</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>L1 (Manhattan)</strong></p></td>
<td><p>`Œ£</p></td>
<td><p>x·µ¢</p></td>
</tr>
<tr class="row-odd"><td><p><strong>L2 (Euclidean)</strong></p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">‚àöŒ£x·µ¢¬≤</span></code></p></td>
<td><p>Standard distance</p></td>
</tr>
<tr class="row-even"><td><p><strong>L‚àû (Maximum)</strong></p></td>
<td><p>( \max</p></td>
<td><p>x_i</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<hr class="docutils" />
<section id="machine-learning-applications">
<h2>4. Machine Learning Applications<a class="headerlink" href="#machine-learning-applications" title="Link to this heading">#</a></h2>
<section id="feature-representation">
<h3>Feature Representation<a class="headerlink" href="#feature-representation" title="Link to this heading">#</a></h3>
<p>Vectors represent data points in ML models.</p>
<p><strong>Example</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># House features: [sqft, bedrooms, bathrooms]
house = np.array([1500, 3, 2])
</pre></div>
</div>
</div>
</div>
</section>
<section id="similarity-analysis">
<h3>Similarity Analysis<a class="headerlink" href="#similarity-analysis" title="Link to this heading">#</a></h3>
<p>Dot products help measure how similar two vectors are.</p>
<p><strong>Example</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># User preferences: [action, comedy, documentary]
user1 = np.array([5, 3, 2])
user2 = np.array([1, 4, 5])
similarity = np.dot(user1, user2)  # Higher value = more similar
</pre></div>
</div>
</div>
</div>
</section>
<section id="geometric-transformations">
<h3>Geometric Transformations<a class="headerlink" href="#geometric-transformations" title="Link to this heading">#</a></h3>
<p>Vectors can be scaled, rotated, or projected.</p>
<ul class="simple">
<li><p><strong>Scaling</strong>: <code class="docutils literal notranslate"><span class="pre">v_scaled</span> <span class="pre">=</span> <span class="pre">2</span> <span class="pre">*</span> <span class="pre">v</span></code></p></li>
<li><p><strong>Rotation</strong>: Using rotation matrices.</p></li>
<li><p><strong>Projection</strong>: Reducing dimensions via techniques like PCA.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="why-vectors-matter-in-ml">
<h2>5. Why Vectors Matter in ML<a class="headerlink" href="#why-vectors-matter-in-ml" title="Link to this heading">#</a></h2>
<p>Vectors are crucial because they:</p>
<ol class="arabic simple">
<li><p><strong>Standardize Data</strong>: Provide a consistent numerical format.</p></li>
<li><p><strong>Enable Algorithms</strong>: Form the basis for models like SVMs and neural networks.</p></li>
<li><p><strong>Boost Efficiency</strong>: Allow fast computations via vectorized operations.</p></li>
<li><p><strong>Aid Visualization</strong>: Help interpret high-dimensional data geometrically.</p></li>
</ol>
<p><strong>Analogy</strong>:</p>
<blockquote>
<div><p>‚ÄúVectors are like GPS coordinates for data‚Äîthey give both direction (relationships) and distance (magnitude).‚Äù</p>
</div></blockquote>
</section>
<hr class="docutils" />
<section id="id1">
<h2>Conclusion<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>Vectors are the starting point for any ML pipeline, translating raw data into a computable form. Mastering vector operations is essential for building and understanding machine learning models.</p>
</section>
</section>
<hr class="docutils" />
<section id="matrices-the-foundation-of-data">
<h1>üåü Matrices: The Foundation of Data<a class="headerlink" href="#matrices-the-foundation-of-data" title="Link to this heading">#</a></h1>
<p>A <strong>matrix</strong> is a rectangular array of numbers organized into rows and columns. In ML and DL, matrices are essential for representing data, such as datasets or transformations.</p>
<section id="what-are-matrices">
<h2>What Are Matrices?<a class="headerlink" href="#what-are-matrices" title="Link to this heading">#</a></h2>
<p>Matrices are two-dimensional grids that store numbers systematically. For example, in a dataset, rows might represent individual samples (like people), and columns might represent features (like age or height).</p>
<p><strong>Example</strong>: A 2√ó2 matrix in Python using NumPy:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np
X = np.array([[1, 2], [3, 4]])
print(X)
</pre></div>
</div>
</div>
</div>
</section>
<section id="types-of-matrices">
<h2>Types of Matrices<a class="headerlink" href="#types-of-matrices" title="Link to this heading">#</a></h2>
<p>Matrices come in different forms, each with specific uses:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Type</p></th>
<th class="head"><p>Definition</p></th>
<th class="head"><p>Example</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Square</strong></p></td>
<td><p>Equal rows and columns</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">[[1,</span> <span class="pre">2],</span> <span class="pre">[3,</span> <span class="pre">4]]</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><strong>Rectangular</strong></p></td>
<td><p>Unequal rows and columns</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">[[1,</span> <span class="pre">2,</span> <span class="pre">3],</span> <span class="pre">[4,</span> <span class="pre">5,</span> <span class="pre">6]]</span></code></p></td>
</tr>
<tr class="row-even"><td><p><strong>Row</strong></p></td>
<td><p>Single row</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">[[1,</span> <span class="pre">2,</span> <span class="pre">3]]</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><strong>Column</strong></p></td>
<td><p>Single column</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">[[1],</span> <span class="pre">[2],</span> <span class="pre">[3]]</span></code></p></td>
</tr>
<tr class="row-even"><td><p><strong>Zero</strong></p></td>
<td><p>All elements are 0</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">[[0,</span> <span class="pre">0],</span> <span class="pre">[0,</span> <span class="pre">0]]</span></code></p></td>
</tr>
<tr class="row-odd"><td><p><strong>Identity</strong></p></td>
<td><p>1s on diagonal, 0s elsewhere</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">[[1,</span> <span class="pre">0],</span> <span class="pre">[0,</span> <span class="pre">1]]</span></code></p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="key-operations">
<h2>Key Operations<a class="headerlink" href="#key-operations" title="Link to this heading">#</a></h2>
<p>Matrices support operations critical to ML:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Operation</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Python Code</p></th>
<th class="head"><p>ML Use Case</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Addition</strong></p></td>
<td><p>Adds corresponding elements</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">np.add(A,</span> <span class="pre">B)</span></code></p></td>
<td><p>Combining datasets</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Multiplication</strong></p></td>
<td><p>Combines rows and columns</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">np.matmul(A,</span> <span class="pre">B)</span></code></p></td>
<td><p>Neural network layers</p></td>
</tr>
<tr class="row-even"><td><p><strong>Transpose</strong></p></td>
<td><p>Flips rows and columns</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">A.T</span></code></p></td>
<td><p>Data reshaping</p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>Example</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>A = np.array([[1, 2], [3, 4]])
B = np.array([[5, 6], [7, 8]])
print(&quot;Addition:\n&quot;, A + B)
print(&quot;Multiplication:\n&quot;, A @ B)
print(&quot;Transpose:\n&quot;, A.T)
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="special-matrices-and-properties">
<h2>Special Matrices and Properties<a class="headerlink" href="#special-matrices-and-properties" title="Link to this heading">#</a></h2>
<p>Some matrices have unique traits:</p>
<ul class="simple">
<li><p><strong>Identity Matrix</strong>: A square matrix with 1s on the diagonal and 0s elsewhere. It leaves other matrices unchanged when multiplied (<code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">√ó</span> <span class="pre">I</span> <span class="pre">=</span> <span class="pre">A</span></code>).</p></li>
<li><p><strong>Inverse Matrix</strong>: For a square matrix <code class="docutils literal notranslate"><span class="pre">A</span></code>, its inverse <code class="docutils literal notranslate"><span class="pre">A‚Åª¬π</span></code> satisfies <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">√ó</span> <span class="pre">A‚Åª¬π</span> <span class="pre">=</span> <span class="pre">I</span></code>. Useful for solving equations.</p></li>
<li><p><strong>Determinant</strong>: A number showing if a matrix is invertible (non-zero means invertible).</p></li>
<li><p><strong>Rank</strong>: The number of independent rows or columns, indicating a matrix‚Äôs information content.</p></li>
</ul>
<p><strong>Example</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>I = np.eye(2)  # 2√ó2 Identity Matrix
print(&quot;Identity:\n&quot;, I)

A = np.array([[4, 7], [2, 6]])
A_inv = np.linalg.inv(A)
print(&quot;Inverse:\n&quot;, A_inv)

rank = np.linalg.matrix_rank(A)
print(&quot;Rank:&quot;, rank)
</pre></div>
</div>
</div>
</div>
</section>
<section id="connection-to-ml">
<h2>Connection to ML<a class="headerlink" href="#connection-to-ml" title="Link to this heading">#</a></h2>
<p>Matrices are everywhere in ML:</p>
<ul class="simple">
<li><p><strong>Data Representation</strong>: Datasets are stored as matrices.</p></li>
<li><p><strong>Transformations</strong>: Matrix multiplication drives neural network layers.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="matrix-factorization-revealing-hidden-structures">
<h1>Matrix Factorization: Revealing Hidden Structures<a class="headerlink" href="#matrix-factorization-revealing-hidden-structures" title="Link to this heading">#</a></h1>
<p><strong>Matrix factorization</strong> breaks a matrix into simpler parts to uncover patterns or reduce complexity. It‚Äôs widely used in ML for tasks like recommendation systems and data compression.</p>
<section id="what-is-matrix-factorization">
<h2>What is Matrix Factorization?<a class="headerlink" href="#what-is-matrix-factorization" title="Link to this heading">#</a></h2>
<p>Matrix factorization decomposes a matrix into components, such as eigenvalues or singular values, to simplify data while keeping its essence.</p>
<p><strong>Example</strong>: In a recommendation system, it predicts missing ratings in a user-item matrix:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>User-Item Matrix:
        Item 1  Item 2  Item 3
User 1     5       ?      3
User 2     ?       4      ?
User 3     2       ?      5
</pre></div>
</div>
</div>
</div>
</section>
<section id="key-techniques">
<h2>Key Techniques<a class="headerlink" href="#key-techniques" title="Link to this heading">#</a></h2>
<section id="singular-value-decomposition-svd">
<h3>Singular Value Decomposition (SVD)<a class="headerlink" href="#singular-value-decomposition-svd" title="Link to this heading">#</a></h3>
<p>SVD splits a matrix <code class="docutils literal notranslate"><span class="pre">A</span></code> into three parts: <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">=</span> <span class="pre">U</span> <span class="pre">Œ£</span> <span class="pre">V·µÄ</span></code>.</p>
<ul class="simple">
<li><p><strong>U</strong>: Left singular vectors.</p></li>
<li><p><strong>Œ£</strong>: Diagonal matrix of singular values.</p></li>
<li><p><strong>V·µÄ</strong>: Right singular vectors (transposed).</p></li>
</ul>
<p><strong>Uses</strong>:</p>
<ul class="simple">
<li><p>Recommendation systems.</p></li>
<li><p>Image compression.</p></li>
</ul>
<p><strong>Example</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>A = np.array([[-1, 2], [3, -2], [5, 7]])
U, s, VT = np.linalg.svd(A)
print(&quot;Singular Values:&quot;, s)
</pre></div>
</div>
</div>
</div>
<p>Output:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>Singular Values: [8.71 2.24]
</pre></div>
</div>
</div>
</div>
</section>
<section id="eigen-decomposition">
<h3>Eigen Decomposition<a class="headerlink" href="#eigen-decomposition" title="Link to this heading">#</a></h3>
<p>For a square matrix <code class="docutils literal notranslate"><span class="pre">A</span></code>, eigen decomposition is <code class="docutils literal notranslate"><span class="pre">A</span> <span class="pre">=</span> <span class="pre">V</span> <span class="pre">Œõ</span> <span class="pre">V‚Åª¬π</span></code>.</p>
<ul class="simple">
<li><p><strong>V</strong>: Matrix of eigenvectors.</p></li>
<li><p><strong>Œõ</strong>: Diagonal matrix of eigenvalues.</p></li>
</ul>
<p><strong>Use</strong>: Principal Component Analysis (PCA).</p>
<p><strong>Example</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>A = np.array([[4, 2], [-5, -3]])
eigenvalues, eigenvectors = np.linalg.eig(A)
print(&quot;Eigenvalues:&quot;, eigenvalues)
</pre></div>
</div>
</div>
</div>
</section>
<section id="id2">
<h3>Principal Component Analysis (PCA)<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<p>PCA reduces data dimensions by finding the most important directions (principal components).</p>
<p><strong>Steps</strong>:</p>
<ol class="arabic simple">
<li><p>Standardize the data.</p></li>
<li><p>Compute the covariance matrix.</p></li>
<li><p>Find eigenvalues and eigenvectors.</p></li>
<li><p>Project data onto top components.</p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>A = np.array([[4,2], [-5,-3]])

eigenvalues, eigenvectors = np.linalg.eig(A)
eigenvector_inverse = np.linalg.inv(eigenvectors)
eigenvalue_diagonal = np.diag(eigenvalues)

# Verify A = V * diagonal matrix * V‚Åª¬π
C = np.dot(eigenvalue_diagonal, eigenvector_inverse)
C
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Calculate PCA components manually
cov_matrix = X.T @ X
eigen_values, eigen_vectors = np.linalg.eig(cov_matrix)
</pre></div>
</div>
</div>
</div>
<p><strong>Example</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from sklearn.decomposition import PCA
X = np.array([[1, 2], [3, 4], [5, 6]])
pca = PCA(n_components=1)
X_reduced = pca.fit_transform(X)
print(X_reduced)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="id3">
<h2>Connection to ML<a class="headerlink" href="#id3" title="Link to this heading">#</a></h2>
<p>Matrix factorization powers:</p>
<ul class="simple">
<li><p><strong>Dimensionality Reduction</strong>: PCA simplifies datasets.</p></li>
<li><p><strong>Recommendations</strong>: SVD predicts user preferences.</p></li>
</ul>
<hr class="docutils" />
<section id="the-moore-penrose-pseudoinverse-mpp">
<h3>4Ô∏è‚É£ <strong>The Moore-Penrose Pseudoinverse (MPP)</strong><a class="headerlink" href="#the-moore-penrose-pseudoinverse-mpp" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Used for matrices that <strong>aren‚Äôt square</strong>, commonly in machine learning applications.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>A = np.array([[-1,2], [3,-2],[5,7]])

mpp = np.linalg.pinv(A)
mpp
</pre></div>
</div>
</div>
</div>
</section>
<section id="trace-operator">
<h3>5Ô∏è‚É£ <strong>Trace Operator</strong><a class="headerlink" href="#trace-operator" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>The sum of a matrix‚Äôs diagonal elements.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>A = np.array([[18,2], [5,6]])
np.trace(A)  # Output: 24
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<hr class="docutils" />
<section id="tensors-beyond-matrices">
<h1>Tensors: Beyond Matrices<a class="headerlink" href="#tensors-beyond-matrices" title="Link to this heading">#</a></h1>
<p><strong>Tensors</strong> extend vectors and matrices to higher dimensions, making them vital for DL tasks like image and video processing.</p>
<section id="what-are-tensors">
<h2>What Are Tensors?<a class="headerlink" href="#what-are-tensors" title="Link to this heading">#</a></h2>
<p>Tensors are multi-dimensional arrays:</p>
<ul class="simple">
<li><p><strong>0D</strong>: Scalar (e.g., <code class="docutils literal notranslate"><span class="pre">5</span></code>).</p></li>
<li><p><strong>1D</strong>: Vector (e.g., <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">3]</span></code>).</p></li>
<li><p><strong>2D</strong>: Matrix (e.g., <code class="docutils literal notranslate"><span class="pre">[[1,</span> <span class="pre">2],</span> <span class="pre">[3,</span> <span class="pre">4]]</span></code>).</p></li>
<li><p><strong>3D+</strong>: Higher dimensions (e.g., image data).</p></li>
</ul>
</section>
<section id="tensor-hierarchy">
<h2>Tensor Hierarchy<a class="headerlink" href="#tensor-hierarchy" title="Link to this heading">#</a></h2>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Dimension</p></th>
<th class="head"><p>Name</p></th>
<th class="head"><p>Example</p></th>
<th class="head"><p>DL Use</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0D</p></td>
<td><p>Scalar</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">5</span></code></p></td>
<td><p>Single value</p></td>
</tr>
<tr class="row-odd"><td><p>1D</p></td>
<td><p>Vector</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">3]</span></code></p></td>
<td><p>Feature list</p></td>
</tr>
<tr class="row-even"><td><p>2D</p></td>
<td><p>Matrix</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">[[1,</span> <span class="pre">2],</span> <span class="pre">[3,</span> <span class="pre">4]]</span></code></p></td>
<td><p>Dataset</p></td>
</tr>
<tr class="row-odd"><td><p>3D</p></td>
<td><p>Tensor</p></td>
<td><p>Image (H√óW√óC)</p></td>
<td><p>Image processing</p></td>
</tr>
<tr class="row-even"><td><p>4D</p></td>
<td><p>Tensor</p></td>
<td><p>Video (T√óH√óW√óC)</p></td>
<td><p>Video analysis</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="deep-learning-applications">
<h2>Deep Learning Applications<a class="headerlink" href="#deep-learning-applications" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Images</strong>: 3D tensors (height √ó width √ó channels, e.g., RGB).</p></li>
<li><p><strong>Videos</strong>: 4D tensors (frames √ó height √ó width √ó channels).</p></li>
</ul>
<p><strong>Example</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import torch
tensor_4d = torch.rand(16, 3, 224, 224)  # Batch of 16 RGB images
print(tensor_4d.shape)
</pre></div>
</div>
</div>
</div>
<p>Output:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>torch.Size([16, 3, 224, 224])
</pre></div>
</div>
</div>
</div>
</section>
<section id="connection-to-dl">
<h2>Connection to DL<a class="headerlink" href="#connection-to-dl" title="Link to this heading">#</a></h2>
<p>Tensors enable:</p>
<ul class="simple">
<li><p><strong>Convolutional Neural Networks (CNNs)</strong>: Process image tensors.</p></li>
<li><p><strong>Recurrent Neural Networks (RNNs)</strong>: Handle sequence tensors (e.g., text).</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="why-linear-algebra-matters">
<h1>Why Linear Algebra Matters<a class="headerlink" href="#why-linear-algebra-matters" title="Link to this heading">#</a></h1>
<p>Linear algebra connects these concepts to ML and DL:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Concept</p></th>
<th class="head"><p>Application</p></th>
<th class="head"><p>Impact</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Matrices</strong></p></td>
<td><p>Data storage</p></td>
<td><p>Organizes datasets</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Multiplication</strong></p></td>
<td><p>Neural layers</p></td>
<td><p>Computes transformations</p></td>
</tr>
<tr class="row-even"><td><p><strong>Factorization</strong></p></td>
<td><p>PCA, SVD</p></td>
<td><p>Reduces dimensions</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Tensors</strong></p></td>
<td><p>CNNs, RNNs</p></td>
<td><p>Handles complex data</p></td>
</tr>
</tbody>
</table>
</div>
<section id="key-takeaways">
<h2>Key Takeaways<a class="headerlink" href="#key-takeaways" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Matrices</strong> structure data and transformations.</p></li>
<li><p><strong>Matrix Factorization</strong> simplifies data for analysis.</p></li>
<li><p><strong>Tensors</strong> power DL by managing multi-dimensional data.</p></li>
<li><p><strong>Efficiency</strong>: Libraries like NumPy and PyTorch speed up computations.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="linear-transformations-in-machine-learning">
<h1>Linear Transformations in Machine Learning<a class="headerlink" href="#linear-transformations-in-machine-learning" title="Link to this heading">#</a></h1>
<p>Linear transformations are essential tools in <strong>machine learning (ML)</strong>, <strong>deep learning (DL)</strong>, <strong>computer vision</strong>, and <strong>3D graphics</strong>. They allow us to modify data, such as images or feature vectors, while preserving linearity. This makes them useful for tasks like scaling, rotation, shearing, and reflection, which are critical in areas like image augmentation, feature engineering, and model optimization.</p>
<hr class="docutils" />
<section id="what-are-linear-transformations">
<h2>What Are Linear Transformations?<a class="headerlink" href="#what-are-linear-transformations" title="Link to this heading">#</a></h2>
<p>A linear transformation is a function that maps vectors from one space to another while preserving two key properties: <strong>addition</strong> and <strong>scalar multiplication</strong>. In simpler terms, it transforms data using matrix multiplication, keeping the operations linear.</p>
<section id="key-properties">
<h3>Key Properties<a class="headerlink" href="#key-properties" title="Link to this heading">#</a></h3>
<p>For a transformation ( T ), it must satisfy:</p>
<ul class="simple">
<li><p>( T(\mathbf{u} + \mathbf{v}) = T(\mathbf{u}) + T(\mathbf{v}) ) (preserves addition)</p></li>
<li><p>( T(k \mathbf{u}) = k T(\mathbf{u}) ) (preserves scalar multiplication)</p></li>
</ul>
<p>Here, ( \mathbf{u} ) and ( \mathbf{v} ) are vectors, and ( k ) is a scalar.</p>
</section>
<section id="how-it-works">
<h3>How It Works<a class="headerlink" href="#how-it-works" title="Link to this heading">#</a></h3>
<p>In practice, linear transformations are represented by matrices. For a vector ( \mathbf{v} ), the transformed vector is:
[
T(\mathbf{v}) = A \mathbf{v}
]
where ( A ) is the transformation matrix.</p>
<p>For example, if you want to scale or rotate an image, you can apply a specific transformation matrix to the image‚Äôs pixel coordinates.</p>
</section>
</section>
<hr class="docutils" />
<section id="common-linear-transformations">
<h2>Common Linear Transformations<a class="headerlink" href="#common-linear-transformations" title="Link to this heading">#</a></h2>
<p>Below, we explore four common linear transformations used in ML and DL. Each includes:</p>
<ul class="simple">
<li><p>A description of the transformation</p></li>
<li><p>Its matrix representation</p></li>
<li><p>A Python example with visualization</p></li>
</ul>
<p>All code examples use NumPy and Matplotlib for clarity and are formatted for easy understanding.</p>
<section id="scaling-adjusting-size">
<h3>1. Scaling ‚Äì Adjusting Size<a class="headerlink" href="#scaling-adjusting-size" title="Link to this heading">#</a></h3>
<p>Scaling changes the size of an object by enlarging or shrinking it while keeping its proportions. In ML, scaling is used for:</p>
<ul class="simple">
<li><p><strong>Feature scaling</strong> in preprocessing (e.g., normalizing data)</p></li>
<li><p><strong>Image augmentation</strong> to create new training samples</p></li>
</ul>
<section id="transformation-matrix">
<h4>Transformation Matrix<a class="headerlink" href="#transformation-matrix" title="Link to this heading">#</a></h4>
<p>For 2D scaling:
[
S = \begin{bmatrix} s_x &amp; 0 \ 0 &amp; s_y \end{bmatrix}
]</p>
<ul class="simple">
<li><p>( s_x ): Scaling factor for the x-axis</p></li>
<li><p>( s_y ): Scaling factor for the y-axis</p></li>
</ul>
</section>
<section id="python-example">
<h4>Python Example<a class="headerlink" href="#python-example" title="Link to this heading">#</a></h4>
<p>Let‚Äôs scale a square by a factor of 2 in both directions.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np
import matplotlib.pyplot as plt

# Original square points (coordinates)
square = np.array([[0, 0], [1, 0], [1, 1], [0, 1], [0, 0]])

# Scaling matrix (scale by 2)
S = np.array([[2, 0], [0, 2]])

# Apply transformation (matrix multiplication)
scaled_square = square @ S.T

# Plot
plt.figure(figsize=(6,6))
plt.plot(square[:, 0], square[:, 1], label=&quot;Original&quot;, linestyle=&quot;--&quot;, marker=&quot;o&quot;)
plt.plot(scaled_square[:, 0], scaled_square[:, 1], label=&quot;Scaled&quot;, linestyle=&quot;-&quot;, marker=&quot;s&quot;)
plt.xlim(-3, 3)
plt.ylim(-3, 3)
plt.axhline(0, color=&#39;black&#39;, linewidth=0.5)
plt.axvline(0, color=&#39;black&#39;, linewidth=0.5)
plt.grid(True)
plt.legend()
plt.title(&quot;Scaling Transformation&quot;)
plt.show()
</pre></div>
</div>
</div>
</div>
</section>
<section id="explanation">
<h4>Explanation<a class="headerlink" href="#explanation" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>The original square has corners at (0,0), (1,0), (1,1), and (0,1).</p></li>
<li><p>After scaling, the square is enlarged uniformly by a factor of 2.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="rotation-spinning-objects">
<h3>2. Rotation ‚Äì Spinning Objects<a class="headerlink" href="#rotation-spinning-objects" title="Link to this heading">#</a></h3>
<p>Rotation turns an object around a fixed point, typically the origin. In DL, rotation is used for:</p>
<ul class="simple">
<li><p><strong>Image augmentation</strong> to make models robust to different orientations</p></li>
<li><p><strong>Feature alignment</strong> in tasks like object detection</p></li>
</ul>
<section id="id4">
<h4>Transformation Matrix<a class="headerlink" href="#id4" title="Link to this heading">#</a></h4>
<p>For 2D rotation by angle ( \theta ) (in radians):
[
R = \begin{bmatrix} \cos\theta &amp; -\sin\theta \ \sin\theta &amp; \cos\theta \end{bmatrix}
]</p>
</section>
<section id="id5">
<h4>Python Example<a class="headerlink" href="#id5" title="Link to this heading">#</a></h4>
<p>Let‚Äôs rotate a square by 30 degrees counterclockwise.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Rotation matrix (30 degrees)
theta = np.radians(30)
R = np.array([[np.cos(theta), -np.sin(theta)], [np.sin(theta), np.cos(theta)]])

# Apply transformation
rotated_square = square @ R.T

# Plot
plt.figure(figsize=(6,6))
plt.plot(square[:, 0], square[:, 1], label=&quot;Original&quot;, linestyle=&quot;--&quot;, marker=&quot;o&quot;)
plt.plot(rotated_square[:, 0], rotated_square[:, 1], label=&quot;Rotated&quot;, linestyle=&quot;-&quot;, marker=&quot;s&quot;)
plt.xlim(-2, 2)
plt.ylim(-2, 2)
plt.axhline(0, color=&#39;black&#39;, linewidth=0.5)
plt.axvline(0, color=&#39;black&#39;, linewidth=0.5)
plt.grid(True)
plt.legend()
plt.title(&quot;Rotation Transformation (30¬∞)&quot;)
plt.show()
</pre></div>
</div>
</div>
</div>
</section>
<section id="id6">
<h4>Explanation<a class="headerlink" href="#id6" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>The square is rotated counterclockwise by 30 degrees.</p></li>
<li><p>The rotation matrix uses sine and cosine functions to achieve this.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="shearing-the-leaning-tower-effect">
<h3>3. Shearing ‚Äì The ‚ÄúLeaning Tower‚Äù Effect<a class="headerlink" href="#shearing-the-leaning-tower-effect" title="Link to this heading">#</a></h3>
<p>Shearing tilts an object by shifting one axis relative to the other. In ML, shearing is used for:</p>
<ul class="simple">
<li><p><strong>Data augmentation</strong> to simulate perspective changes</p></li>
<li><p><strong>Feature transformation</strong> in tasks like text recognition</p></li>
</ul>
<section id="id7">
<h4>Transformation Matrix<a class="headerlink" href="#id7" title="Link to this heading">#</a></h4>
<p>For 2D shearing along the x-axis:
[
H = \begin{bmatrix} 1 &amp; k \ 0 &amp; 1 \end{bmatrix}
]</p>
<ul class="simple">
<li><p>( k ): Shear factor (controls the tilt)</p></li>
</ul>
</section>
<section id="id8">
<h4>Python Example<a class="headerlink" href="#id8" title="Link to this heading">#</a></h4>
<p>Let‚Äôs shear a square with a shear factor of 0.5.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Shearing matrix (k=0.5)
k = 0.5
H = np.array([[1, k], [0, 1]])

# Apply transformation
sheared_square = square @ H.T

# Plot
plt.figure(figsize=(6,6))
plt.plot(square[:, 0], square[:, 1], label=&quot;Original&quot;, linestyle=&quot;--&quot;, marker=&quot;o&quot;)
plt.plot(sheared_square[:, 0], sheared_square[:, 1], label=&quot;Sheared&quot;, linestyle=&quot;-&quot;, marker=&quot;s&quot;)
plt.xlim(-2, 2)
plt.ylim(-2, 2)
plt.axhline(0, color=&#39;black&#39;, linewidth=0.5)
plt.axvline(0, color=&#39;black&#39;, linewidth=0.5)
plt.grid(True)
plt.legend()
plt.title(&quot;Shearing Transformation&quot;)
plt.show()
</pre></div>
</div>
</div>
</div>
</section>
<section id="id9">
<h4>Explanation<a class="headerlink" href="#id9" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>The square is tilted to the right, creating a parallelogram.</p></li>
<li><p>Shearing shifts the x-coordinates based on the y-coordinates.</p></li>
</ul>
</section>
</section>
<hr class="docutils" />
<section id="reflection-the-mirror-effect">
<h3>4. Reflection ‚Äì The Mirror Effect<a class="headerlink" href="#reflection-the-mirror-effect" title="Link to this heading">#</a></h3>
<p>Reflection flips an object across an axis, creating a mirror image. In ML, reflection is used for:</p>
<ul class="simple">
<li><p><strong>Data augmentation</strong> in image classification</p></li>
<li><p><strong>Symmetry analysis</strong> in tasks like facial recognition</p></li>
</ul>
<section id="id10">
<h4>Transformation Matrix<a class="headerlink" href="#id10" title="Link to this heading">#</a></h4>
<p>For reflection over the x-axis:
[
F_x = \begin{bmatrix} 1 &amp; 0 \ 0 &amp; -1 \end{bmatrix}
]</p>
</section>
<section id="id11">
<h4>Python Example<a class="headerlink" href="#id11" title="Link to this heading">#</a></h4>
<p>Let‚Äôs reflect a square over the x-axis.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Reflection matrix (over x-axis)
F_x = np.array([[1, 0], [0, -1]])

# Apply transformation
reflected_square = square @ F_x.T

# Plot
plt.figure(figsize=(6,6))
plt.plot(square[:, 0], square[:, 1], label=&quot;Original&quot;, linestyle=&quot;--&quot;, marker=&quot;o&quot;)
plt.plot(reflected_square[:, 0], reflected_square[:, 1], label=&quot;Reflected&quot;, linestyle=&quot;-&quot;, marker=&quot;s&quot;)
plt.xlim(-2, 2)
plt.ylim(-2, 2)
plt.axhline(0, color=&#39;black&#39;, linewidth=0.5)
plt.axvline(0, color=&#39;black&#39;, linewidth=0.5)
plt.grid(True)
plt.legend()
plt.title(&quot;Reflection Across X-Axis&quot;)
plt.show()
</pre></div>
</div>
</div>
</div>
</section>
<section id="id12">
<h4>Explanation<a class="headerlink" href="#id12" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p>The square is flipped upside down across the x-axis.</p></li>
<li><p>The y-coordinates are negated, creating the mirror effect.</p></li>
</ul>
</section>
</section>
</section>
<hr class="docutils" />
<section id="summary-of-common-transformations">
<h2>Summary of Common Transformations<a class="headerlink" href="#summary-of-common-transformations" title="Link to this heading">#</a></h2>
<p>Here‚Äôs a quick reference for the transformation matrices:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Transformation</p></th>
<th class="head"><p>Matrix Representation</p></th>
<th class="head"><p>Purpose in ML/DL</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Scaling</strong></p></td>
<td><p>[ \begin{bmatrix} s_x &amp; 0 \ 0 &amp; s_y \end{bmatrix} ]</p></td>
<td><p>Feature scaling, image augmentation</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Rotation</strong> (Œ∏¬∞)</p></td>
<td><p>[ \begin{bmatrix} \cos\theta &amp; -\sin\theta \ \sin\theta &amp; \cos\theta \end{bmatrix} ]</p></td>
<td><p>Image augmentation, feature alignment</p></td>
</tr>
<tr class="row-even"><td><p><strong>Shearing</strong> (x-axis)</p></td>
<td><p>[ \begin{bmatrix} 1 &amp; k \ 0 &amp; 1 \end{bmatrix} ]</p></td>
<td><p>Data augmentation, perspective simulation</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Reflection</strong> (x-axis)</p></td>
<td><p>[ \begin{bmatrix} 1 &amp; 0 \ 0 &amp; -1 \end{bmatrix} ]</p></td>
<td><p>Data augmentation, symmetry analysis</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<hr class="docutils" />
<section id="id13">
<h2>Applications in Machine Learning<a class="headerlink" href="#id13" title="Link to this heading">#</a></h2>
<p>Linear transformations are not just theoretical‚Äîthey power many practical tasks in ML and DL:</p>
<ul class="simple">
<li><p><strong>Image Augmentation</strong>:</p>
<ul>
<li><p>Scaling, rotation, and reflection create new training samples.</p></li>
<li><p>This improves model generalization by exposing it to diverse data.</p></li>
</ul>
</li>
<li><p><strong>Feature Engineering</strong>:</p>
<ul>
<li><p>Transformations align features or reduce dimensions.</p></li>
<li><p>For example, Principal Component Analysis (PCA) uses rotations to find principal components.</p></li>
</ul>
</li>
<li><p><strong>Model Optimization</strong>:</p>
<ul>
<li><p>Understanding transformations helps design efficient architectures.</p></li>
<li><p>It also aids in debugging models by analyzing data flow.</p></li>
</ul>
</li>
</ul>
<p>By mastering linear transformations, you can manipulate data effectively, leading to better model performance and insights.</p>
</section>
<hr class="docutils" />
<section id="final-notes">
<h2>Final Notes<a class="headerlink" href="#final-notes" title="Link to this heading">#</a></h2>
<p>By connecting linear transformations to real-world ML applications, we ensure relevance and practical value. Whether you‚Äôre working on image processing, feature engineering, or model optimization, these concepts will help you succeed.</p>
<hr class="docutils" />
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./contents\0_maths"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="0_essential.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Essential Maths</p>
      </div>
    </a>
    <a class="right-next"
       href="2_probability.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Probability in ML</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Linear Algebra</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-algebra-the-foundation-of-ml-dl">Linear Algebra: The Foundation of ML &amp; DL</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-linear-algebra">Introduction to Linear Algebra</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#core-concepts">Core Concepts</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-solving-techniques">Problem-Solving Techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#methods-to-solve-ax-b">Methods to Solve ( Ax = b )</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#solution-types">Solution Types</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#applications-in-machine-learning">Applications in Machine Learning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression">1. Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#principal-component-analysis-pca">2. Principal Component Analysis (PCA)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks">3. Neural Networks</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#non-linear-vs-linear-systems">Non-Linear vs Linear Systems</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-linear-algebra-matters-in-ai-ml">Why Linear Algebra Matters in AI/ML</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#critical-linear-algebra-operations">Critical Linear Algebra Operations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#vectors-the-building-blocks">Vectors: The Building Blocks</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction-to-vectors">1. Introduction to Vectors</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#essential-vector-properties">2. Essential Vector Properties</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#magnitude-length">Magnitude (Length)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#direction">Direction</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vector-types">Vector Types</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fundamental-vector-operations">3. Fundamental Vector Operations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-operations">Basic Operations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dot-product">Dot Product</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#norm-comparisons">Norm Comparisons</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#machine-learning-applications">4. Machine Learning Applications</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feature-representation">Feature Representation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#similarity-analysis">Similarity Analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#geometric-transformations">Geometric Transformations</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-vectors-matter-in-ml">5. Why Vectors Matter in ML</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Conclusion</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#matrices-the-foundation-of-data">üåü Matrices: The Foundation of Data</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-matrices">What Are Matrices?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-matrices">Types of Matrices</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-operations">Key Operations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#special-matrices-and-properties">Special Matrices and Properties</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-ml">Connection to ML</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#matrix-factorization-revealing-hidden-structures">Matrix Factorization: Revealing Hidden Structures</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-matrix-factorization">What is Matrix Factorization?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-techniques">Key Techniques</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#singular-value-decomposition-svd">Singular Value Decomposition (SVD)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#eigen-decomposition">Eigen Decomposition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">Principal Component Analysis (PCA)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Connection to ML</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-moore-penrose-pseudoinverse-mpp">4Ô∏è‚É£ <strong>The Moore-Penrose Pseudoinverse (MPP)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#trace-operator">5Ô∏è‚É£ <strong>Trace Operator</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#tensors-beyond-matrices">Tensors: Beyond Matrices</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-tensors">What Are Tensors?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tensor-hierarchy">Tensor Hierarchy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-applications">Deep Learning Applications</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#connection-to-dl">Connection to DL</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#why-linear-algebra-matters">Why Linear Algebra Matters</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-takeaways">Key Takeaways</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-transformations-in-machine-learning">Linear Transformations in Machine Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-linear-transformations">What Are Linear Transformations?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-properties">Key Properties</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#how-it-works">How It Works</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#common-linear-transformations">Common Linear Transformations</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#scaling-adjusting-size">1. Scaling ‚Äì Adjusting Size</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#transformation-matrix">Transformation Matrix</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#python-example">Python Example</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation">Explanation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rotation-spinning-objects">2. Rotation ‚Äì Spinning Objects</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">Transformation Matrix</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">Python Example</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Explanation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#shearing-the-leaning-tower-effect">3. Shearing ‚Äì The ‚ÄúLeaning Tower‚Äù Effect</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">Transformation Matrix</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">Python Example</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">Explanation</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reflection-the-mirror-effect">4. Reflection ‚Äì The Mirror Effect</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">Transformation Matrix</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">Python Example</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">Explanation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#summary-of-common-transformations">Summary of Common Transformations</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">Applications in Machine Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#final-notes">Final Notes</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gajanesh
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright ¬© 2025 Gajanesh. All rights reserved..
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>