
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Calculus &#8212; Data Science Books</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-dropdown.css?v=995e94df" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-bootstrap.min.css?v=21c0b90a" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=d567e03f" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'contents/0_maths/5_calculus';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Explanatory and Response Variables" href="6_regression_analysis.html" />
    <link rel="prev" title="Inferential Statistics" href="3_inferential.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Data Science Books</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Machine Learning
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Part I ‚Äî Foundations</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="0_essential.html">Essential Mathematics</a></li>
<li class="toctree-l1"><a class="reference internal" href="4_linear_algebra.html">Linear Algebra</a></li>







<li class="toctree-l1"><a class="reference internal" href="2_probability.html">Probability Theory</a></li>
<li class="toctree-l1"><a class="reference internal" href="1_descriptive.html">Descriptive Statistics</a></li>








<li class="toctree-l1"><a class="reference internal" href="3_inferential.html">Inferential Statistics</a></li>



<li class="toctree-l1 current active"><a class="current reference internal" href="#">Calculus</a></li>







<li class="toctree-l1"><a class="reference internal" href="6_regression_analysis.html">Explanatory and Response Variables</a></li>


<li class="toctree-l1"><a class="reference internal" href="../1_python/1_basics.html">Python Basics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/2_advanced.html">Advanced Python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/3_data_structures.html">Data Structures</a></li>





<li class="toctree-l1"><a class="reference internal" href="../1_python/4_modules_packages.html">Modules &amp; Packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/5_functions.html">Functions &amp; Modular Programming</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/6_oop.html">Object-Oriented Programming</a></li>





<li class="toctree-l1"><a class="reference internal" href="../1_python/8_exceptions.html">Exception Management</a></li>
<li class="toctree-l1"><a class="reference internal" href="../1_python/9_regex.html">Regular Expressions</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_numpy/1_numpy.html">NumPy (<strong>Numerical Python</strong>)</a></li>



<li class="toctree-l1"><a class="reference internal" href="../2_pandas/1_series.html">Pandas Series</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_pandas/2_dataframes.html">Pandas DataFrame</a></li>
















<li class="toctree-l1"><a class="reference internal" href="../2_pandas/3_visualization.html"><strong>What is Data Visualization in Data Science?</strong></a></li>


<li class="toctree-l1"><a class="reference internal" href="../2_pandas/4_eda.html">Exploratory Data Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../2_pandas/5_feature_engineering.html"><strong>What is Feature Engineering?</strong></a></li>


</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part II ‚Äî Classical Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../3_ml/1_foundations.html">ML Foundational</a></li>





<li class="toctree-l1"><a class="reference internal" href="../3_ml/2_data_preparation.html">2Ô∏è‚É£ Data Handling</a></li>





<li class="toctree-l1"><a class="reference internal" href="../3_ml/2_train_test_split.html">Train‚ÄìTest Split</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/5_model_evaluation.html">4Ô∏è‚É£ Model Evaluation</a></li>




<li class="toctree-l1"><a class="reference internal" href="../3_ml/11_supervised_learning.html"><strong>Supervised Learning</strong></a></li>



<li class="toctree-l1"><a class="reference internal" href="../3_ml/12_regression.html">Regression Algorithms</a></li>







<li class="toctree-l1"><a class="reference internal" href="../3_ml/13_classification.html">Classification Algorithms</a></li>

<li class="toctree-l1"><a class="reference internal" href="../3_ml/10_core_algo.html">Core ML Algorithms</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/14_decision_trees.html">Decision Trees</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/15_ensemble_methods.html">Ensemble Methods</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/16_svm.html">Support Vector Machine (SVM) in Detail</a></li>


<li class="toctree-l1"><a class="reference internal" href="../3_ml/17_knn.html">k-Nearest Neighbors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/18_naive_bayes.html">Naive Bayes</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part III ‚Äî Advanced Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../3_ml/20_unsupervised_learning.html">Unsupervised Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../3_ml/21_clustering.html">Clustering Techniques</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../3_ml/7_optimization_and_training.html">5Ô∏è‚É£ Optimization &amp; Training</a></li>







<li class="toctree-l1 has-children"><a class="reference internal" href="../3_ml/6_ml_lifecycle.html">ML Lifecycle</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../3_ml/6_training.html">Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3_ml/6_evaluation.html">Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../3_ml/6_deployment.html">Deployment</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part IV ‚Äî Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl1_Introduction.html">Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl2_Neuron.html">Neuron</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl3_Libraries.html">Deep Learning Libraries</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl4_Terminology.html">Terminology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl5_multi_layer.html">Multi-Layer Perceptron</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl6_first_nn.html">First Neural Network</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl7_evaluating_model.html">Model Evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl8_multiclass_classification.html">Multiclass Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl9_multiclass_classification_hand.html">Handwritten Digit Classification</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl10_saving_and_loading.html">Saving &amp; Loading Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl11_checkpointing.html">Model Checkpointing</a></li>
<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl12_visualizing_model_training.html"><strong>Visualizing Model Training History in Deep Learning</strong></a></li>

<li class="toctree-l1"><a class="reference internal" href="../4_dl/dl13_loss_functions_activation_functions_and_optimizers.html">Loss Functions &amp; Optimizers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part V ‚Äî NLP</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp1.html">NLP Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp2.html">Text Cleaning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp3.html">Text Normalization</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp4.html">NLP Components</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp5.html">Bag of Words, TF-IDF</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp6.html">Sentiment Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../5_nlp/nlp7.html">NLP with SpaCy</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Part VI ‚Äî Career &amp; MLOps</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../6_interview/self%20introduction.html">Self Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6_interview/py.html">Python: Interview Guide</a></li>





<li class="toctree-l1"><a class="reference internal" href="../6_interview/pd.html">üìö Pandas: Interview Guide</a></li>


<li class="toctree-l1"><a class="reference internal" href="../6_interview/ml.html">Machine Learning: Interview Guide</a></li>













<li class="toctree-l1"><a class="reference internal" href="../6_interview/git.html">Git</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6_interview/dvc.html">DVC</a></li>
<li class="toctree-l1"><a class="reference internal" href="../6_interview/mlflow.html">MLflow</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/coding-fame/Data-science-books" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/coding-fame/Data-science-books/edit/main/contents/0_maths/5_calculus.ipynb" target="_blank"
   class="btn btn-sm btn-source-edit-button dropdown-item"
   title="Suggest edit"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-pencil-alt"></i>
  </span>
<span class="btn__text-container">Suggest edit</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/coding-fame/Data-science-books/issues/new?title=Issue%20on%20page%20%2Fcontents/0_maths/5_calculus.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/contents/0_maths/5_calculus.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Calculus</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Calculus</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-calculus-matters-in-ml-and-dl">Why Calculus Matters in ML and DL</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#derivatives-measuring-change">Derivatives: Measuring Change</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-derivatives">What Are Derivatives?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-they-help-in-ml">Why They Help in ML</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-derivatives-handling-multiple-variables">Partial Derivatives: Handling Multiple Variables</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-partial-derivatives">What Are Partial Derivatives?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-theyre-important">Why They‚Äôre Important</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-linear-regression">Example: Linear Regression</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#gradients-directions-for-improvement">Gradients: Directions for Improvement</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-gradients">What Are Gradients?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-gradients-help">How Gradients Help</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-step-by-step-optimization">Gradient Descent: Step-by-Step Optimization</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-gradient-descent">What Is Gradient Descent?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-it-works">How It Works</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-linear-regression-in-action">Example: Linear Regression in Action</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#python-code-example">Python Code Example</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-neural-networks-cnns-calculus-in-deep-learning">Convolutional Neural Networks (CNNs): Calculus in Deep Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-cnns">What Are CNNs?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-calculus-helps-cnns">How Calculus Helps CNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-parts-of-a-cnn">Key Parts of a CNN</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-image-classification">Example: Image Classification</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-integrals-in-ml"><strong>What are Integrals in ML?</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-probability-and-pdfs"><strong>a. Probability and PDFs</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#b-normalization"><strong>b. Normalization</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#c-expectation-mean"><strong>c. Expectation (Mean)</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-variance"><strong>d. Variance</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#e-bayesian-marginalization"><strong>e. Bayesian Marginalization</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#integrals-in-ml-algorithms"><strong>5. Integrals in ML Algorithms</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-kernel-density-estimation-kde"><strong>a. Kernel Density Estimation (KDE)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#b-bayesian-neural-networks"><strong>b. Bayesian Neural Networks</strong></a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <hr class="docutils" />
<section id="calculus">
<h1>Calculus<a class="headerlink" href="#calculus" title="Link to this heading">#</a></h1>
<p>Calculus is a branch of mathematics that studies how things change. In Machine Learning (ML) and Deep Learning (DL), it helps us improve models by finding the best settings for their parameters. Two main ideas from calculus <strong>derivatives</strong> and <strong>gradients</strong> are especially important for this process.</p>
<section id="why-calculus-matters-in-ml-and-dl">
<h2>Why Calculus Matters in ML and DL<a class="headerlink" href="#why-calculus-matters-in-ml-and-dl" title="Link to this heading">#</a></h2>
<p>Calculus is the mathematics of change, and in ML and DL, it‚Äôs essential for:</p>
<ul class="simple">
<li><p><strong>Optimization</strong>: Finding the best model parameters by minimizing loss functions (e.g., gradient descent).</p></li>
<li><p><strong>Understanding Models</strong>: Analyzing how inputs affect outputs (e.g., backpropagation).</p></li>
<li><p><strong>Regularization</strong>: Balancing model complexity and fit.</p></li>
<li><p><strong>Probability</strong>: Linking to probability density functions and expectations.</p></li>
</ul>
<p><strong>Key Roles</strong></p>
<ol class="arabic simple">
<li><p><strong>Derivatives</strong>: Measure how functions change, critical for optimization.</p></li>
<li><p><strong>Gradients</strong>: Multi-dimensional derivatives, guiding parameter updates.</p></li>
<li><p><strong>Integrals</strong>: Compute areas under curves, used in probability and normalization.</p></li>
</ol>
</section>
</section>
<hr class="docutils" />
<section id="derivatives-measuring-change">
<h1>Derivatives: Measuring Change<a class="headerlink" href="#derivatives-measuring-change" title="Link to this heading">#</a></h1>
<section id="what-are-derivatives">
<h2>What Are Derivatives?<a class="headerlink" href="#what-are-derivatives" title="Link to this heading">#</a></h2>
<p>A <strong>derivative</strong> measures how a function changes when its input changes. In simple terms, it‚Äôs the ‚Äúslope‚Äù or steepness of the function at a specific point.</p>
</section>
<section id="why-they-help-in-ml">
<h2>Why They Help in ML<a class="headerlink" href="#why-they-help-in-ml" title="Link to this heading">#</a></h2>
<p>In ML, we use a <strong>loss function</strong> to measure how far off our predictions are from the actual results. The derivative of this loss function tells us how to adjust the model‚Äôs parameters to reduce the error.</p>
</section>
<section id="example">
<h2>Example<a class="headerlink" href="#example" title="Link to this heading">#</a></h2>
<p>Consider a simple function:
[
f(x) = x^2
]
Its derivative is:
[
f‚Äô(x) = 2x
]</p>
<ul class="simple">
<li><p>At ( x = 1 ), the derivative is ( f‚Äô(1) = 2 ) (a positive slope, meaning the function is increasing).</p></li>
<li><p>At ( x = -1 ), it‚Äôs ( f‚Äô(-1) = -2 ) (a negative slope, meaning it‚Äôs decreasing).</p></li>
</ul>
<p>This slope helps us decide how to change ( x ) to make ( f(x) ) smaller, which is key in training ML models.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from sympy import symbols, diff

# Define symbolic variable and function
x = symbols(&#39;x&#39;)
f = x**2 + 3*x + 2

# Derivative
f_prime = diff(f, x)
print(&quot;Derivative of f(x) = x^2 + 3x + 2:&quot;, f_prime)  # Output: 2*x + 3

# Evaluate at x = 2
value = f_prime.subs(x, 2)
print(&quot;Slope at x = 2:&quot;, value)  # Output: 7
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="partial-derivatives-handling-multiple-variables">
<h1>Partial Derivatives: Handling Multiple Variables<a class="headerlink" href="#partial-derivatives-handling-multiple-variables" title="Link to this heading">#</a></h1>
<section id="what-are-partial-derivatives">
<h2>What Are Partial Derivatives?<a class="headerlink" href="#what-are-partial-derivatives" title="Link to this heading">#</a></h2>
<p>When a function has more than one variable‚Äîlike ( f(x, y) ) a <strong>partial derivative</strong> measures how the function changes when only one variable changes, while the others stay fixed. In ML, models have many parameters (e.g., weights in a neural network), and partial derivatives help us adjust each one separately.</p>
</section>
<section id="why-theyre-important">
<h2>Why They‚Äôre Important<a class="headerlink" href="#why-theyre-important" title="Link to this heading">#</a></h2>
<p>ML models often predict based on multiple inputs, like size and location for house prices. Partial derivatives tell us how each parameter affects the error, letting us fine-tune them to improve predictions.</p>
</section>
<section id="example-linear-regression">
<h2>Example: Linear Regression<a class="headerlink" href="#example-linear-regression" title="Link to this heading">#</a></h2>
<p>In <strong>linear regression</strong>, we fit a line to data:
[
y = w_1 x + w_0
]</p>
<ul class="simple">
<li><p>( y ): Predicted value</p></li>
<li><p>( x ): Input (e.g., house size)</p></li>
<li><p>( w_1 ): Slope (weight)</p></li>
<li><p>( w_0 ): Intercept (bias)</p></li>
</ul>
<p>We measure error with the <strong>Mean Squared Error (MSE)</strong>:
[
L(w_1, w_0) = \frac{1}{n} \sum_{i=1}^{n} (y_i - (w_1 x_i + w_0))^2
]</p>
<ul class="simple">
<li><p>( n ): Number of data points</p></li>
<li><p>( y_i ): Actual value</p></li>
<li><p>( x_i ): Input value</p></li>
</ul>
<p>To minimize this error, we calculate partial derivatives:</p>
<ol class="arabic simple">
<li><p>For ( w_1 ):
[
\frac{\partial L}{\partial w_1} = \frac{2}{n} \sum_{i=1}^{n} (y_i - (w_1 x_i + w_0)) (-x_i)
]</p></li>
<li><p>For ( w_0 ):
[
\frac{\partial L}{\partial w_0} = \frac{2}{n} \sum_{i=1}^{n} (y_i - (w_1 x_i + w_0)) (-1)
]</p></li>
</ol>
<p>These tell us how to adjust ( w_1 ) and ( w_0 ) to lower the error.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Function with two variables
y = symbols(&#39;y&#39;)
g = x**2 + y**2 + x*y

# Partial derivatives
dg_dx = diff(g, x)
dg_dy = diff(g, y)
print(&quot;‚àÇg/‚àÇx:&quot;, dg_dx)  # Output: 2*x + y
print(&quot;‚àÇg/‚àÇy:&quot;, dg_dy)  # Output: 2*y + x
</pre></div>
</div>
</div>
</div>
</section>
</section>
<hr class="docutils" />
<section id="gradients-directions-for-improvement">
<h1>Gradients: Directions for Improvement<a class="headerlink" href="#gradients-directions-for-improvement" title="Link to this heading">#</a></h1>
<section id="what-are-gradients">
<h2>What Are Gradients?<a class="headerlink" href="#what-are-gradients" title="Link to this heading">#</a></h2>
<p>A <strong>gradient</strong> is a collection of partial derivatives for a function with multiple variables. It‚Äôs a vector that points in the direction where the function increases the most.</p>
</section>
<section id="how-gradients-help">
<h2>How Gradients Help<a class="headerlink" href="#how-gradients-help" title="Link to this heading">#</a></h2>
<p>In ML, we use gradients to find the minimum of the loss function. By moving in the <strong>opposite direction</strong> of the gradient (called the negative gradient), we reduce the error step by step.</p>
</section>
</section>
<hr class="docutils" />
<section id="gradient-descent-step-by-step-optimization">
<h1>Gradient Descent: Step-by-Step Optimization<a class="headerlink" href="#gradient-descent-step-by-step-optimization" title="Link to this heading">#</a></h1>
<section id="what-is-gradient-descent">
<h2>What Is Gradient Descent?<a class="headerlink" href="#what-is-gradient-descent" title="Link to this heading">#</a></h2>
<p><strong>Gradient descent</strong> is a method to minimize a loss function by repeatedly adjusting its parameters in the opposite direction of the gradient.</p>
</section>
<section id="how-it-works">
<h2>How It Works<a class="headerlink" href="#how-it-works" title="Link to this heading">#</a></h2>
<p>Imagine you‚Äôre on a hill and want to reach the bottom:</p>
<ul class="simple">
<li><p>The gradient shows the steepest way up.</p></li>
<li><p>You step downhill (negative gradient) a little at a time until you reach the lowest point.</p></li>
</ul>
<p>In ML, we update parameters like this:
[
w_1 = w_1 - \eta \cdot \frac{\partial L}{\partial w_1}
]
[
w_0 = w_0 - \eta \cdot \frac{\partial L}{\partial w_0}
]</p>
<ul class="simple">
<li><p>( \eta ): Learning rate (how big each step is)</p></li>
</ul>
</section>
<section id="example-linear-regression-in-action">
<h2>Example: Linear Regression in Action<a class="headerlink" href="#example-linear-regression-in-action" title="Link to this heading">#</a></h2>
<p>Here‚Äôs a small dataset:</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>( x ) (Size)</p></th>
<th class="head"><p>( y ) (Price)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>2</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>4</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>6</p></td>
</tr>
</tbody>
</table>
</div>
<p>Using gradient descent, we adjust ( w_1 ) and ( w_0 ) until the line ( y = w_1 x + w_0 ) fits the data. After many steps, ( w_1 ) becomes 2 and ( w_0 ) becomes 0, giving ( y = 2x )‚Äîthe best fit.</p>
</section>
<section id="python-code-example">
<h2>Python Code Example<a class="headerlink" href="#python-code-example" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import numpy as np
import matplotlib.pyplot as plt

# Sample data
X = np.array([[1], [2], [3]])
y = np.array([[2], [4], [6]])

# Starting values
m = 0.0  # Slope
b = 0.0  # Intercept
learning_rate = 0.01
steps = 1000
n = len(X)

# Gradient descent
for _ in range(steps):
    y_pred = m * X + b
    gradient_m = (-2/n) * np.sum(X * (y - y_pred))
    gradient_b = (-2/n) * np.sum(y - y_pred)
    m -= learning_rate * gradient_m
    b -= learning_rate * gradient_b

# Plot results
plt.scatter(X, y, color=&#39;blue&#39;, label=&#39;Data&#39;)
plt.plot(X, m * X + b, color=&#39;red&#39;, label=&#39;Best Fit Line&#39;)
plt.xlabel(&#39;Size&#39;)
plt.ylabel(&#39;Price&#39;)
plt.title(&#39;Linear Regression with Gradient Descent&#39;)
plt.legend()
plt.show()

print(f&quot;Slope (m): {m:.2f}, Intercept (b): {b:.2f}&quot;)
</pre></div>
</div>
</div>
</div>
<p>This code finds the best-fit line by minimizing the error.</p>
</section>
</section>
<hr class="docutils" />
<section id="convolutional-neural-networks-cnns-calculus-in-deep-learning">
<h1>Convolutional Neural Networks (CNNs): Calculus in Deep Learning<a class="headerlink" href="#convolutional-neural-networks-cnns-calculus-in-deep-learning" title="Link to this heading">#</a></h1>
<section id="what-are-cnns">
<h2>What Are CNNs?<a class="headerlink" href="#what-are-cnns" title="Link to this heading">#</a></h2>
<p><strong>Convolutional Neural Networks (CNNs)</strong> are DL models designed for tasks like image recognition. They use calculus to learn patterns (e.g., edges, shapes) in images.</p>
</section>
<section id="how-calculus-helps-cnns">
<h2>How Calculus Helps CNNs<a class="headerlink" href="#how-calculus-helps-cnns" title="Link to this heading">#</a></h2>
<p>CNNs minimize a loss function using gradient descent, just like linear regression. However, they have many more parameters (weights in layers), and partial derivatives guide updates across all of them.</p>
</section>
<section id="key-parts-of-a-cnn">
<h2>Key Parts of a CNN<a class="headerlink" href="#key-parts-of-a-cnn" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><strong>Convolutional Layer</strong>: Detects patterns using filters.</p></li>
<li><p><strong>Pooling Layer</strong>: Shrinks data to focus on key features.</p></li>
<li><p><strong>Fully Connected Layer</strong>: Combines features for predictions.</p></li>
</ol>
</section>
<section id="example-image-classification">
<h2>Example: Image Classification<a class="headerlink" href="#example-image-classification" title="Link to this heading">#</a></h2>
<p>Here‚Äôs a simple CNN for classifying images from the CIFAR-10 dataset:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt

# Load data
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize

# Build CNN
model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation=&#39;relu&#39;, input_shape=(32, 32, 3)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation=&#39;relu&#39;),
    layers.Flatten(),
    layers.Dense(64, activation=&#39;relu&#39;),
    layers.Dense(10)  # 10 classes
])

# Compile and train
model.compile(optimizer=&#39;adam&#39;, loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[&#39;accuracy&#39;])
model.fit(x_train, y_train, epochs=5, validation_data=(x_test, y_test))

# Test accuracy
test_loss, test_acc = model.evaluate(x_test, y_test)
print(f&quot;Test accuracy: {test_acc:.2f}&quot;)

# Show a prediction
predictions = model.predict(x_test)
plt.imshow(x_test[0])
plt.title(f&quot;Predicted: {predictions[0].argmax()}, True: {y_test[0][0]}&quot;)
plt.show()
</pre></div>
</div>
</div>
</div>
<p>This CNN uses gradient descent to adjust weights and classify images.</p>
</section>
</section>
<hr class="docutils" />
<section id="summary">
<h1>Summary<a class="headerlink" href="#summary" title="Link to this heading">#</a></h1>
<ul class="simple">
<li><p><strong>Derivatives</strong> show how a function changes, helping adjust model parameters.</p></li>
<li><p><strong>Partial Derivatives</strong> handle multiple variables, key for complex models.</p></li>
<li><p><strong>Gradients</strong> guide us to the minimum error using <strong>gradient descent</strong>.</p></li>
<li><p>In ML (e.g., linear regression) and DL (e.g., CNNs), calculus optimizes models for better predictions.</p></li>
</ul>
</section>
<hr class="docutils" />
<section id="what-are-integrals-in-ml">
<h1><strong>What are Integrals in ML?</strong><a class="headerlink" href="#what-are-integrals-in-ml" title="Link to this heading">#</a></h1>
<p>Integrals represent the accumulation of quantities, often visualized as the area under a curve. In ML, they‚Äôre used to:</p>
<ul class="simple">
<li><p><strong>Compute Probabilities</strong>: Integrate probability density functions (PDFs) over intervals.</p></li>
<li><p><strong>Normalization</strong>: Ensure distributions sum to 1.</p></li>
<li><p><strong>Expectations</strong>: Calculate averages or moments of random variables.</p></li>
<li><p><strong>Loss Functions</strong>: Define continuous objectives in some models.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from sympy import integrate

# Integral of f(x) = 2x
h = 2*x
integral = integrate(h, x)
print(&quot;Integral of 2x:&quot;, integral)  # Output: x^2

# Definite integral from 0 to 2
def_integral = integrate(h, (x, 0, 2))
print(&quot;Area from 0 to 2:&quot;, def_integral)  # Output: 4
</pre></div>
</div>
</div>
</div>
<section id="a-probability-and-pdfs">
<h2><strong>a. Probability and PDFs</strong><a class="headerlink" href="#a-probability-and-pdfs" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Concept</strong>: For a continuous random variable (X) with PDF (f(x)), the probability over an interval ([a, b]) is:
[
P(a \leq X \leq b) = \int_a^b f(x) , dx
]</p></li>
<li><p><strong>Use</strong>: Compute likelihoods in generative models.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from scipy import stats
import numpy as np
import matplotlib.pyplot as plt

# Normal distribution (mu=0, sigma=1)
norm = stats.norm(loc=0, scale=1)
x = np.linspace(-3, 3, 100)
pdf = norm.pdf(x)

# Probability P(-1 &lt; X &lt; 1)
prob = norm.cdf(1) - norm.cdf(-1)  # Cumulative Distribution Function (integral)
print(&quot;P(-1 &lt; X &lt; 1):&quot;, prob)  # Output: ~0.6827

# Visualize
plt.plot(x, pdf, label=&quot;PDF&quot;)
plt.fill_between(x, pdf, where=(x &gt;= -1) &amp; (x &lt;= 1), alpha=0.3, color=&quot;skyblue&quot;, label=&quot;P(-1 &lt; X &lt; 1)&quot;)
plt.title(&quot;Normal Distribution&quot;)
plt.legend()
plt.show()
</pre></div>
</div>
</div>
</div>
</section>
<section id="b-normalization">
<h2><strong>b. Normalization</strong><a class="headerlink" href="#b-normalization" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Concept</strong>: Ensure a PDF integrates to 1 over its domain: ( \int_{-\infty}^{\infty} f(x) , dx = 1 ).</p></li>
<li><p><strong>Use</strong>: Validate or adjust custom distributions.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from sympy import symbols, integrate, exp

# Symbolic example: Exponential PDF
x = symbols(&#39;x&#39;)
lam = 2  # Rate parameter
f = lam * exp(-lam * x)  # f(x) = Œªe^(-Œªx), x &gt;= 0

# Check normalization
integral = integrate(f, (x, 0, np.inf))
print(&quot;Integral (Normalization):&quot;, integral)  # Output: 1
</pre></div>
</div>
</div>
</div>
</section>
<section id="c-expectation-mean">
<h2><strong>c. Expectation (Mean)</strong><a class="headerlink" href="#c-expectation-mean" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Concept</strong>: ( E[X] = \int_{-\infty}^{\infty} x f(x) , dx ) for continuous variables.</p></li>
<li><p><strong>Use</strong>: Predict average outcomes in probabilistic models.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Numerical integration for E[X] (normal distribution)
from scipy.integrate import quad

def integrand(x):
    return x * norm.pdf(x)

expectation, _ = quad(integrand, -np.inf, np.inf)
print(&quot;Expected Value (Mean):&quot;, expectation)  # Output: ~0 (matches mu=0)
</pre></div>
</div>
</div>
</div>
</section>
<section id="d-variance">
<h2><strong>d. Variance</strong><a class="headerlink" href="#d-variance" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Concept</strong>: ( \text{Var}(X) = E[(X - \mu)^2] = \int_{-\infty}^{\infty} (x - \mu)^2 f(x) , dx ).</p></li>
<li><p><strong>Use</strong>: Measure spread in uncertainty quantification.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>def var_integrand(x):
    return (x - norm.mean())**2 * norm.pdf(x)

variance, _ = quad(var_integrand, -np.inf, np.inf)
print(&quot;Variance:&quot;, variance)  # Output: ~1 (matches sigma^2=1)
</pre></div>
</div>
</div>
</div>
</section>
<section id="e-bayesian-marginalization">
<h2><strong>e. Bayesian Marginalization</strong><a class="headerlink" href="#e-bayesian-marginalization" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>Concept</strong>: Marginal probability ( P(X) = \int P(X, Y) , dY ).</p></li>
<li><p><strong>Use</strong>: Integrate out nuisance variables in Bayesian ML.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span># Joint PDF: P(X, Y) = N(X; 0, 1) * N(Y; 0, 1)
def joint_pdf(x, y):
    return norm.pdf(x) * norm.pdf(y)

# Marginal P(X) at x=0
marginal, _ = quad(lambda y: joint_pdf(0, y), -np.inf, np.inf)
print(&quot;Marginal P(X=0):&quot;, marginal)  # Output: ~0.3989 (matches N(0,1) PDF at 0)
</pre></div>
</div>
</div>
</div>
</section>
<hr class="docutils" />
<section id="integrals-in-ml-algorithms">
<h2><strong>5. Integrals in ML Algorithms</strong><a class="headerlink" href="#integrals-in-ml-algorithms" title="Link to this heading">#</a></h2>
<section id="a-kernel-density-estimation-kde">
<h3><strong>a. Kernel Density Estimation (KDE)</strong><a class="headerlink" href="#a-kernel-density-estimation-kde" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Integrates kernel functions to estimate PDFs.</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>from sklearn.neighbors import KernelDensity

kde = KernelDensity(kernel=&quot;gaussian&quot;, bandwidth=1).fit(tenure.reshape(-1, 1))
x = np.linspace(0, 30, 100)
log_dens = kde.score_samples(x.reshape(-1, 1))
plt.plot(x, np.exp(log_dens), label=&quot;KDE&quot;)
plt.title(&quot;KDE of Tenure&quot;)
plt.show()
</pre></div>
</div>
</div>
</div>
</section>
<section id="b-bayesian-neural-networks">
<h3><strong>b. Bayesian Neural Networks</strong><a class="headerlink" href="#b-bayesian-neural-networks" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Integrate over weight distributions (often approximated).</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import tensorflow_probability as tfp

# Simple BNN prior
dist = tfp.distributions.Normal(loc=0., scale=1.)
x = np.linspace(-3, 3, 100)
pdf = dist.prob(x)
plt.plot(x, pdf)
plt.title(&quot;Prior Distribution&quot;)
plt.show()
</pre></div>
</div>
</div>
</div>
<hr class="docutils" />
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./contents\0_maths"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="3_inferential.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Inferential Statistics</p>
      </div>
    </a>
    <a class="right-next"
       href="6_regression_analysis.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Explanatory and Response Variables</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Calculus</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-calculus-matters-in-ml-and-dl">Why Calculus Matters in ML and DL</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#derivatives-measuring-change">Derivatives: Measuring Change</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-derivatives">What Are Derivatives?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-they-help-in-ml">Why They Help in ML</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example">Example</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#partial-derivatives-handling-multiple-variables">Partial Derivatives: Handling Multiple Variables</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-partial-derivatives">What Are Partial Derivatives?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#why-theyre-important">Why They‚Äôre Important</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-linear-regression">Example: Linear Regression</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#gradients-directions-for-improvement">Gradients: Directions for Improvement</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-gradients">What Are Gradients?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-gradients-help">How Gradients Help</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#gradient-descent-step-by-step-optimization">Gradient Descent: Step-by-Step Optimization</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-gradient-descent">What Is Gradient Descent?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-it-works">How It Works</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-linear-regression-in-action">Example: Linear Regression in Action</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#python-code-example">Python Code Example</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional-neural-networks-cnns-calculus-in-deep-learning">Convolutional Neural Networks (CNNs): Calculus in Deep Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-cnns">What Are CNNs?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-calculus-helps-cnns">How Calculus Helps CNNs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-parts-of-a-cnn">Key Parts of a CNN</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-image-classification">Example: Image Classification</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#summary">Summary</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#what-are-integrals-in-ml"><strong>What are Integrals in ML?</strong></a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-probability-and-pdfs"><strong>a. Probability and PDFs</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#b-normalization"><strong>b. Normalization</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#c-expectation-mean"><strong>c. Expectation (Mean)</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-variance"><strong>d. Variance</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#e-bayesian-marginalization"><strong>e. Bayesian Marginalization</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#integrals-in-ml-algorithms"><strong>5. Integrals in ML Algorithms</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-kernel-density-estimation-kde"><strong>a. Kernel Density Estimation (KDE)</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#b-bayesian-neural-networks"><strong>b. Bayesian Neural Networks</strong></a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Gajanesh
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright ¬© 2025 Gajanesh. All rights reserved..
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>