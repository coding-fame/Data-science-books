{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49cbdf85",
   "metadata": {},
   "source": [
    "# **What is Feature Engineering?**\n",
    "\n",
    "Feature engineering is the process of creating, selecting, and transforming raw data into features (input variables) that enhance a machine learning model‚Äôs ability to learn patterns and make predictions. Well-engineered features can often outweigh the choice of algorithm in terms of impact on model performance.\n",
    "\n",
    "## **Goals**\n",
    "- Improve model accuracy and interpretability.\n",
    "- Reduce noise and redundancy.\n",
    "- Handle missing data, outliers, and domain-specific requirements.\n",
    "\n",
    "## **Key Aspects**\n",
    "- **Creation**: Generating new features from existing data.\n",
    "- **Transformation**: Scaling, encoding, or normalizing data.\n",
    "- **Selection**: Choosing the most relevant features.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Why This Matters?\n",
    "- **Impact**: Proper preprocessing can improve model accuracy by 15-30%.\n",
    "\n",
    "> üß† *Real-World Analogy:* \"Just like sharpening tools before building a house - preprocessing shapes raw data for optimal model performance\"\n",
    "\n",
    "---\n",
    "\n",
    "# **1. Common Feature Engineering Techniques**\n",
    "\n",
    "## **a. Handling Missing Data**\n",
    "Missing values can skew models; feature engineering addresses this by imputation or creating indicators.\n",
    "\n",
    "Four Common Approaches:\n",
    "1. **Drop rows** with NaNs.\n",
    "2. **Drop columns** with NaNs.\n",
    "3. **Fill NaNs** with imputed values.\n",
    "4. **Use models that handle NaNs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a77e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Sample DataFrame with missing values\n",
    "data = pd.DataFrame({\n",
    "    \"Age\": [25, np.nan, 30, 35],\n",
    "    \"Salary\": [50000, 60000, np.nan, 75000]\n",
    "})\n",
    "\n",
    "# Imputation with mean\n",
    "data[\"Age_filled\"] = data[\"Age\"].fillna(data[\"Age\"].mean())\n",
    "print(data)\n",
    "# Output:\n",
    "#    Age  Salary  Age_filled\n",
    "# 0  25.0  50000        25.0\n",
    "# 1   NaN  60000        30.0\n",
    "# 2  30.0    NaN        30.0\n",
    "# 3  35.0  75000        35.0\n",
    "\n",
    "# Missing indicator\n",
    "data[\"Salary_missing\"] = data[\"Salary\"].isna().astype(int)\n",
    "print(data)\n",
    "# Output:\n",
    "#    Age  Salary  Age_filled  Salary_missing\n",
    "# 0  25.0  50000        25.0               0\n",
    "# 1   NaN  60000        30.0               0\n",
    "# 2  30.0    NaN        30.0               1\n",
    "# 3  35.0  75000        35.0               0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5469f8",
   "metadata": {},
   "source": [
    "### 3. Missing Value Imputation\n",
    "Missing values can disrupt model training. Here‚Äôs how to handle them:\n",
    "\n",
    "- **Simple Imputation**: Use mean, median, or mode.\n",
    "- **Advanced Imputation**: Use algorithms like KNN to estimate missing values.\n",
    "\n",
    "#### üß© Code Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b363024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([[1, np.nan], [3, 4], [np.nan, 6]])\n",
    "\n",
    "# Mean Imputation\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "imputed_data_mean = imputer.fit_transform(data)\n",
    "print(\"Mean Imputed Data:\\n\", imputed_data_mean)\n",
    "\n",
    "# KNN Imputation (Advanced)\n",
    "knn_imputer = KNNImputer(n_neighbors=2)\n",
    "imputed_data_knn = knn_imputer.fit_transform(data)\n",
    "print(\"KNN Imputed Data:\\n\", imputed_data_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddcbcfb",
   "metadata": {},
   "source": [
    "**Why It Matters**: ML models like **Decision Trees** can handle missing values, but most require complete data.\n",
    "\n",
    "---\n",
    "\n",
    "### Imputing Categorical Missing Values\n",
    "Use the most frequent category or a new category like \"Unknown\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d237027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "df['Category'] = imputer.fit_transform(df[['Category']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdb1d03",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **c. Scaling and Normalization**\n",
    "Features with different scales (e.g., age vs. salary) can bias models. Scaling ensures uniformity.\n",
    "\n",
    "- **StandardScaler**: Zero mean, unit variance.\n",
    "- **MinMaxScaler**: Scales to a range (e.g., 0 to 1).\n",
    "\n",
    "### üß™ What is Feature Scaling?\n",
    "**Feature Scaling** is a technique used to standardize the independent features present in the data within a fixed range.\n",
    "\n",
    "Feature Scaling standardizes the range of numerical features so that no single feature dominates the model due to its scale. This is especially important for algorithms like:\n",
    "- **Gradient Descent** in ML.\n",
    "- **Backpropagation** in Deep Learning (DL).\n",
    "- **K-Nearest Neighbors (KNN)** and **Support Vector Machines (SVM)**, which rely on distance calculations.\n",
    "\n",
    "| **Technique**      | **Formula**                               | **When to Use**                              | **Code Example**                  |\n",
    "|--------------------|-------------------------------------------|----------------------------------------------|-----------------------------------|\n",
    "| **Min-Max Scaling**| \\( `(x - min(x)) /(max(x) - min(x))` \\) | When you need data in a fixed range (0-1).    | `MinMaxScaler(feature_range=(0,1))` |\n",
    "| **Standard Scaling**| \\( `(x - mu) /sigma` \\)              | When data is normally distributed.           | `StandardScaler()`                |\n",
    "| **Robust Scaling** | \\( `(x - Q1) /(Q3 - Q1)` \\)              | When data contains outliers.                 | `RobustScaler()`                  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7325dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "data = pd.DataFrame({\"Age\": [25, 30, 35], \"Salary\": [50000, 60000, 75000]})\n",
    "\n",
    "# Standardization\n",
    "scaler = StandardScaler()\n",
    "data[[\"Age_std\", \"Salary_std\"]] = scaler.fit_transform(data[[\"Age\", \"Salary\"]])\n",
    "print(data)\n",
    "# Output:\n",
    "#    Age  Salary   Age_std  Salary_std\n",
    "# 0   25   50000 -1.161895   -1.161895\n",
    "# 1   30   60000  0.000000    0.000000\n",
    "# 2   35   75000  1.161895    1.161895\n",
    "\n",
    "# Min-Max Scaling\n",
    "minmax = MinMaxScaler()\n",
    "data[[\"Age_minmax\", \"Salary_minmax\"]] = minmax.fit_transform(data[[\"Age\", \"Salary\"]])\n",
    "print(data)\n",
    "# Output includes:\n",
    "#    Age_minmax  Salary_minmax\n",
    "# 0         0.0            0.0\n",
    "# 1         0.5            0.4\n",
    "# 2         1.0            1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bafa600",
   "metadata": {},
   "source": [
    "**Why It Matters**: Algorithms like KNN and SVM are sensitive to feature scales. Scaling ensures fair distance calculations.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Outlier Handling Strategies\n",
    "Outliers are extreme values that can skew model performance. \n",
    "\n",
    "Ways to handle outliers:\n",
    "\n",
    "- **Drop outliers**\n",
    "- **Mark outliers using boolean conditions**\n",
    "- **Transform outliers into features**\n",
    "\n",
    "Here's how to handle them:\n",
    "\n",
    "| **Method**      | **Use Case**                 | **Code Snippet**                                      |\n",
    "|-----------------|------------------------------|-------------------------------------------------------|\n",
    "| **IQR Filter**  | Moderate outliers            | ```python\\nQ1 = df.quantile(0.25)\\nQ3 = df.quantile(0.75)\\nIQR = Q3 - Q1\\nfiltered_df = df[(df >= Q1 - 1.5*IQR) & (df <= Q3 + 1.5*IQR)]\\n``` |\n",
    "| **Z-Score**     | Extreme values               | ```python\\nfrom scipy import stats\\nz = np.abs(stats.zscore(data))\\nfiltered_data = data[z < 3]\\n``` |\n",
    "| **Capping**     | Preserve data shape          | ```python\\ndf = df.clip(lower=Q1-1.5*IQR, upper=Q3+1.5*IQR)\\n``` |\n",
    "\n",
    "> **Pro Tip**: Visualize outliers using boxplots before deciding on a strategy.\n",
    "\n",
    "**Why It Matters**: Outliers can mislead models like **Linear Regression**, causing poor predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## **Encoding Categorical Variables**\n",
    "\n",
    "Categorical data represents categories (e.g., colors, cities) and needs to be converted into numbers for ML models.\n",
    "\n",
    "### Types of Categorical Data:\n",
    "1. **Nominal**: No order (e.g., colors: red, blue, green).\n",
    "2. **Ordinal**: Has order (e.g., ratings: low, medium, high).\n",
    "\n",
    "In practice, nominal variables are more common.\n",
    "\n",
    "Machine learning models require numerical inputs, so categorical data must be transformed.\n",
    "\n",
    "- **Label Encoding**: Assigns integers to categories.\n",
    "- **One-Hot Encoding**: Creates binary columns for each category.\n",
    "\n",
    "---\n",
    "\n",
    "### Label Encoding\n",
    "\n",
    "Label encoding converts categorical labels into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3d4c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({\"City\": [\"NY\", \"LA\", \"NY\", \"SF\"]})\n",
    "\n",
    "# Label Encoding\n",
    "le = LabelEncoder()\n",
    "data[\"City_Label\"] = le.fit_transform(data[\"City\"])\n",
    "print(data)\n",
    "# Output:\n",
    "#   City  City_Label\n",
    "# 0   NY          1\n",
    "# 1   LA          0\n",
    "# 2   NY          1\n",
    "# 3   SF          2\n",
    "\n",
    "# One-Hot Encoding\n",
    "ohe = pd.get_dummies(data[\"City\"], prefix=\"City\")\n",
    "data = pd.concat([data, ohe], axis=1)\n",
    "print(data)\n",
    "# Output:\n",
    "#   City  City_Label  City_LA  City_NY  City_SF\n",
    "# 0   NY          1        0        1        0\n",
    "# 1   LA          0        1        0        0\n",
    "# 2   NY          1        0        1        0\n",
    "# 3   SF          2        0        0        1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474af36b",
   "metadata": {},
   "source": [
    "### Ordinal Encoding\n",
    "\n",
    "Ordinal encoding assigns integer values to each category, but be mindful that it may introduce unintended relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b825ec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "data = [['blue'], ['green'], ['red']]\n",
    "encoder = OrdinalEncoder()\n",
    "result = encoder.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67d2ee1",
   "metadata": {},
   "source": [
    "> **Problem with Ordinal Encoding**: If applied to nominal data, it can introduce a false relationship between categories, which might lead to poor model performance. Use **One-Hot Encoding** to avoid this issue.\n",
    "\n",
    "### One-Hot Encoding\n",
    "\n",
    "One-Hot Encoding creates binary columns for each category, ensuring no implied relationship between categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff63f5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "a = [['apple'], ['pear'], ['apple'], ['pear'], ['apple']]\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "onehot = encoder.fit_transform(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd920271",
   "metadata": {},
   "source": [
    "### Dummy Variable Encoding\n",
    "\n",
    "Similar to One-Hot Encoding but drops one column to avoid redundancy\n",
    "\n",
    "Dummy variable encoding drops one category to avoid redundancy. It is typically used in situations where the first category is implied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c2103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(drop='first', sparse=False)\n",
    "onehot = encoder.fit_transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed99edc",
   "metadata": {},
   "source": [
    "Encoding converts categories into numbers:\n",
    "\n",
    "| **Technique**      | **Best For**          | **Dimensionality** | **Code Example**               |\n",
    "|--------------------|-----------------------|--------------------|--------------------------------|\n",
    "| **One-Hot Encoding**| Nominal data          | High               | `OneHotEncoder()`              |\n",
    "| **Label Encoding** | Ordinal data          | Low                | `LabelEncoder()`               |\n",
    "| **Target Encoding**| High-cardinality data | Medium             | `category_encoders.TargetEncoder()` |\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Advanced Techniques\n",
    "\n",
    "#### ü™Ñ Handling Rare Categories\n",
    "Group rare categories into \"Other\" to reduce dimensionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c3ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_categories = df['Category'].value_counts().nlargest(5).index\n",
    "df['Category'] = np.where(df['Category'].isin(top_categories), df['Category'], 'Other')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b99a50",
   "metadata": {},
   "source": [
    "#### üéØ Target Encoding\n",
    "Encode categories based on the target variable‚Äôs mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef880a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from category_encoders import TargetEncoder\n",
    "encoder = TargetEncoder()\n",
    "df['Encoded_Cat'] = encoder.fit_transform(df['Category'], df['Target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a5da29",
   "metadata": {},
   "source": [
    "**Why It Matters**: Reduces dimensionality in high-cardinality data, improving model efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "### **d. Feature Creation**\n",
    "New features can be derived from existing ones (e.g., ratios, interactions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4b4d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "data = pd.DataFrame({\n",
    "    \"Height\": [160, 170, 180],\n",
    "    \"Weight\": [60, 70, 80]\n",
    "})\n",
    "\n",
    "# Creating BMI feature\n",
    "data[\"BMI\"] = data[\"Weight\"] / (data[\"Height\"] / 100) ** 2\n",
    "print(data)\n",
    "# Output:\n",
    "#    Height  Weight        BMI\n",
    "# 0     160      60  23.437500\n",
    "# 1     170      70  24.221453\n",
    "# 2     180      80  24.691358\n",
    "\n",
    "# Interaction feature\n",
    "data[\"Height_Weight\"] = data[\"Height\"] * data[\"Weight\"]\n",
    "print(data)\n",
    "# Output includes:\n",
    "#    Height_Weight\n",
    "# 0          9600\n",
    "# 1         11900\n",
    "# 2         14400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338a6801",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **e. Binning Continuous Variables**\n",
    "Convert continuous data into discrete categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7111e821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binning Age\n",
    "data = pd.DataFrame({\"Age\": [15, 25, 35, 45, 55]})\n",
    "bins = [0, 18, 30, 50, 100]\n",
    "labels = [\"Teen\", \"Young\", \"Adult\", \"Senior\"]\n",
    "data[\"Age_Group\"] = pd.cut(data[\"Age\"], bins=bins, labels=labels)\n",
    "print(data)\n",
    "# Output:\n",
    "#    Age Age_Group\n",
    "# 0   15      Teen\n",
    "# 1   25     Young\n",
    "# 2   35     Adult\n",
    "# 3   45     Adult\n",
    "# 4   55    Senior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12dffb43",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **f. Handling Datetime Features**\n",
    "Extract meaningful components from dates/times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc5f48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample datetime data\n",
    "data = pd.DataFrame({\"Date\": pd.to_datetime([\"2023-01-15\", \"2023-06-20\", \"2023-12-25\"])})\n",
    "\n",
    "# Extract features\n",
    "data[\"Year\"] = data[\"Date\"].dt.year\n",
    "data[\"Month\"] = data[\"Date\"].dt.month\n",
    "data[\"Day\"] = data[\"Date\"].dt.day\n",
    "data[\"Weekday\"] = data[\"Date\"].dt.weekday\n",
    "print(data)\n",
    "# Output:\n",
    "#         Date  Year  Month  Day  Weekday\n",
    "# 0 2023-01-15  2023      1   15        6\n",
    "# 1 2023-06-20  2023      6   20        1\n",
    "# 2 2023-12-25  2023     12   25        0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3e9359",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **g. Text Feature Engineering**\n",
    "Extract features from text data (e.g., word counts, TF-IDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc80971c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Sample text data\n",
    "data = pd.Series([\"I love coding\", \"Coding is fun\", \"I love Python\"])\n",
    "\n",
    "# Word count (Bag of Words)\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(data)\n",
    "print(vectorizer.get_feature_names_out())  # Output: ['coding' 'fun' 'is' 'love' 'python']\n",
    "print(X.toarray())\n",
    "# Output:\n",
    "# [[1 0 0 1 0]\n",
    "#  [1 1 1 0 0]\n",
    "#  [0 0 0 1 1]]\n",
    "\n",
    "# TF-IDF\n",
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(data)\n",
    "print(X_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7756054",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### **h. Polynomial Features**\n",
    "Generate polynomial and interaction terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5943883",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "data = pd.DataFrame({\"X\": [1, 2, 3], \"Y\": [4, 5, 6]})\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "poly_features = poly.fit_transform(data)\n",
    "print(poly.get_feature_names_out())  # Output: ['X' 'Y' 'X^2' 'X Y' 'Y^2']\n",
    "print(poly_features)\n",
    "# Output:\n",
    "# [[ 1.  4.  1.  4. 16.]\n",
    "#  [ 2.  5.  4. 10. 25.]\n",
    "#  [ 3.  6.  9. 18. 36.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b2f693",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# **2. Feature Selection**\n",
    "Not all features are useful‚Äîselecting the best reduces noise and computation.\n",
    "\n",
    "## **a. Filter Methods**\n",
    "- Use statistical measures (e.g., correlation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758d6af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation-based selection\n",
    "data = pd.DataFrame({\n",
    "    \"X1\": [1, 2, 3, 4],\n",
    "    \"X2\": [2, 4, 6, 8],\n",
    "    \"Target\": [0, 1, 0, 1]\n",
    "})\n",
    "corr = data.corr()[\"Target\"].abs()\n",
    "print(corr)  # X2 is perfectly correlated with Target\n",
    "# Output:\n",
    "# X1        0.0\n",
    "# X2        1.0\n",
    "# Target    1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f09fda",
   "metadata": {},
   "source": [
    "## **b. Wrapper Methods**\n",
    "- Use a model to evaluate feature subsets (e.g., Recursive Feature Elimination)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb31119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = data[[\"X1\", \"X2\"]]\n",
    "y = data[\"Target\"]\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, n_features_to_select=1)\n",
    "fit = rfe.fit(X, y)\n",
    "print(\"Selected feature:\", X.columns[fit.support_])  # Output: Index(['X2'], dtype='object')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5a6bde",
   "metadata": {},
   "source": [
    "## **c. Embedded Methods**\n",
    "- Feature importance from models (e.g., tree-based)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beead86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)\n",
    "importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "print(importances)  # X2 is more important\n",
    "# Output:\n",
    "# X1    0.0\n",
    "# X2    1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2ef46b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **3. Practical Example: House Price Prediction**\n",
    "Let‚Äôs tie it all together with a realistic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b327e0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample house data\n",
    "data = pd.DataFrame({\n",
    "    \"Size\": [1500, 2000, 1800, np.nan],\n",
    "    \"Rooms\": [3, 4, 3, 5],\n",
    "    \"Built\": [\"2010-05-01\", \"2000-12-15\", \"2015-03-10\", \"1995-08-20\"],\n",
    "    \"Location\": [\"Urban\", \"Suburban\", \"Urban\", \"Rural\"],\n",
    "    \"Price\": [300000, 400000, 350000, 250000]\n",
    "})\n",
    "\n",
    "# Step 1: Handle missing data\n",
    "data[\"Size\"] = data[\"Size\"].fillna(data[\"Size\"].median())\n",
    "\n",
    "# Step 2: Datetime features\n",
    "data[\"Built\"] = pd.to_datetime(data[\"Built\"])\n",
    "data[\"Age\"] = (pd.Timestamp(\"2025-03-02\") - data[\"Built\"]).dt.days / 365\n",
    "\n",
    "# Step 3: Encoding categorical variables\n",
    "data = pd.get_dummies(data, columns=[\"Location\"], prefix=\"Loc\")\n",
    "\n",
    "# Step 4: Feature creation\n",
    "data[\"Size_per_Room\"] = data[\"Size\"] / data[\"Rooms\"]\n",
    "\n",
    "# Step 5: Scaling\n",
    "scaler = StandardScaler()\n",
    "data[[\"Size\", \"Age\"]] = scaler.fit_transform(data[[\"Size\", \"Age\"]])\n",
    "\n",
    "print(data)\n",
    "# Output (simplified):\n",
    "#       Size  Rooms      Built       Age  Price  Loc_Rural  Loc_Suburban  Loc_Urban  Size_per_Room\n",
    "# 0 -1.297771      3 2010-05-01 -0.137158  300000          0             0          1      500.000000\n",
    "# 1  0.866514      4 2000-12-15  1.191053  400000          0             1          0      500.000000\n",
    "# 2 -0.123743      3 2015-03-10 -0.701090  350000          0             0          1      600.000000\n",
    "# 3  0.555000      5 1995-08-20  1.352747  250000          1             0          0      360.000000\n",
    "\n",
    "# Step 6: Feature selection (correlation with Price)\n",
    "corr = data.corr()[\"Price\"].abs()\n",
    "print(corr.sort_values(ascending=False))\n",
    "# Output (example):\n",
    "# Price            1.000000\n",
    "# Size             0.986754\n",
    "# Size_per_Room    0.933394\n",
    "# Rooms            0.188982\n",
    "# Age              0.086066"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ca011d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **4. Tools and Libraries**\n",
    "- **`pandas`**: Data manipulation (`fillna`, `get_dummies`, `cut`).\n",
    "- **`numpy`**: Mathematical operations.\n",
    "- **`scikit-learn`**: Preprocessing (`StandardScaler`, `PolynomialFeatures`), feature selection (`RFE`), and modeling.\n",
    "- **`category_encoders`**: Advanced encoding (e.g., Target Encoding).\n",
    "  ```python\n",
    "  from category_encoders import TargetEncoder\n",
    "  encoder = TargetEncoder()\n",
    "  data[\"Location_encoded\"] = encoder.fit_transform(data[\"Location\"], data[\"Price\"])\n",
    "  ```\n",
    "- **`featuretools`**: Automated feature engineering.\n",
    "  ```python\n",
    "  import featuretools as ft\n",
    "  es = ft.EntitySet(id=\"house_data\")\n",
    "  es = es.add_dataframe(dataframe_name=\"houses\", dataframe=data, index=\"index\")\n",
    "  features, feature_defs = ft.dfs(entityset=es, target_dataframe_name=\"houses\")\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Level 3: Pipeline Power\n",
    "\n",
    "### 1. The Assembly Line Approach\n",
    "**Pipelines** automate preprocessing and modeling steps, ensuring consistency and reducing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287f15e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "numerical_cols = ['Age', 'Income']\n",
    "categorical_cols = ['Gender', 'Occupation']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_cols),\n",
    "        ('cat', OneHotEncoder(), categorical_cols)\n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestClassifier())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2553aebc",
   "metadata": {},
   "source": [
    "**Success Metric**: Reduce deployment errors by 40% using pipelines.\n",
    "\n",
    "**Why It Matters**: Pipelines ensure that preprocessing is applied consistently across training and testing data, avoiding data leakage.\n",
    "\n",
    "---\n",
    "## üöÄ Interview Prep Kit\n",
    "\n",
    "### Common Questions & Answers\n",
    "1. **Q:** \"Why is feature scaling important?\"  \n",
    "   **A:** It ensures features with larger ranges don‚Äôt dominate the model, improving performance in algorithms like KNN or SVM.\n",
    "\n",
    "2. **Q:** \"When to use One-Hot vs. Label Encoding?\"  \n",
    "   **A:** Use **One-Hot Encoding** for nominal data and **Label Encoding** for ordinal data.\n",
    "\n",
    "3. **Q:** \"How to handle high-cardinality categorical data?\"  \n",
    "   **A:** Use **Target Encoding** or group rare categories into \"Other.\"\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
