{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc8787ce",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "\n",
    "## **What Does \"Train the Model\" Mean?**\n",
    "\n",
    "Training the model involves feeding the training data into an algorithm so it can learn patterns and relationships. It’s like teaching a student by working through practice problems together.\n",
    "\n",
    "Training a model involves optimizing its parameters using a dataset (training data) so it can accurately predict outcomes or classify data points. The process adjusts the model based on a loss function, iteratively improving its performance by learning from examples.\n",
    "\n",
    "## **Why Train a Model?**\n",
    "- **Learn Patterns**: Capture relationships between features and targets.\n",
    "- **Generalization**: Enable predictions on unseen data.\n",
    "- **Task-Specific**: Tailor the model to classification, regression, or other objectives.\n",
    "\n",
    "\n",
    "## **Why It’s Important**\n",
    "- **Pattern Recognition**: The model adjusts its parameters (e.g., weights in a neural network) to fit the data, enabling it to make predictions.\n",
    "- **Algorithm Selection**: The right algorithm—like decision trees for simple tasks or neural networks for complex ones depends on your problem.\n",
    "- **Optimization**: Tuning hyperparameters (e.g., learning rate, number of trees) refines the model’s performance.\n",
    "\n",
    "## **Key Concepts and Methods**\n",
    "\n",
    "### **a. Data Preparation**\n",
    "- **Splitting**: Divide data into training (fit model), validation (tune hyperparameters), and test (final evaluation) sets.\n",
    "- **Preprocessing**: Scale features, encode categoricals, handle missing values.\n",
    "\n",
    "### **b. Model Selection**\n",
    "- **Supervised**: Linear regression, logistic regression, decision trees, etc.\n",
    "- **Unsupervised**: K-Means, PCA.\n",
    "- **Deep Learning**: Neural networks via TensorFlow/Keras.\n",
    "\n",
    "### **c. Loss Functions**\n",
    "- **Regression**: MSE, MAE.\n",
    "- **Classification**: Cross-entropy, log-loss.\n",
    "\n",
    "### **d. Optimization**\n",
    "- **Gradient Descent**: Adjust parameters by minimizing loss via gradients.\n",
    "- **Stochastic Gradient Descent (SGD)**: Batch-based updates.\n",
    "- **Advanced Optimizers**: Adam, RMSprop (TensorFlow).\n",
    "\n",
    "### **e. Training Process**\n",
    "- **Fit**: Adjust model parameters to data.\n",
    "- **Epochs/Iterations**: Number of passes over data (deep learning) or optimization steps.\n",
    "- **Early Stopping**: Halt training if validation loss stops improving.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Tools and Methods Summary**\n",
    "- **Modeling**: `sklearn.linear_model`, `tensorflow.keras.Sequential`, `xgboost.XGBClassifier`.\n",
    "- **Data Prep**: `sklearn.model_selection.train_test_split`, `sklearn.preprocessing.StandardScaler`.\n",
    "- **Evaluation**: `sklearn.metrics.accuracy_score`, `mean_squared_error`.\n",
    "- **Visualization**: `matplotlib.pyplot`, `seaborn`.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Components of Training a Model\n",
    "\n",
    "### 1. **Algorithm Selection**\n",
    "- **Purpose**: Choose an algorithm based on the problem type classification (predicting categories) or regression (predicting numbers).\n",
    "- **Examples**:\n",
    "  - **Logistic Regression**: For binary classification (e.g., spam vs. not spam).\n",
    "  - **Decision Trees**: For classification or regression tasks.\n",
    "  - **Random Forest**: An ensemble method combining multiple decision trees for better accuracy.\n",
    "  - **Support Vector Machines (SVM)**: For complex classification problems.\n",
    "  - **Neural Networks**: For advanced tasks like image or speech recognition.\n",
    "\n",
    "### 2. **Model Training**\n",
    "- **Purpose**: Teach the algorithm patterns in the data.\n",
    "- **Process**:\n",
    "  - Initialize the algorithm’s parameters (e.g., weights in a neural network).\n",
    "  - Feed the training data into the algorithm.\n",
    "  - Adjust parameters to minimize the loss function using optimization techniques (e.g., gradient descent).\n",
    "\n",
    "### 3. **Hyperparameter Tuning**\n",
    "- **Purpose**: Optimize settings that aren’t learned from the data (e.g., learning rate, regularization strength).\n",
    "- **Methods**:\n",
    "  - **Grid Search**: Tests all possible combinations of hyperparameters.\n",
    "  - **Random Search**: Samples random combinations, often faster than Grid Search.\n",
    "  - **Bayesian Optimization**: Uses probabilistic models to find the best settings efficiently.\n",
    "\n",
    "### 4. **Cross-Validation**\n",
    "- **Purpose**: Evaluate the model’s performance and prevent overfitting (where the model memorizes the training data instead of learning general patterns).\n",
    "- **Technique**: **K-Fold Cross-Validation** splits the data into K subsets, trains on K-1 subsets, and validates on the remaining one, repeating K times.\n",
    "\n",
    "---\n",
    "\n",
    "## Practical Example: Training a Model with the Titanic Dataset\n",
    "\n",
    "Let’s train a model to predict whether a Titanic passenger survived (0 = No, 1 = Yes) using **Logistic Regression**. We’ll walk through data preparation, training, tuning, and validation, providing a complete Python code example.\n",
    "\n",
    "### Step 1: Prepare the Data\n",
    "We’ll use the Titanic dataset, preprocess it (handle missing values, encode categorical variables), and split it into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4abd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Titanic dataset\n",
    "url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv'\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "# Drop irrelevant columns and handle missing values\n",
    "df = df.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
    "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Encode categorical variables (Sex, Embarked)\n",
    "df = pd.get_dummies(df, columns=['Sex', 'Embarked'], drop_first=True)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = df.drop('Survived', axis=1)\n",
    "y = df['Survived']\n",
    "\n",
    "# Split into training and test sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b41268",
   "metadata": {},
   "source": [
    "- **Explanation**:\n",
    "  - Dropped columns like `Name` and `Cabin` that aren’t useful for prediction.\n",
    "  - Filled missing `Age` with the median and `Embarked` with the mode.\n",
    "  - Converted categorical variables into numerical ones using one-hot encoding.\n",
    "\n",
    "### Step 2: Select and Train the Model\n",
    "We’ll use Logistic Regression, a simple yet effective algorithm for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c334f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize and train the model\n",
    "model = LogisticRegression(max_iter=200)  # Increased iterations for convergence\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1025c5c",
   "metadata": {},
   "source": [
    "- **Explanation**:\n",
    "  - `max_iter=200`: Ensures the algorithm runs enough iterations to find the best parameters.\n",
    "  - `fit()`: Trains the model by adjusting parameters to predict `Survived` based on features.\n",
    "\n",
    "### Step 3: Hyperparameter Tuning\n",
    "We’ll use **Grid Search** to find the best hyperparameters for Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17cc786",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],  # Regularization strength\n",
    "    'penalty': ['l1', 'l2']    # Type of regularization (L1 = Lasso, L2 = Ridge)\n",
    "}\n",
    "\n",
    "# Set up Grid Search with 5-fold cross-validation\n",
    "grid_search = GridSearchCV(LogisticRegression(max_iter=200, solver='liblinear'), param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit Grid Search to training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b66daf",
   "metadata": {},
   "source": [
    "- **Explanation**:\n",
    "  - `C`: Controls regularization strength (smaller values = stronger regularization).\n",
    "  - `penalty`: L1 removes unimportant features; L2 reduces their impact.\n",
    "  - `solver='liblinear'`: Required for L1 penalty in Logistic Regression.\n",
    "  - `cv=5`: Uses 5-fold cross-validation to evaluate each combination.\n",
    "\n",
    "### Step 4: Cross-Validation\n",
    "We’ll assess the model’s consistency with **K-Fold Cross-Validation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888103f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform 5-fold cross-validation on the base model\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(\"Cross-Validation Scores:\", cv_scores)\n",
    "print(\"Mean CV Accuracy:\", cv_scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bff1482",
   "metadata": {},
   "source": [
    "- **Explanation**:\n",
    "  - Splits the training data into 5 parts, training on 4 and testing on 1 each time.\n",
    "  - Outputs a range of accuracy scores and their mean, showing model robustness.\n",
    "\n",
    "### Step 5: Train the Final Model\n",
    "Using the best parameters from Grid Search, we’ll train the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d334e0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final model with optimized parameters\n",
    "best_model = LogisticRegression(\n",
    "    C=grid_search.best_params_['C'],\n",
    "    penalty=grid_search.best_params_['penalty'],\n",
    "    max_iter=200,\n",
    "    solver='liblinear'\n",
    ")\n",
    "best_model.fit(X_train, y_train)\n",
    "print(\"Final model trained with optimized parameters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9594ba",
   "metadata": {},
   "source": [
    "- **Explanation**:\n",
    "  - Uses the best `C` and `penalty` found by Grid Search.\n",
    "  - Ready to make predictions on `X_test` or new data.\n",
    "\n",
    "### Step 6: Evaluate on Test Set (Optional)\n",
    "To see how the model performs on unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e2229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "print(\"Test Set Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57411dd7",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
