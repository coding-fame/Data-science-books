{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78c2d26e",
   "metadata": {},
   "source": [
    "# K-Means Clustering\n",
    "\n",
    "K-Means Clustering is a popular unsupervised machine learning algorithm used to partition a dataset into **K** distinct, non-overlapping groups or clusters based on similarity. Each cluster is defined by a centroid (the mean of all points in the cluster), and the algorithm works by minimizing the variance within each cluster while maximizing the separation between clusters. \n",
    "\n",
    "---\n",
    "## **What is K-Means Clustering?**\n",
    "K-Means is an iterative algorithm that partitions a dataset into \\(k\\) distinct, non-overlapping clusters by minimizing the variance within each cluster. It assigns each data point to the cluster with the nearest centroid (mean) and updates centroids based on the assigned points.\n",
    "\n",
    "### **Why Use K-Means?**\n",
    "- **Simplicity**: Easy to understand and implement.\n",
    "- **Scalability**: Efficient for large datasets with moderate dimensions.\n",
    "- **Versatility**: Applicable to various domains (e.g., customer segmentation, image compression).\n",
    "- **Unsupervised**: Requires no labeled data.\n",
    "\n",
    "## **How It Works**\n",
    "1. **Initialization**: Choose \\(k\\) initial centroids (randomly or via methods like K-Means++).\n",
    "2. **Assignment**: Assign each point to the nearest centroid based on distance metric (typically Euclidean).\n",
    "3. **Update**: Recalculate each centroid as the mean of all points assigned to its cluster.\n",
    "4. **Iteration**: Repeat until centroids stabilize or a maximum number of iterations is reached.\n",
    "\n",
    "![clustering](../images/Clusters3D.gif)\n",
    "\n",
    "## **2. Key Concepts and Methods**\n",
    "\n",
    "### **a. Core Mechanics**\n",
    "\n",
    "- **Centroid**: The central point of a cluster, calculated as the mean of all data points assigned to it.\n",
    "- **Cluster Assignment**: Each data point is assigned to the cluster whose centroid is closest.\n",
    "- **Objective**: Minimize the **Within-Cluster Sum of Squares (WCSS)**, which is the sum of squared distances between each point and its centroid.\n",
    "    `WCSS = Σ (xᵢ - μⱼ)²`  \n",
    "    Where:  \n",
    "    - `xᵢ` = Data point  \n",
    "    - `μⱼ` = Cluster centroid\n",
    "\n",
    "- **Distance**: Euclidean distance is default: \\( \\|x - \\mu\\| = \\sqrt{\\sum (x_j - \\mu_j)^2} \\).\n",
    "- **Convergence**: Stops when centroids no longer move significantly or max iterations are reached.\n",
    "\n",
    "### **b. Hyperparameters**\n",
    "`kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, random_state=42)`\n",
    "- `n_clusters` (\\(k\\)): Number of clusters (must be specified).\n",
    "- `init`: Initialization method (\"random\" or \"k-means++\" for better starting points).\n",
    "- `max_iter`: Maximum iterations for convergence.\n",
    "- `n_init`: Number of times to run with different initial centroids (picks best).\n",
    "\n",
    "---\n",
    "\n",
    "## Advantages and Limitations\n",
    "\n",
    "### Advantages\n",
    "\n",
    "- **Simplicity**: Easy to understand and implement.\n",
    "- **Efficiency**: Computationally efficient, with a time complexity of O(N*K*I), where N is the number of points, K is the number of clusters, and I is the number of iterations.\n",
    "- **Versatility**: Works with various data types and can adapt to different distance metrics.\n",
    "\n",
    "### Limitations\n",
    "\n",
    "- **Choosing K**: Requires specifying the number of clusters beforehand, which isn’t always intuitive.\n",
    "- **Sensitivity to Initialization**: Random starting centroids can lead to suboptimal results.\n",
    "- **Assumes Spherical Clusters**: Performs poorly with clusters of irregular shapes or varying densities.\n",
    "- **Outlier Sensitivity**: Outliers can disproportionately affect centroid positions.\n",
    "\n",
    "---\n",
    "## Choosing the Optimal Number of Clusters (K)\n",
    "\n",
    "Selecting the right **K** is critical for effective clustering. Here are three common methods, all of which we’ll implement in Python:\n",
    "\n",
    "1. **Elbow Method**: Plots WCSS against K to find a point where adding more clusters yields diminishing returns (the \"elbow\").\n",
    "2. **Silhouette Score**: Measures how well-separated and cohesive clusters are, with higher scores indicating better clustering.\n",
    "3. **Gap Statistic**: Compares WCSS of the data to that of random data (not implemented here due to complexity, but widely recognized).\n",
    "\n",
    "---\n",
    "\n",
    "## Applications of K-Means\n",
    "\n",
    "K-Means is widely used across industries and domains. Examples include:\n",
    "\n",
    "- **Customer Segmentation**: Grouping customers based on purchase history or demographics for targeted marketing.\n",
    "- **Image Compression**: Reducing the number of colors in an image by clustering similar colors.\n",
    "- **Document Clustering**: Organizing articles or texts into topics based on word usage.\n",
    "- **Anomaly Detection**: Identifying outliers that don’t fit well into any cluster.\n",
    "\n",
    "---\n",
    "\n",
    "## Implementing K-Means in Python: A One-Stop Solution\n",
    "\n",
    "We’ll use the **Iris dataset**, which contains 150 samples of iris flowers with four features (sepal length, sepal width, petal length, petal width) and three species. Although K-Means is unsupervised and doesn’t use labels during clustering, we’ll use the true labels afterward to evaluate performance.\n",
    "\n",
    "### Step 1: Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34b5014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features (150 samples, 4 features)\n",
    "y = iris.target  # True labels (for evaluation)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6993b65f",
   "metadata": {},
   "source": [
    "- **Why Standardize?** K-Means relies on distance calculations, so scaling features to have a mean of 0 and variance of 1 ensures all features contribute equally.\n",
    "\n",
    "### Step 2: Determine the Optimal Number of Clusters\n",
    "\n",
    "Elbow Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559a832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate WCSS for K ranging from 1 to 10\n",
    "wcss = []\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(X_scaled)\n",
    "    wcss.append(kmeans.inertia_)  # Inertia is WCSS\n",
    "\n",
    "# Plot the Elbow Curve\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, 11), wcss, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "plt.title('Elbow Method for Optimal K')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b48035",
   "metadata": {},
   "source": [
    "- **Interpretation**: Look for an \"elbow\" where WCSS decreases more slowly. For the Iris dataset, K=3 often appears as a reasonable choice.\n",
    "\n",
    "#### Silhouette Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc87c54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Silhouette Score for K ranging from 2 to 10\n",
    "sil_scores = []\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    sil_scores.append(silhouette_score(X_scaled, labels))\n",
    "\n",
    "# Plot Silhouette Scores\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(2, 11), sil_scores, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score for Optimal K')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a7af3e",
   "metadata": {},
   "source": [
    "- **Interpretation**: The highest score indicates the best K. For Iris, K=2 or K=3 typically scores well, with K=3 aligning with the known species count.\n",
    "\n",
    "### Step 3: Train the K-Means Model\n",
    "\n",
    "Based on the Elbow and Silhouette analysis, let’s use K=3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe145235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit K-Means with K=3\n",
    "kmeans = KMeans(n_clusters=3, init='k-means++', n_init=10, random_state=42)\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "# Get cluster assignments and centroids\n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c5ea37",
   "metadata": {},
   "source": [
    "- **K-Means++**: Improves initialization by spreading out initial centroids, reducing the risk of poor clustering.\n",
    "- **n_init=10**: Runs the algorithm 10 times with different initializations and selects the best result.\n",
    "\n",
    "### Step 4: Visualize the Clusters\n",
    "\n",
    "Since we can’t plot all four features, we’ll visualize the first two (sepal length and width):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6beaac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels, cmap='viridis', marker='o', edgecolor='k', s=50)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x', s=200, label='Centroids')\n",
    "plt.xlabel('Sepal Length (Scaled)')\n",
    "plt.ylabel('Sepal Width (Scaled)')\n",
    "plt.title('K-Means Clustering of Iris Dataset (K=3)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c5fc36",
   "metadata": {},
   "source": [
    "- **What You See**: Points are colored by cluster, and red X’s mark the centroids.\n",
    "\n",
    "### Step 5: Evaluate Clustering Performance\n",
    "\n",
    "Since we have true labels, we can compare clusters to species using a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41081366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix to compare clusters with true labels\n",
    "conf_matrix = confusion_matrix(y, labels)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044300aa",
   "metadata": {},
   "source": [
    "- **Output Example**:\n",
    "  ```\n",
    "  Confusion Matrix:\n",
    "  [[50  0  0]\n",
    "   [ 0 48  2]\n",
    "   [ 0 14 36]]\n",
    "  ```\n",
    "- **Interpretation**: Rows are true species (0, 1, 2), and columns are clusters. Here, cluster 0 perfectly matches species 0, while species 1 and 2 have some overlap.\n",
    "\n",
    "---\n",
    "\n",
    "## **Tools and Methods Summary**\n",
    "- **Modeling**: `sklearn.cluster.KMeans`.\n",
    "- **Evaluation**: `sklearn.metrics.silhouette_score`, `.inertia_` (WCSS).\n",
    "- **Visualization**: `matplotlib.pyplot.scatter()`, `seaborn.scatterplot()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece34cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict cluster for new data\n",
    "new_data = np.array([[0, 0], [5, 5]])\n",
    "clusters = kmeans.predict(new_data)\n",
    "print(\"New Data Clusters:\", clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0367301",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "K-Means Clustering is a straightforward, effective algorithm for unsupervised grouping, minimizing within-cluster variance to uncover patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## Example with Synthetic Data\n",
    "\n",
    "Let’s apply K-Means to a synthetic dataset for variety:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b575e177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate synthetic data\n",
    "X_synth, y_synth = make_blobs(n_samples=300, centers=4, cluster_std=1.0, random_state=42)\n",
    "\n",
    "# Standardize\n",
    "X_synth_scaled = scaler.fit_transform(X_synth)\n",
    "\n",
    "# Fit K-Means\n",
    "kmeans_synth = KMeans(n_clusters=4, random_state=42)\n",
    "kmeans_synth.fit(X_synth_scaled)\n",
    "labels_synth = kmeans_synth.labels_\n",
    "centroids_synth = kmeans_synth.cluster_centers_\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_synth_scaled[:, 0], X_synth_scaled[:, 1], c=labels_synth, cmap='plasma', s=50)\n",
    "plt.scatter(centroids_synth[:, 0], centroids_synth[:, 1], c='red', marker='x', s=200, label='Centroids')\n",
    "plt.title('K-Means Clustering of Synthetic Data (K=4)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e631a",
   "metadata": {},
   "source": [
    "- **Result**: Clear separation of four clusters, demonstrating K-Means’ effectiveness on well-separated data.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
